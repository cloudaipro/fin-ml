{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#logging\n",
    "from myutil.logconf import logging\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.ERROR)\n",
    "log.setLevel(logging.INFO)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%run 'nb_utils.ipynb'\n",
    "task_name = get_filename_of_ipynb()\n",
    "print(task_name)\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/{task_name}'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "\n",
    "# Loading the data\n",
    "stk_symbols = [\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"AMZN\",\n",
    "    \"NVDA\",\n",
    "    \"GOOGL\",\n",
    "    \"GOOG\",\n",
    "    \"META\",\n",
    "    \"TSLA\",\n",
    "    \"UNH\",\n",
    "    \"LLY\",\n",
    "    \"JPM\",\n",
    "    \"XOM\",\n",
    "    \"JNJ\",\n",
    "    \"V\",\n",
    "    \"PG\",\n",
    "    \"AVGO\",\n",
    "    \"MA\",\n",
    "    \"HD\",\n",
    "    \"CVX\",\n",
    "    \"MRK\",\n",
    "    \"ABBV\",\n",
    "    \"PEP\",\n",
    "    \"COST\",\n",
    "    \"ADBE\",\n",
    "    \"KO\",\n",
    "    \"CSCO\",\n",
    "    \"WMT\",\n",
    "    \"TMO\",\n",
    "    \"MCD\",\n",
    "    \"PFE\",\n",
    "    \"CRM\",\n",
    "    \"BAC\",\n",
    "    \"ACN\",\n",
    "    \"CMCSA\",\n",
    "    \"LIN\",\n",
    "    \"NFLX\",\n",
    "    \"ABT\",\n",
    "    \"ORCL\",\n",
    "    \"DHR\",\n",
    "    \"AMD\",\n",
    "    \"WFC\",\n",
    "    \"DIS\",\n",
    "    \"TXN\",\n",
    "    \"PM\",\n",
    "    \"VZ\",\n",
    "    \"INTU\",\n",
    "    \"COP\",\n",
    "    \"CAT\",\n",
    "    \"AMGN\",\n",
    "    \"NEE\",\n",
    "    \"INTC\",\n",
    "    \"UNP\",\n",
    "    \"LOW\",\n",
    "    \"IBM\",\n",
    "    \"BMY\",\n",
    "    \"SPGI\",\n",
    "    \"RTX\",\n",
    "    \"HON\",\n",
    "    \"BA\",\n",
    "    \"UPS\",\n",
    "    \"GE\",\n",
    "    \"QCOM\",\n",
    "    \"AMAT\",\n",
    "    \"NKE\",\n",
    "    \"PLD\",\n",
    "    \"NOW\",\n",
    "    \"BKNG\",\n",
    "    \"SBUX\",\n",
    "    \"MS\",\n",
    "    \"ELV\",\n",
    "    \"MDT\",\n",
    "    \"GS\",\n",
    "    \"DE\",\n",
    "    \"ADP\",\n",
    "    \"LMT\",\n",
    "    \"TJX\",\n",
    "    \"T\",\n",
    "    \"BLK\",\n",
    "    \"ISRG\",\n",
    "    \"MDLZ\",\n",
    "    \"GILD\",\n",
    "    \"MMC\",\n",
    "    \"AXP\",\n",
    "    \"SYK\",\n",
    "    \"REGN\",\n",
    "    \"VRTX\",\n",
    "    \"ETN\",\n",
    "    \"LRCX\",\n",
    "    \"ADI\",\n",
    "    \"SCHW\",\n",
    "    \"CVS\",\n",
    "    \"ZTS\",\n",
    "    \"CI\",\n",
    "    \"CB\",\n",
    "    \"AMT\",\n",
    "    \"SLB\",\n",
    "    \"C\",\n",
    "    \"BDX\",\n",
    "    \"MO\",\n",
    "    \"PGR\",\n",
    "    \"TMUS\",\n",
    "    \"FI\",\n",
    "    \"SO\",\n",
    "    \"EOG\",\n",
    "    \"BSX\",\n",
    "    \"CME\",\n",
    "    \"EQIX\",\n",
    "    \"MU\",\n",
    "    \"DUK\",\n",
    "    \"PANW\",\n",
    "    \"PYPL\",\n",
    "    \"AON\",\n",
    "    \"SNPS\",\n",
    "    \"ITW\",\n",
    "    \"KLAC\",\n",
    "    \"LULU\",\n",
    "    \"ICE\",\n",
    "    \"APD\",\n",
    "    \"SHW\",\n",
    "    \"CDNS\",\n",
    "    \"CSX\",\n",
    "    \"NOC\",\n",
    "    \"CL\",\n",
    "    \"MPC\",\n",
    "    \"HUM\",\n",
    "    \"FDX\",\n",
    "    \"WM\",\n",
    "    \"MCK\",\n",
    "    \"TGT\",\n",
    "    \"ORLY\",\n",
    "    \"HCA\",\n",
    "    \"FCX\",\n",
    "    \"EMR\",\n",
    "    \"PXD\",\n",
    "    \"MMM\",\n",
    "    \"MCO\",\n",
    "    \"ROP\",\n",
    "    \"CMG\",\n",
    "    \"PSX\",\n",
    "    \"MAR\",\n",
    "    \"PH\",\n",
    "    \"APH\",\n",
    "    \"GD\",\n",
    "    \"USB\",\n",
    "    \"NXPI\",\n",
    "    \"AJG\",\n",
    "    \"NSC\",\n",
    "    \"PNC\",\n",
    "    \"VLO\",\n",
    "    \"GBP\",\n",
    "    \"F\",\n",
    "    \"MSI\",\n",
    "    \"GM\",\n",
    "    \"TT\",\n",
    "    \"EW\",\n",
    "    \"CARR\",\n",
    "    \"AZO\",\n",
    "    \"ADSK\",\n",
    "    \"TDG\",\n",
    "    \"ANET\",\n",
    "    \"SRE\",\n",
    "    \"ECL\",\n",
    "    \"OXY\",\n",
    "    \"PCAR\",\n",
    "    \"ADM\",\n",
    "    \"MNST\",\n",
    "    \"KMB\",\n",
    "    \"PSA\",\n",
    "    \"CCI\",\n",
    "    \"CHTR\",\n",
    "    \"MCHP\",\n",
    "    \"MSCI\",\n",
    "    \"CTAS\",\n",
    "    \"WMB\",\n",
    "    \"AIG\",\n",
    "    \"STZ\",\n",
    "    \"HES\",\n",
    "    \"NUE\",\n",
    "    \"ROST\",\n",
    "    \"AFL\",\n",
    "    \"AEP\",\n",
    "    \"IDXX\",\n",
    "    \"D\",\n",
    "    \"TEL\",\n",
    "    \"JCI\",\n",
    "    \"MET\",\n",
    "    \"GIS\",\n",
    "    \"IQV\",\n",
    "    \"EXC\",\n",
    "    \"WELL\",\n",
    "    \"DXCM\",\n",
    "    \"HLT\",\n",
    "    \"ON\",\n",
    "    \"COF\",\n",
    "    \"PAYX\",\n",
    "    \"TFC\",\n",
    "    \"USD\",\n",
    "    \"BIIB\",\n",
    "    \"O\",\n",
    "    \"FTNT\",\n",
    "    \"DOW\",\n",
    "    \"TRV\",\n",
    "    \"DLR\",\n",
    "    \"MRNA\",\n",
    "    \"CPRT\",\n",
    "    \"ODFL\",\n",
    "    \"DHI\",\n",
    "    \"YUM\",\n",
    "    \"SPG\",\n",
    "    \"CTSH\",\n",
    "    \"AME\",\n",
    "    \"BKR\",\n",
    "    \"SYY\",\n",
    "    \"A\",\n",
    "    \"CTVA\",\n",
    "    \"CNC\",\n",
    "    \"EL\",\n",
    "    \"AMP\",\n",
    "    \"CEG\",  # PCT <= -0.05,  size = 0\n",
    "    \"HAL\",\n",
    "    \"OTIS\",  # PCT <= -0.05,  size = 0\n",
    "    \"ROK\",\n",
    "    \"PRU\",\n",
    "    \"DD\",\n",
    "    \"KMI\",\n",
    "    \"VRSK\",\n",
    "    \"LHX\",\n",
    "    \"DG\",\n",
    "    \"FIS\",\n",
    "    \"CMI\",\n",
    "    \"CSGP\",\n",
    "    \"FAST\",\n",
    "    \"PPG\",\n",
    "    \"GPN\",\n",
    "    \"GWW\",\n",
    "    \"HSY\",\n",
    "    \"BK\",\n",
    "    \"XEL\",\n",
    "    \"DVN\",\n",
    "    \"EA\",\n",
    "    \"NEM\",\n",
    "    \"ED\",\n",
    "    \"URI\",\n",
    "    \"VICI\",\n",
    "    \"PEG\",\n",
    "    \"KR\",\n",
    "    \"RSG\",\n",
    "    \"LEN\",\n",
    "    \"PWR\",\n",
    "    \"WST\",\n",
    "    \"COR\",\n",
    "    \"OKE\",\n",
    "    \"VMC\",\n",
    "    \"KDP\",\n",
    "    \"WBD\",\n",
    "    \"ACGL\",\n",
    "    \"ALL\",\n",
    "    \"IR\",\n",
    "    \"CDW\",\n",
    "    \"FANG\",\n",
    "    \"MLM\",\n",
    "    \"PCG\",\n",
    "    \"DAL\",\n",
    "    \"EXR\",\n",
    "    \"FTV\",\n",
    "    \"AWK\",\n",
    "    \"IT\",\n",
    "    \"KHC\",\n",
    "    \"GEHC\",  # PCT <= -0.05,  size = 0\n",
    "    \"WEC\",\n",
    "    \"HPQ\",\n",
    "    \"EIX\",\n",
    "    \"CBRE\",\n",
    "    \"APTV\",\n",
    "    \"ANSS\",\n",
    "    \"MTD\",\n",
    "    \"DLTR\",\n",
    "    \"AVB\",\n",
    "    \"ILMN\",\n",
    "    \"ALGN\",\n",
    "    \"LYB\",\n",
    "    \"TROW\",\n",
    "    \"GLW\",\n",
    "    \"EFX\",\n",
    "    \"WY\",\n",
    "    \"ZBH\",\n",
    "    \"XYL\",\n",
    "    \"SBAC\",\n",
    "    \"RMD\",\n",
    "    \"TSCO\",\n",
    "    \"EBAY\",\n",
    "    \"KEYS\",\n",
    "    \"CHD\",\n",
    "    \"STT\",\n",
    "    \"DFS\",\n",
    "    \"HIG\",\n",
    "    \"ALB\",\n",
    "    \"STE\",\n",
    "    \"ES\",\n",
    "    \"TTWO\",\n",
    "    \"MPWR\",\n",
    "    \"CAH\",\n",
    "    \"EQR\",\n",
    "    \"RCL\",\n",
    "    \"WTW\",\n",
    "    \"HPE\",\n",
    "    \"DTE\",\n",
    "    \"GPC\",\n",
    "    \"BR\",\n",
    "    \"ULTA\",\n",
    "    \"FICO\",\n",
    "    \"CTRA\",\n",
    "    \"BAX\",\n",
    "    \"AEE\",\n",
    "    \"MTB\",\n",
    "    \"MKC\",\n",
    "    \"ETR\",\n",
    "    \"WAB\",\n",
    "    \"DOV\",\n",
    "    \"FE\",\n",
    "    \"RJF\",\n",
    "    \"INVH\",\n",
    "    \"FLT\",\n",
    "    \"CLX\",\n",
    "    \"TDY\",\n",
    "    \"TRGP\",\n",
    "    \"DRI\",\n",
    "    \"LH\",\n",
    "    \"HOLX\",\n",
    "    \"VRSN\",\n",
    "    \"MOH\",\n",
    "    \"LUV\",\n",
    "    \"PPL\",\n",
    "    \"ARE\",\n",
    "    \"NVR\",\n",
    "    \"COO\",\n",
    "    \"WBA\",\n",
    "    \"PHM\",\n",
    "    \"NDAQ\",\n",
    "    \"HWM\",\n",
    "    \"RF\",\n",
    "    \"CNP\",\n",
    "    \"IRM\",\n",
    "    \"LVS\",\n",
    "    \"FITB\",\n",
    "    \"EXPD\",\n",
    "    \"VTR\",\n",
    "    \"FSLR\",\n",
    "    \"PFG\",\n",
    "    \"BRO\",\n",
    "    \"J\",\n",
    "    \"IEX\",\n",
    "    \"BG\",\n",
    "    \"ATO\",\n",
    "    \"FDS\",\n",
    "    \"ENPH\",\n",
    "    \"MAA\",\n",
    "    \"CMS\",\n",
    "    \"IFF\",\n",
    "    \"BALL\",\n",
    "    \"SWKS\",\n",
    "    \"CINF\",\n",
    "    \"NTAP\",\n",
    "    \"STLD\",\n",
    "    \"UAL\",\n",
    "    \"WAT\",\n",
    "    \"OMC\",\n",
    "    \"TER\",\n",
    "    \"CCL\",\n",
    "    \"JBHT\",\n",
    "    \"MRO\",\n",
    "    \"TYL\",\n",
    "    \"HBAN\",\n",
    "    \"K\",\n",
    "    \"GRMN\",\n",
    "    \"CBOE\",\n",
    "    \"NTRS\",\n",
    "    \"TSN\",\n",
    "    \"AKAM\",\n",
    "    \"EG\",\n",
    "    \"ESS\",\n",
    "    \"EQT\",\n",
    "    \"TXT\",\n",
    "    \"EXPE\",\n",
    "    \"SJM\",\n",
    "    \"PTC\",\n",
    "    \"DGX\",\n",
    "    \"AVY\",\n",
    "    \"RVTY\",\n",
    "    \"BBY\",\n",
    "    \"CF\",\n",
    "    \"CAG\",\n",
    "    \"EPAM\",\n",
    "    \"AMCR\",\n",
    "    \"LW\",\n",
    "    \"PAYC\",\n",
    "    \"SNA\",\n",
    "    \"AXON\",\n",
    "    \"POOL\",\n",
    "    \"SYF\",\n",
    "    \"SWK\",\n",
    "    \"ZBRA\",\n",
    "    \"DPZ\",\n",
    "    \"PKG\",\n",
    "    \"CFG\",\n",
    "    \"LDOS\",\n",
    "    \"VTRS\",\n",
    "    \"PODD\",\n",
    "    \"LKQ\",\n",
    "    \"MOS\",\n",
    "    \"APA\",\n",
    "    \"EVRG\",\n",
    "    \"TRMB\",\n",
    "    \"MGM\",\n",
    "    \"NDSN\",\n",
    "    \"WDC\",\n",
    "    \"MAS\",\n",
    "    \"LNT\",\n",
    "    \"IPG\",\n",
    "    \"MTCH\",\n",
    "    \"STX\",\n",
    "    \"KMX\",\n",
    "    \"TECH\",\n",
    "    \"WRB\",\n",
    "    \"LYV\",\n",
    "    \"IP\",\n",
    "    \"UDR\",\n",
    "    \"AES\",\n",
    "    \"CE\",\n",
    "    \"INCY\",\n",
    "    \"L\",\n",
    "    \"TAP\",\n",
    "    \"GEN\",\n",
    "    \"CPT\",\n",
    "    \"KIM\",\n",
    "    \"JKHY\",\n",
    "    \"HRL\",\n",
    "    \"HST\",\n",
    "    \"FMC\",\n",
    "    \"CZR\",\n",
    "    \"PEAK\",\n",
    "    \"CDAY\",\n",
    "    \"PNR\",\n",
    "    \"NI\",\n",
    "    \"CHRW\",\n",
    "    \"HSIC\",\n",
    "    \"CRL\",\n",
    "    \"REG\",\n",
    "    \"QRVO\",\n",
    "    \"TFX\",\n",
    "    \"KEY\",\n",
    "    \"GL\",\n",
    "    \"EMN\",\n",
    "    \"WYNN\",\n",
    "    \"ALLE\",\n",
    "    \"AAL\",\n",
    "    \"FFIV\",\n",
    "    \"BWA\",\n",
    "    \"BXP\",\n",
    "    \"MKTX\",\n",
    "    \"ROL\",\n",
    "    \"JNPR\",\n",
    "    \"PNW\",\n",
    "    \"ETSY\",\n",
    "    \"BLDR\",\n",
    "    \"FOXA\",\n",
    "    \"AOS\",\n",
    "    \"HAS\",\n",
    "    \"HII\",\n",
    "    \"NRG\",\n",
    "    \"CPB\",\n",
    "    \"UHS\",\n",
    "    \"BIO\",\n",
    "    \"WRK\",\n",
    "    \"RHI\",\n",
    "    \"CTLT\",\n",
    "    \"XRAY\",\n",
    "    \"BBWI\",\n",
    "    \"NWSA\",\n",
    "    \"TPR\",\n",
    "    \"PARA\",\n",
    "    \"WHR\",\n",
    "    \"BEN\",\n",
    "    \"AIZ\",\n",
    "    \"NCLH\",\n",
    "    \"GNRC\",\n",
    "    \"FRT\",\n",
    "    \"IVZ\",\n",
    "    \"VFC\",\n",
    "    \"CMA\",\n",
    "    \"DVA\",\n",
    "    \"JBL\",\n",
    "    \"HUBB\",\n",
    "    \"ZION\",\n",
    "    \"UBER\",\n",
    "    \"MHK\",\n",
    "    \"RL\",\n",
    "    \"FOX\",\n",
    "    \"BX\",\n",
    "    \"ABNB\",\n",
    "    \"NWS\",\n",
    "]\n",
    "# # stk_symbols = [\n",
    "# #     \"AAPL\",\n",
    "# #     \"MSFT\",\n",
    "# #     \"AMZN\",\n",
    "# #     \"NVDA\",\n",
    "# #     \"GOOGL\",\n",
    "# #     \"TSLA\",\n",
    "# #     \"META\",\n",
    "# #     \"GOOG\",\n",
    "# #     \"ADBE\",\n",
    "# #     \"NFLX\",\n",
    "# #     \"CSCO\",\n",
    "# #     \"INTC\",\n",
    "# #     \"INTU\",\n",
    "# #     \"CMCSA\",\n",
    "# #     \"TXN\",\n",
    "# #     \"AMAT\",\n",
    "# #     \"ADSK\",\n",
    "# #     \"AMD\",\n",
    "# #     \"QCOM\",\n",
    "# #     \"MU\",\n",
    "# # ]\n",
    "# stk_symbols = [\n",
    "#     \"AAPL\",\n",
    "#     \"MSFT\",\n",
    "#     \"AMZN\",\n",
    "#     \"NVDA\",\n",
    "#     \"GOOGL\",\n",
    "#     \"TSLA\",\n",
    "#     \"META\",\n",
    "#     \"GOOG\",\n",
    "# ]\n",
    "\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for symbol in stk_symbols:\n",
    "    stk_file = f\"{data_dir}{symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index(\"Date\")\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {symbol} from yfin and write to {stk_file} completely!\")\n",
    "    ticks_data.append(_stk_data)\n",
    "    print(f\"{symbol}, size:{len(_stk_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_name = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_name)\n",
    "return_period = 5\n",
    "seq_len = 3\n",
    "validation_size = 0.2\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "shuffle = True\n",
    "print(f\"device_name:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_threshold = 0.05\n",
    "class_percentage_threshold = 0.08  # percentage threshold for class size\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "\n",
    "# number of classes = 3\n",
    "# 0: PCT <= -0.05\n",
    "# 1: 0.05 < PCT < -0.05\n",
    "# 2: PCT >= -0.05\n",
    "# num_classes = 3\n",
    "\n",
    "# def gen_pct_label(stk_data, _return_period):\n",
    "#     max_price_period = (\n",
    "#         stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "#     )\n",
    "#     max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "#     pct_label = max_pct_period.apply(\n",
    "#         lambda x: 2 if x >= pct_threshold else 0 if x <= -pct_threshold else 1\n",
    "#     ).astype(\"int8\")\n",
    "#     pct_label.name = \"label\"\n",
    "#     return pct_label\n",
    "\n",
    "\n",
    "# number of classes = 2\n",
    "# 0: PCT < 0.05\n",
    "# 1: PCT >= -0.05\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "def gen_pct_label(stk_data, _return_period):\n",
    "    max_price_period = (\n",
    "        stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "    )\n",
    "    max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "    pct_label = max_pct_period.apply(lambda x: 1 if x >= pct_threshold else 0).astype(\n",
    "        \"int8\"\n",
    "    )\n",
    "    pct_label.name = \"label\"\n",
    "    return pct_label\n",
    "\n",
    "\n",
    "def class_percentage(analysis_data):\n",
    "    stat = analysis_data.groupby(\"label\").size()\n",
    "    total = len(analysis_data)\n",
    "    p = []\n",
    "    for i in range(num_classes):\n",
    "        p.append(stat[i] / total if i in stat.index else 0.0)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=10),\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=60),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    buy_signal = sma[\"SMA_10\"] > sma[\"SMA_60\"]\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal[\"Signal\"] = (buy_signal).astype(\"int\")\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "\n",
    "def gen_analysis_data(stk_data, _return_period):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.adosc(),\n",
    "            stk_data.ta.kvo(),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=10) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=30) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=200) / 100,\n",
    "            stk_data.ta.stoch(k=10) / 100,\n",
    "            stk_data.ta.stoch(k=30) / 100,\n",
    "            stk_data.ta.stoch(k=200) / 100,\n",
    "            gen_buy_sell_signal(stk_data),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data = pd.concat(\n",
    "        [data.astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(_return_period, verbose=False):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    ticks_dataset = []\n",
    "    ignore_ticks_data_count = 0\n",
    "    for i, tick_data in enumerate(tqdm(ticks_data)):\n",
    "        analysis_data = gen_analysis_data(tick_data, _return_period)\n",
    "        classes_percentage = class_percentage(analysis_data)\n",
    "        if 0 in classes_percentage:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Some classes don't have any data  : {stk_symbols[i]}, {classes_percentage}\"\n",
    "                )\n",
    "            ignore_ticks_data_count += 1\n",
    "        elif any(p < class_percentage_threshold for p in classes_percentage):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Some classes are too small  : {stk_symbols[i]}, {classes_percentage}\"\n",
    "                )\n",
    "            ignore_ticks_data_count += 1\n",
    "        else:\n",
    "            ticks_dataset.append(analysis_data)\n",
    "    if ignore_ticks_data_count > 0:\n",
    "        print(\n",
    "            f\"There are {ignore_ticks_data_count} stocks in total, some classes have no data or are too small\"\n",
    "        )\n",
    "    return ticks_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = prepare_dataset(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_percentage(stk_data):\n",
    "    stat = stk_data.groupby(\"label\").size()\n",
    "    total = len(stk_data)\n",
    "    p = []\n",
    "    for i in range(num_classes):\n",
    "        p.append(stat[i] / total if i in stat.index else 0.0)\n",
    "    return p\n",
    "\n",
    "\n",
    "r = class_percentage(gen_analysis_data(ticks_data[0], return_period))\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.DataFrame()\n",
    "for i, stk_data in enumerate(ticks_data):\n",
    "    label_stat = gen_analysis_data(stk_data, return_period).groupby(\"label\").size()\n",
    "    label_stat.name = stk_symbols[i]\n",
    "    classes_df = pd.concat([classes_df, label_stat], axis=1)\n",
    "print(classes_df)\n",
    "classes_count = [classes_df.iloc[i].sum() for i in range(num_classes)]\n",
    "total_recs = sum(classes_count)\n",
    "for i, v in enumerate(classes_count):\n",
    "    print(f\"class {i}: {v*100/total_recs:.3f}%, {v}\")\n",
    "pyplot.bar([\"< 0.05% \", \">= 0.05%\"], classes_count)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.DataFrame()\n",
    "for i, stk_data in enumerate(ticks_data):\n",
    "    label_stat = gen_analysis_data(stk_data, return_period).groupby(\"label\").size()\n",
    "    label_stat.name = stk_symbols[i]\n",
    "    classes_df = pd.concat([classes_df, label_stat], axis=1)\n",
    "print(classes_df)\n",
    "# classes_count = [classes_df.iloc[i].sum() for i in range(num_classes)]\n",
    "# total_recs = sum(classes_count)\n",
    "# for i, v in enumerate(classes_count):\n",
    "#     print(f\"class {i}: {v*100/total_recs:.3f}%, {v}\")\n",
    "# pyplot.bar([\"<= -0.5%\", \" between \", \">= 0.05%\"], classes_count)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df_t = classes_df.T\n",
    "classes_df_t[\"pos_%\"] = classes_df_t[1] / (classes_df_t[0] + classes_df_t[1])\n",
    "sorted = classes_df_t.sort_values(\"pos_%\")\n",
    "print(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_small = classes_df_t[classes_df_t[\"pos_%\"] < stk_pos_threshold]  # 0.08\n",
    "print(too_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df_t = classes_df.T\n",
    "print(classes_df_t)\n",
    "fig, ax = pyplot.subplots(figsize=(10, 200))\n",
    "# pyplot.tight_layout()\n",
    "labels = classes_df_t.index\n",
    "width = 0.4  # the width of the bars\n",
    "y = np.arange(len(labels))\n",
    "ax.barh(y + width, classes_df_t[0], width, label=\"< 0.5%\")\n",
    "ax.barh(y - width, classes_df_t[1], width, label=\">= 0.05%\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stk_symbols.index(\"CEG\"))\n",
    "print(stk_symbols.index(\"OTIS\"))\n",
    "print(stk_symbols.index(\"GEHC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    classes_df_t = classes_df.T\n",
    "    print(classes_df_t[classes_df_t.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from myutil.util import ivmax\n",
    "\n",
    "\n",
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, ticks_data_X, ticks_data_Y, _seq_len, balanced, pattern):\n",
    "        self.ticks_data_X = ticks_data_X\n",
    "        self.ticks_data_Y = ticks_data_Y\n",
    "        self.seq_len = _seq_len\n",
    "        self.balanced = balanced\n",
    "        self.pattern = pattern\n",
    "        self.pattern_size = 0 if pattern == None else len(pattern)\n",
    "\n",
    "        len_array = [len(d) - self.seq_len + 1 for d in ticks_data_X]\n",
    "        self.idx_boundary = [len_array[0]]\n",
    "\n",
    "        for i in range(1, len(len_array)):\n",
    "            self.idx_boundary.append(len_array[i] + self.idx_boundary[i - 1])\n",
    "\n",
    "        self.build_class_indices()\n",
    "        if self.balanced and self.pattern != None and len(self.pattern) > 0:\n",
    "            self.build_pattern_info()\n",
    "        # print(self.idx_boundary[-1])\n",
    "        # print(self.__len__())\n",
    "\n",
    "    def build_class_indices(self):\n",
    "        total_y = pd.concat(\n",
    "            [t[self.seq_len - 1 :][\"label\"] for t in self.ticks_data_Y]\n",
    "        ).reset_index()\n",
    "        self.class_indices = []\n",
    "        for i in range(num_classes):\n",
    "            class_idx_list = total_y.index[total_y[\"label\"] == i].tolist()\n",
    "            random.shuffle(class_idx_list)\n",
    "            self.class_indices.append(class_idx_list)\n",
    "\n",
    "        self.class_num_of_max_size, self.max_class_size = ivmax(\n",
    "            [len(class_idx_list) for class_idx_list in self.class_indices]\n",
    "        )\n",
    "\n",
    "    def build_pattern_info(self):\n",
    "        self.inner_class_count_of_pattern = list(np.zeros(num_classes, dtype=int))\n",
    "        self.inner_offset_of_pattern = list(np.zeros(self.pattern_size, dtype=int))\n",
    "        for i, c in enumerate(self.pattern):\n",
    "            self.inner_offset_of_pattern[i] = self.inner_class_count_of_pattern[c]\n",
    "            self.inner_class_count_of_pattern[c] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(f\"len of dataset:{self.idx_boundary[-1]}\")\n",
    "        # return self.idx_boundary[-1]  # len(self.X) - self.seq_len + 1\n",
    "        if self.balanced:\n",
    "            if self.pattern != None and len(self.pattern) > 0:\n",
    "                return math.ceil(\n",
    "                    self.max_class_size\n",
    "                    * (\n",
    "                        len(self.pattern)\n",
    "                        / self.inner_class_count_of_pattern[self.class_num_of_max_size]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                return self.max_class_size * num_classes\n",
    "\n",
    "        return self.idx_boundary[-1]\n",
    "\n",
    "    def idx_of_balanced_data_to_original_idx(self, idx_of_balanced_data):\n",
    "        if self.pattern != None and self.pattern_size > 0:\n",
    "            pattern_idx = idx_of_balanced_data % self.pattern_size\n",
    "            selected_class = self.pattern[pattern_idx]\n",
    "            idx_of_balanced_class = (\n",
    "                (idx_of_balanced_data // self.pattern_size)\n",
    "                * self.inner_class_count_of_pattern[selected_class]\n",
    "            ) + self.inner_offset_of_pattern[pattern_idx]\n",
    "        else:\n",
    "            selected_class = idx_of_balanced_data % num_classes\n",
    "            idx_of_balanced_class = idx_of_balanced_data // num_classes\n",
    "\n",
    "        offset_balanced_class = idx_of_balanced_class % len(\n",
    "            self.class_indices[selected_class]\n",
    "        )\n",
    "        return self.class_indices[selected_class][offset_balanced_class]\n",
    "\n",
    "    def __getitem__(self, idx_of_balanced_data):\n",
    "        idx = (\n",
    "            self.idx_of_balanced_data_to_original_idx(idx_of_balanced_data)\n",
    "            if self.balanced\n",
    "            else idx_of_balanced_data\n",
    "        )\n",
    "\n",
    "        # print(f\"getitem, idx_of_balanced_data:{idx_of_balanced_data}, idx:{idx}\")\n",
    "        for ticks_data_idx in range(len(self.ticks_data_X)):\n",
    "            if self.idx_boundary[ticks_data_idx] > idx:\n",
    "                break\n",
    "        offset = (\n",
    "            idx if ticks_data_idx == 0 else idx - self.idx_boundary[ticks_data_idx - 1]\n",
    "        )\n",
    "        # print(f\"{ticks_data_idx}, {offset}\")\n",
    "        # print(f\"{len(self.ticks_data_Y[ticks_data_idx])}, {offset + self.seq_len - 1}\")\n",
    "        x = np.array(self.ticks_data_X[ticks_data_idx][offset : offset + self.seq_len])\n",
    "        y = int(self.ticks_data_Y[ticks_data_idx].iloc[offset + self.seq_len - 1, :])\n",
    "        # if x.shape[1] == 34:\n",
    "        #     print(f\"sssssssssssssssssssssssssss {ticks_data_idx}, {idx}\")\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = prepare_dataset(return_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "_return_period = return_period\n",
    "_seq_len = seq_len\n",
    "\n",
    "\n",
    "ticks_dataset = [gen_analysis_data(d, _return_period) for d in ticks_data]\n",
    "ticks_X_train_data = []\n",
    "ticks_Y_train_data = []\n",
    "ticks_X_test_data = []\n",
    "ticks_Y_test_data = []\n",
    "ticks_X_dfm = []\n",
    "for dataset in ticks_dataset:\n",
    "    # test_size = int(dataset.shape[0] * validation_size)\n",
    "    train_size = int(dataset.shape[0] * (1 - validation_size))\n",
    "    # random.seed(42)\n",
    "    train_data = dataset.iloc[0:train_size]\n",
    "    test_data = dataset.iloc[train_size - seq_len + 1 :]\n",
    "\n",
    "    X_train_data = train_data.iloc[:, :-1]\n",
    "    Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "    X_test_data = test_data.iloc[:, :-1]\n",
    "    Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "    features = [\n",
    "        ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "    ]\n",
    "    features.extend([([column], None) for column in X_train_data.columns[3:].values])\n",
    "    # print(features)\n",
    "    X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "    X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "    X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "    ticks_X_dfm.append(X_dfm)\n",
    "    ticks_X_train_data.append(X_train_data)\n",
    "    ticks_Y_train_data.append(Y_train_data)\n",
    "    ticks_X_test_data.append(X_test_data)\n",
    "    ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "train_ds = LSTMDataSet(\n",
    "    ticks_X_train_data, ticks_Y_train_data, _seq_len, True, [0, 0, 1]\n",
    ")\n",
    "# test_ds = LSTMDataSet(ticks_X_test_data, ticks_Y_test_data, _seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.pattern)\n",
    "print(train_ds.__len__())\n",
    "print(train_ds.idx_boundary)\n",
    "for i in train_ds.class_indices:\n",
    "    print(len(i) * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ticks_data[180]\n",
    "x = gen_analysis_data(d, return_period)\n",
    "print(f\"{stk_tickers[180]}, {len(x.columns)}\")\n",
    "# print(ticks_data[180])\n",
    "# print(x)\n",
    "\n",
    "print(d.ta.adosc())\n",
    "print(d.ta.kvo())\n",
    "print(d.ta.rsi(close=\"Adj Close\", length=10) / 100)\n",
    "print(d.ta.rsi(close=\"Adj Close\", length=30) / 100)\n",
    "print(d.ta.rsi(close=\"Adj Close\", length=200) / 100)\n",
    "print(d.ta.stoch(k=10) / 100)\n",
    "print(d.ta.stoch(k=30) / 100)\n",
    "print(d.ta.stoch(k=200) / 100)\n",
    "print(gen_buy_sell_signal(d))\n",
    "\n",
    "# data = pd.concat(\n",
    "#     [data.astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "#     axis=1,\n",
    "# ).dropna()\n",
    "# return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_LSTMDataset(_return_period, _seq_len, train_data_pattern=None):\n",
    "    ticks_dataset = prepare_dataset(_return_period)\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for dataset in ticks_dataset:\n",
    "        # test_size = int(dataset.shape[0] * validation_size)\n",
    "        train_size = int(dataset.shape[0] * (1 - validation_size))\n",
    "        # random.seed(42)\n",
    "        train_data = dataset.iloc[0:train_size]\n",
    "        test_data = dataset.iloc[train_size - seq_len + 1 :]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[3:].values]\n",
    "        )\n",
    "        # print(features)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    train_dataset = LSTMDataSet(\n",
    "        ticks_X_train_data,\n",
    "        ticks_Y_train_data,\n",
    "        _seq_len,\n",
    "        balanced=True,\n",
    "        pattern=train_data_pattern,\n",
    "    )\n",
    "    test_dataset = LSTMDataSet(\n",
    "        ticks_X_test_data, ticks_Y_test_data, _seq_len, balanced=False, pattern=None\n",
    "    )\n",
    "\n",
    "    print(f\"Original training data size: {train_dataset.idx_boundary[-1]}\")\n",
    "    for i in range(num_classes):\n",
    "        print(\n",
    "            f\"class {i}: {len(train_dataset.class_indices[i]) * 100 /train_dataset.idx_boundary[-1]:.1f}% {len(train_dataset.class_indices[i])}\"\n",
    "        )\n",
    "    print(f\"Training dataset size: {train_dataset.__len__()}\")\n",
    "\n",
    "    print(f\"Original testing data size: {test_dataset.idx_boundary[-1]}\")\n",
    "    for i in range(num_classes):\n",
    "        print(\n",
    "            f\"class {i}: {len(test_dataset.class_indices[i]) * 100 /test_dataset.idx_boundary[-1]:.1f}% {len(test_dataset.class_indices[i])}\"\n",
    "        )\n",
    "    print(f\"Testing dataset size: {test_dataset.__len__()}\")\n",
    "\n",
    "    return [train_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds, test_ds = prepare_LSTMDataset(return_period, seq_len, [0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(tr_ds.class_indices[0])}, {len(tr_ds.class_indices[1])}\")\n",
    "print(f\"{tr_ds.class_indices[0][:20]}\")\n",
    "print(f\"{tr_ds.class_indices[1][:10]}\")\n",
    "print(tr_ds.idx_boundary)\n",
    "print(tr_ds.ticks_data_X[0].iloc[1847])\n",
    "print(tr_ds.ticks_data_Y[0].iloc[1847])\n",
    "print(tr_ds.__getitem__(1844))\n",
    "# for i in range(7):\n",
    "#     print(tr_ds.__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_dataloader(_return_period, _seq_len, train_data_pattern=None):\n",
    "    data = prepare_LSTMDataset(_return_period, _seq_len, train_data_pattern)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data[0],\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        data[1],\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, data[0].ticks_data_X[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = t_r.dataset\n",
    "# print(ds.ticks_data_Y[0][2:39])\n",
    "print(ds.class_indices[1][:20])\n",
    "print(ds.class_indices[0][:20])\n",
    "print(ds.__getitem__(1))\n",
    "print(ds.ticks_data_Y[0].head(20))\n",
    "print(ds.ticks_data_X[0].head(20))\n",
    "print(ds.ticks_data_X[0][12:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class StockPCTLabelPredictLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_fc_layers,\n",
    "            activation_type,\n",
    "        )\n",
    "\n",
    "    def __init__(self, input_size, config):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_fc_layers=config[\"num_fc_layers\"],\n",
    "            activation_type=config[\"activation_type\"],\n",
    "        )\n",
    "\n",
    "    def setup_model(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \"\"\"\n",
    "            input_size    : The number of expected features in the input x\n",
    "            hidden_size   : The number of features in the hidden state h\n",
    "            num_layers    : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "            bias          : If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            batch_first   : If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "            dropout       : If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            bidirectional : If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size     : If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        layers = []\n",
    "        in_features = self.hidden_size\n",
    "        for i in range(1, num_fc_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if out_features <= num_classes:\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            (\n",
    "                layers.append(nn.ReLU() if activation_type == 1 else nn.Sigmoid())\n",
    "                if activation_type == 2\n",
    "                else nn.Tanh()\n",
    "            )\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange = 0.5\n",
    "            nn.init.uniform_(m.weight, -initrange, initrange)\n",
    "            nn.init.zeros_(m.bias)\n",
    "            # print(f\"{m.in_features},{m.out_features}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        out, (h_out, _) = self.rnn(x, (h_0, c_0))\n",
    "\n",
    "        fc_input = h_out[-1].view(-1, self.hidden_size)\n",
    "        return self.fc(fc_input)\n",
    "\n",
    "\n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        \"epoch_num\": epoch_num,\n",
    "        \"time\": str(datetime.now),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hyper_parameters\": hyper_parameters,\n",
    "    }\n",
    "    # print(f\"save model:{file_path}\")\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict[\"hyper_parameters\"]\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=data_dict[\"input_size\"],\n",
    "        hidden_size=int(hyper_parameters[\"hidden_size\"]),\n",
    "        num_layers=int(hyper_parameters[\"num_layers\"]),\n",
    "        num_fc_layers=int(hyper_parameters[\"num_fc_layers\"]),\n",
    "        activation_type=int(hyper_parameters[\"activation_type\"]),\n",
    "    )\n",
    "    model.load_state_dict(data_dict[\"model_state\"])\n",
    "    return model, hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "METRICS_LABEL_NDX = 0  # ground_truth\n",
    "METRICS_PBTY_NDX = 1  # Probability of predicition\n",
    "METRICS_PRED_NDX = 2  # class(label) of predicition\n",
    "METRICS_LOSS_NDX = 3\n",
    "METRICS_SIZE = 4\n",
    "softmax = nn.Softmax(dim=1)\n",
    "totalTrainingSamples_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def logMetrics(\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5,\n",
    "    config=None,\n",
    "    log_hparam=False,\n",
    "):\n",
    "    log.info(\n",
    "        \"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            task_name,\n",
    "        )\n",
    "    )\n",
    "    F1_rec = namedtuple(\n",
    "        \"f1_rec\",\n",
    "        \"target_class pos_correct neg_correct pos_count neg_count pos_loss neg_loss precision recall F1\",\n",
    "    )\n",
    "    F1_metrics = []\n",
    "    for target_class in reversed(range(num_classes)):\n",
    "        posLabel_mask = metrics_t[METRICS_LABEL_NDX] == target_class\n",
    "        pos_count = posLabel_mask.sum()\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] != target_class\n",
    "        neg_count = negLabel_mask.sum()\n",
    "\n",
    "        posPred_mask = metrics_t[METRICS_PRED_NDX] == target_class\n",
    "        threshold_mask = metrics_t[METRICS_PBTY_NDX] > classificationThreshold\n",
    "        # TP, truePos_count\n",
    "        TP = pos_correct = int((posLabel_mask & posPred_mask & threshold_mask).sum())\n",
    "\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] != target_class\n",
    "        # TN, trueNeg_count\n",
    "        TN = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "\n",
    "        # FP, falsePos_count\n",
    "        FP = neg_count - neg_correct\n",
    "        # FN, falseNeg_count\n",
    "        FN = pos_count - pos_correct\n",
    "\n",
    "        # precision = TP / (TP + FP)\n",
    "        precision = 0.0 if (TP + FP) == 0 else TP / np.float32(TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        recall = 0.0 if (TP + FN) == 0 else TP / np.float32(TP + FN)\n",
    "        # F1 = 2 * precision * recall / (precision + recall)\n",
    "        F1 = (\n",
    "            0.0\n",
    "            if (precision + recall) == 0.0\n",
    "            else (2 * precision * recall) / np.float32(precision + recall)\n",
    "        )\n",
    "        F1_metrics.append(\n",
    "            F1_rec(\n",
    "                target_class,\n",
    "                pos_correct,\n",
    "                neg_correct,\n",
    "                pos_count,\n",
    "                neg_count,\n",
    "                metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean(),\n",
    "                metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean(),\n",
    "                precision,\n",
    "                recall,\n",
    "                F1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if num_classes == 2:\n",
    "            break\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\" e_loss/all\"] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "    log.info(\n",
    "        (\"E{} {:8} { e_loss/all:.4f} loss\").format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            **metrics_dict,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for target_class, rec in enumerate(F1_metrics):\n",
    "        target_class_str = f\"class {rec.target_class}\" if num_classes > 2 else \"\"\n",
    "        metrics_dict[f\"{target_class_str} e_loss/pos\"] = rec.pos_loss\n",
    "        metrics_dict[f\"{target_class_str} e_loss/neg\"] = rec.neg_loss\n",
    "\n",
    "        metrics_dict[f\"{target_class_str} correct/all\"] = (\n",
    "            (rec.pos_correct + rec.neg_correct) / metrics_t.shape[1] * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} correct/neg\"] = (\n",
    "            (rec.neg_correct) / rec.neg_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} correct/pos\"] = (\n",
    "            (rec.pos_correct) / rec.pos_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} pr/precision\"] = rec.precision\n",
    "        metrics_dict[f\"{target_class_str} pr/recall\"] = rec.recall\n",
    "        metrics_dict[f\"{target_class_str} pr/f1_score\"] = rec.F1\n",
    "\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + \" correct/all:-5.1f}% correct, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/precision:.4f} precision, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/recall:.4f} recall, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/f1_score:.4f} f1 score\"\n",
    "            ).format(epoch_ndx, mode_str, target_class_str, **metrics_dict)\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + \" e_loss/neg:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_neg\",\n",
    "                target_class_str,\n",
    "                neg_correct=rec.neg_correct,\n",
    "                neg_count=rec.neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + \" e_loss/pos:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_pos\",\n",
    "                target_class_str,\n",
    "                pos_correct=rec.pos_correct,\n",
    "                pos_count=rec.pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir + f\"/{mode_str}_cls\")\n",
    "    for key, value in metrics_dict.items():\n",
    "        writer.add_scalar(key, value, totalTrainingSamples_count)\n",
    "\n",
    "    writer.add_pr_curve(\n",
    "        \"pr\",\n",
    "        metrics_t[METRICS_LABEL_NDX],\n",
    "        metrics_t[METRICS_PRED_NDX],\n",
    "        totalTrainingSamples_count,\n",
    "    )\n",
    "\n",
    "    bins = [x / 50.0 for x in range(51)]\n",
    "    negHist_mask = negLabel_mask & (metrics_t[METRICS_PBTY_NDX] > 0.01)\n",
    "    posHist_mask = posLabel_mask & (metrics_t[METRICS_PBTY_NDX] < 0.99)\n",
    "    if negHist_mask.any():\n",
    "        writer.add_histogram(\n",
    "            \"is_neg\",\n",
    "            metrics_t[METRICS_PBTY_NDX, negHist_mask],\n",
    "            totalTrainingSamples_count,\n",
    "            bins=bins,\n",
    "        )\n",
    "    if posHist_mask.any():\n",
    "        writer.add_histogram(\n",
    "            \"is_pos\",\n",
    "            metrics_t[METRICS_PBTY_NDX, posHist_mask],\n",
    "            totalTrainingSamples_count,\n",
    "            bins=bins,\n",
    "        )\n",
    "\n",
    "    if log_hparam:\n",
    "        hparam = config.copy()\n",
    "        hparam[\"0:trn,1:val\"] = 0 if mode_str == \"trn\" else 1\n",
    "        writer.add_hparams(\n",
    "            hparam,\n",
    "            {\n",
    "                \"loss\": metrics_t[METRICS_LOSS_NDX].mean(),\n",
    "                \"F1\": F1_metrics[-1].F1,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return float(metrics_dict[\" e_loss/all\"]), F1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBatchLoss(model, loss_fn, x, y, metrics_g, batch_idx):\n",
    "    x_g = x.to(device)\n",
    "    y_g = y.to(device)\n",
    "    outputs = model(x_g)\n",
    "    loss_g = loss_fn(outputs, y_g)\n",
    "    probability_g, predition_g = torch.max(softmax(outputs), dim=1)\n",
    "\n",
    "    start_ndx = batch_idx * batch_size\n",
    "    end_ndx = start_ndx + y.size(0)\n",
    "\n",
    "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = y_g\n",
    "    metrics_g[METRICS_PBTY_NDX, start_ndx:end_ndx] = probability_g\n",
    "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predition_g\n",
    "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
    "\n",
    "    return loss_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutil.util import enumerateWithEstimate\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def doTraining(model, optimizer, loss_fn, epoch_ndx, train_dl):\n",
    "    global totalTrainingSamples_count\n",
    "    model.train()\n",
    "    trnMetrics_g = torch.zeros(\n",
    "        METRICS_SIZE,\n",
    "        len(train_dl.dataset),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, (x, y) in batch_iter:\n",
    "        # for batch_ndx, (x, y) in enumerate(tqdm(train_dl)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = computeBatchLoss(\n",
    "            model,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            trnMetrics_g,\n",
    "            batch_ndx,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    totalTrainingSamples_count += len(train_dl.dataset)\n",
    "    return trnMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(model, loss_fn, epoch_ndx, val_dl):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, (x, y) in batch_iter:\n",
    "            # for batch_ndx, (x, y) in enumerate(tqdm(val_dl)):\n",
    "            computeBatchLoss(model, loss_fn, x, y, valMetrics_g, batch_ndx)\n",
    "\n",
    "    return valMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(config):\n",
    "    global totalTrainingSamples_count\n",
    "    best_f1 = 0\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    # print(id_str)\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "\n",
    "    train_loader, test_loader, features_size = prepare_dataloader(\n",
    "        config[\"return_period\"], config[\"seq_len\"], train_data_pattern=[0, 0, 1]\n",
    "    )\n",
    "\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        loss, _ = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"trn\",\n",
    "            trnMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        _, F1_metrics = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"val\",\n",
    "            valMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "        if F1_metrics[0].F1 > best_f1:\n",
    "            best_f1 = F1_metrics[0].F1\n",
    "            save_model(model, config, model_name)\n",
    "            print(f\"current loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "config = {\n",
    "    \"return_period\": return_period,\n",
    "    \"seq_len\": seq_len,\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.11646759543664197,\n",
    "    \"optim_type\": 2,  # 1: Adam, 2: SGD  => Adam bad result\n",
    "    \"weight decay\": 0.00001,\n",
    "    \"num_layers\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_fc_layers\": 1,\n",
    "    \"activation_type\": 2,  # Sigmoid\n",
    "}\n",
    "epoch_num = 3\n",
    "# os.mkdir(log_dir)\n",
    "report_f1 = False\n",
    "print(log_dir)\n",
    "start = datetime.now()\n",
    "train_LSTM(config)\n",
    "print(f\"Elapsed time:{datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_train_task(config, data):\n",
    "    global totalTrainingSamples_count\n",
    "    global log_dir\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    # print(id_str)\n",
    "    log_dir = f\"{log_dir_base}/{time_str}/{id_str}\"\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data[0],\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        data[1],\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    features_size = data[0].ticks_data_X[0].shape[1]\n",
    "\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=config[\"weight decay\"],\n",
    "        )\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        loss, _ = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"trn\",\n",
    "            trnMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        _, F1_metrics = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"val\",\n",
    "            valMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "        if F1_metrics[0].F1 > best_f1:\n",
    "            best_f1 = F1_metrics[0].F1\n",
    "            save_model(model, config, model_name)\n",
    "\n",
    "        train.report(\n",
    "            {\n",
    "                \"loss\": loss,\n",
    "                \"f1_score\": F1_metrics[0].F1,\n",
    "                \"precision\": F1_metrics[0].precision,\n",
    "                \"recall\": F1_metrics[0].recall,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    # \"return_period\": tune.grid_search([5]),  # [2,3,5,10]\n",
    "    # \"seq_len\": tune.grid_search([3]),  # 10]),\n",
    "    \"lr\": tune.grid_search([0.1]),  # , 0.01, 0.1, 0.08, 0.12]\n",
    "    \"momentum\": tune.grid_search([0.14647, 0]),  # tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([2]),  # 1: Adam, 2: SGD  => Adam bad result\n",
    "    \"weight decay\": tune.grid_search([0.00001]),  # best value\n",
    "    \"num_layers\": tune.grid_search([1, 2, 3]),  # [1, 2, 4, 8] best value = 4\n",
    "    \"hidden_size\": tune.grid_search([256]),  # [8, 16, 32, 64, 128]\n",
    "    \"num_fc_layers\": tune.grid_search([1]),  # 1, 2, 3]),\n",
    "    \"activation_type\": tune.grid_search(\n",
    "        [2]\n",
    "    ),  # 1: ReLU(),  2: Sigmoid(),  3: Tanh()  => meaningless num_fc_layers == 1\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if (\n",
    "        type(v).__name__ == \"dict\"\n",
    "        and list(v.keys())[0] == \"grid_search\"\n",
    "        and len(list(v.values())[0]) > 1\n",
    "    ):\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "os.mkdir(log_dir)\n",
    "\n",
    "data = prepare_LSTMDataset(return_period, seq_len, train_data_pattern=[0, 1, 0, 1, 0])\n",
    "# analysis = tune.run(\n",
    "#     train_LSTM,\n",
    "#     config=search_space,\n",
    "#     resources_per_trial={\"cpu\": 0.1, \"gpu\": 0.1},\n",
    "#     metric=\"f1_score\",\n",
    "#     mode=\"max\",\n",
    "# )\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(ray_train_task, data=data),\n",
    "        resources={\"cpu\": 0.33, \"gpu\": 0.33},\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"f1_score\",\n",
    "        mode=\"max\",\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "trial_list = list(analysis.trial_dataframes.values())\n",
    "for i, trial in enumerate(trial_list):\n",
    "    if trial.empty == False:\n",
    "        d = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"mean_accuracy\": trial.describe().loc[\"mean\", \"mean_accuracy\"],\n",
    "                \"trial_id\": trial.loc[0:0, \"trial_id\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": [np.NaN], \"trial_id\": [np.NaN]})\n",
    "    accuracy_list.append(d)\n",
    "accuracy_df = pd.concat(accuracy_list)\n",
    "accuracy_df = accuracy_df.reset_index().loc[:, [\"mean_accuracy\", \"trial_id\"]]\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "config_df = pd.DataFrame(analysis.get_all_configs().values())\n",
    "print(config_df)\n",
    "\n",
    "results = pd.concat([accuracy_df, config_df], axis=1)\n",
    "print(results)\n",
    "\n",
    "sorted_results = results.sort_values(by=\"mean_accuracy\", ascending=False)\n",
    "print(sorted_results.head(100))\n",
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results.to_csv(sorted_results_file)\n",
    "\n",
    "best_config = config_df.iloc[sorted_results.index[0]]\n",
    "id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in best_config.to_list())\n",
    "best_model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(best_model_name, f\"{log_dir_base}/{task_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_desc = sorted_results[\"mean_accuracy\"].astype(\"float32\").describe()\n",
    "xlimit_range = [\n",
    "    accuracy_desc[\"min\"] - accuracy_desc[\"std\"],\n",
    "    accuracy_desc[\"max\"] + accuracy_desc[\"std\"],\n",
    "]\n",
    "for hperparameter_name in turning_parameters:\n",
    "    parameter_group = sorted_results.groupby(hperparameter_name)\n",
    "    fix, axs = pyplot.subplots(\n",
    "        1,\n",
    "        len(parameter_group),\n",
    "        layout=\"constrained\",\n",
    "        sharex=False,\n",
    "        sharey=True,\n",
    "        figsize=(12, 2),\n",
    "    )\n",
    "    for i, g in enumerate(parameter_group):\n",
    "        g[1][\"mean_accuracy\"].astype(\"float32\").plot(\n",
    "            kind=\"hist\", bins=50, subplots=True, sharex=False, sharey=True, ax=axs[i]\n",
    "        )\n",
    "        axs[i].set_title(f\"{hperparameter_name}_{g[0]}\")\n",
    "\n",
    "pyplot.xlim(xlimit_range)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results = pd.read_csv(sorted_results_file, dtype=\"str\")\n",
    "best_config = sorted_results.loc[0]\n",
    "print(best_config)\n",
    "# id_str_of_best = f\"5_5_0.01_{best_config.momentum}_{best_config.optim_type}_{best_config.num_layers}_{best_config.hidden_size}_{best_config.num_fc_layers}_{best_config.activation_type}\"\n",
    "# best_model_name = f\"/mnt/AIWorkSpace/work/fin-ml/runs/{_TARGET_STK}/{time_str}/{id_str_of_best}.pt\"\n",
    "# print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "model, config = load_model(f\"{log_dir_base}/{task_name}.pt\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader, features_size = prepare_dataloader(config[\"return_period\"])\n",
    "model.eval()\n",
    "\n",
    "(trainAccuracy, trainF1) = eval_dl_method(model, train_loader, device=device)\n",
    "(testAccuracy, testF1) = eval_dl_method(model, test_loader, device=device)\n",
    "print(f\"Train Accuracy: {trainAccuracy:.2f}\\nTest Accuracy: {testAccuracy:.5f}\")\n",
    "print(f\"Train F1: {trainF1:.2f}\\nTest F1: {testF1:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
