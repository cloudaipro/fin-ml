{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/USB/work/fin-ml/data/\n",
      "/Volumes/USB/work/fin-ml/runs/BuySellSignal\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/BuySellSignal'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 16:26:12,240\tERROR services.py:1329 -- Failed to start the dashboard , return code 1\n",
      "2024-01-29 16:26:12,242\tERROR services.py:1354 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2024-01-29 16:26:12,245\tERROR services.py:1398 -- \n",
      "The last 20 lines of /tmp/ray/session_2024-01-29_16-26-10_660913_2136/logs/dashboard.log (it contains the error message from the dashboard): \n",
      "    from ray.util.state.common import (\n",
      "  File \"/Users/alex/py_env/ml4t/lib/python3.8/site-packages/ray/util/state/__init__.py\", line 1, in <module>\n",
      "    from ray.util.state.api import (\n",
      "  File \"/Users/alex/py_env/ml4t/lib/python3.8/site-packages/ray/util/state/api.py\", line 17, in <module>\n",
      "    from ray.util.state.common import (\n",
      "  File \"/Users/alex/py_env/ml4t/lib/python3.8/site-packages/ray/util/state/common.py\", line 416, in <module>\n",
      "    class ActorState(StateSchema):\n",
      "  File \"pydantic/dataclasses.py\", line 211, in pydantic.dataclasses.dataclass.wrap\n",
      "  File \"pydantic/dataclasses.py\", line 323, in pydantic.dataclasses._add_pydantic_validation_attributes\n",
      "  File \"pydantic/dataclasses.py\", line 378, in pydantic.dataclasses.create_pydantic_model_from_dataclass\n",
      "  File \"pydantic/main.py\", line 1026, in pydantic.main.create_model\n",
      "  File \"pydantic/main.py\", line 198, in pydantic.main.ModelMetaclass.__new__\n",
      "  File \"pydantic/fields.py\", line 506, in pydantic.fields.ModelField.infer\n",
      "  File \"pydantic/fields.py\", line 436, in pydantic.fields.ModelField.__init__\n",
      "  File \"pydantic/fields.py\", line 552, in pydantic.fields.ModelField.prepare\n",
      "  File \"pydantic/fields.py\", line 668, in pydantic.fields.ModelField._type_analysis\n",
      "  File \"/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/typing.py\", line 774, in __subclasscheck__\n",
      "    return issubclass(cls, self.__origin__)\n",
      "TypeError: issubclass() arg 1 must be a class\n",
      "2024-01-29 16:26:12,368\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b749efaa1254549a8ac158b2d6c0b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.8.18</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.9.1</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url=None, python_version='3.8.18', ray_version='2.9.1', ray_commit='cfbf98c315cfb2710c56039a3c96477d196de049', protocol_version=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_of_ipynb():\n",
    "    from IPython import get_ipython\n",
    "    import os\n",
    "    ip = get_ipython()\n",
    "    path = ''\n",
    "    if '__vsc_ipynb_file__' in ip.user_ns:\n",
    "        path = ip.user_ns['__vsc_ipynb_file__']\n",
    "\n",
    "    return '.'.join(os.path.basename(path).split('.')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read /Volumes/USB/work/fin-ml/data/AAPL.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/MSFT.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/AMZN.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/NVDA.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/GOOGL.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/TSLA.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/META.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/GOOG.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/ADBE.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/NFLX.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/CSCO.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/INTC.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/INTU.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/CMCSA.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/TXN.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/AMAT.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/ADSK.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/AMD.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/QCOM.csv completely!\n",
      "read /Volumes/USB/work/fin-ml/data/MU.csv completely!\n",
      "[              Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  19.846  19.894  19.715  19.755     17.319  234684800\n",
      "2014-01-03  19.745  19.775  19.301  19.321     16.938  392467600\n",
      "2014-01-06  19.195  19.529  19.057  19.426     17.031  412610800\n",
      "2014-01-07  19.440  19.499  19.211  19.287     16.909  317209200\n",
      "2014-01-08  19.243  19.484  19.239  19.409     17.016  258529600\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 195.180 195.410 192.970 193.600    193.600   37122800\n",
      "2023-12-26 193.610 193.890 192.830 193.050    193.050   28919300\n",
      "2023-12-27 192.490 193.500 191.090 193.150    193.150   48087700\n",
      "2023-12-28 194.140 194.660 193.170 193.580    193.580   34049900\n",
      "2023-12-29 193.900 194.400 191.730 192.530    192.530   42628800\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  37.350  37.400  37.100  37.160     31.291  30632200\n",
      "2014-01-03  37.200  37.220  36.600  36.910     31.080  31134800\n",
      "2014-01-06  36.850  36.890  36.110  36.130     30.424  43603700\n",
      "2014-01-07  36.330  36.490  36.210  36.410     30.659  35802800\n",
      "2014-01-08  36.000  36.140  35.580  35.760     30.112  59971700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 373.680 375.180 372.710 374.580    374.580  17091100\n",
      "2023-12-26 375.000 376.940 373.500 374.660    374.660  12673100\n",
      "2023-12-27 373.690 375.060 372.810 374.070    374.070  14905400\n",
      "2023-12-28 375.370 376.460 374.160 375.280    375.280  14327000\n",
      "2023-12-29 376.000 377.160 373.480 376.040    376.040  18723000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  19.940  19.968  19.701  19.899     19.899  42756000\n",
      "2014-01-03  19.914  20.135  19.811  19.822     19.822  44204000\n",
      "2014-01-06  19.792  19.850  19.421  19.681     19.681  63412000\n",
      "2014-01-07  19.752  19.924  19.715  19.902     19.902  38320000\n",
      "2014-01-08  19.924  20.150  19.802  20.096     20.096  46330000\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 153.770 154.350 152.710 153.420    153.420  29480100\n",
      "2023-12-26 153.560 153.980 153.030 153.410    153.410  25067200\n",
      "2023-12-27 153.560 154.780 153.120 153.340    153.340  31434700\n",
      "2023-12-28 153.720 154.080 152.950 153.380    153.380  27057000\n",
      "2023-12-29 153.100 153.890 151.030 151.940    151.940  39789000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.980   3.995   3.930   3.965      3.741  26009200\n",
      "2014-01-03   3.973   3.980   3.905   3.918      3.696  25933200\n",
      "2014-01-06   3.957   4.000   3.920   3.970      3.745  40949200\n",
      "2014-01-07   4.010   4.050   3.983   4.035      3.807  33328800\n",
      "2014-01-08   4.050   4.110   4.035   4.090      3.859  30819200\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 491.950 493.830 484.670 488.300    488.300  25213900\n",
      "2023-12-26 489.680 496.000 489.600 492.790    492.790  24420000\n",
      "2023-12-27 495.110 496.800 490.850 494.170    494.170  23364800\n",
      "2023-12-28 496.430 498.840 494.120 495.220    495.220  24658700\n",
      "2023-12-29 498.130 499.970 487.510 495.220    495.220  38869000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.914  27.972  27.734  27.856     27.856   72783144\n",
      "2014-01-03  27.903  27.951  27.651  27.653     27.653   66601332\n",
      "2014-01-06  27.853  27.999  27.689  27.961     27.961   70701228\n",
      "2014-01-07  28.153  28.521  28.057  28.500     28.500  102001896\n",
      "2014-01-08  28.679  28.712  28.361  28.559     28.559   89610300\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 140.770 141.990 140.710 141.490    141.490   26514600\n",
      "2023-12-26 141.590 142.680 141.190 141.520    141.520   16780300\n",
      "2023-12-27 141.590 142.080 139.890 140.370    140.370   19628600\n",
      "2023-12-28 140.780 141.140 139.750 140.230    140.230   16045700\n",
      "2023-12-29 139.630 140.360 138.780 139.690    139.690   18727200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02   9.987  10.165   9.770  10.007     10.007   92826000\n",
      "2014-01-03  10.000  10.146   9.907   9.971      9.971   70425000\n",
      "2014-01-06  10.000  10.027   9.683   9.800      9.800   80416500\n",
      "2014-01-07   9.841  10.027   9.683   9.957      9.957   75511500\n",
      "2014-01-08   9.923  10.247   9.917  10.085     10.085   92448000\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 256.760 258.220 251.370 252.540    252.540   93249800\n",
      "2023-12-26 254.490 257.970 252.910 256.610    256.610   86892400\n",
      "2023-12-27 258.350 263.340 257.520 261.440    261.440  106494400\n",
      "2023-12-28 263.660 265.130 252.710 253.180    253.180  113619900\n",
      "2023-12-29 255.100 255.190 247.430 248.480    248.480  100615300\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  54.830  55.220  54.190  54.710     54.710  43195500\n",
      "2014-01-03  55.020  55.650  54.530  54.560     54.560  38246200\n",
      "2014-01-06  54.420  57.260  54.050  57.200     57.200  68852600\n",
      "2014-01-07  57.700  58.550  57.220  57.920     57.920  77207400\n",
      "2014-01-08  57.600  58.410  57.230  58.230     58.230  56682400\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 355.580 357.200 351.220 353.390    353.390  11764200\n",
      "2023-12-26 354.990 356.980 353.450 354.830    354.830   9898600\n",
      "2023-12-27 356.070 359.000 355.310 357.830    357.830  13207900\n",
      "2023-12-28 359.700 361.900 357.810 358.320    358.320  11798800\n",
      "2023-12-29 358.990 360.000 351.820 353.960    353.960  14980500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.782  27.839  27.603  27.724     27.724   73129082\n",
      "2014-01-03  27.771  27.819  27.520  27.522     27.522   66917888\n",
      "2014-01-06  27.721  27.867  27.558  27.829     27.829   71037271\n",
      "2014-01-07  28.020  28.386  27.924  28.365     28.365  102486711\n",
      "2014-01-08  28.543  28.576  28.226  28.424     28.424   90036218\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 142.130 143.250 142.055 142.720    142.720   18494700\n",
      "2023-12-26 142.980 143.945 142.500 142.820    142.820   11170100\n",
      "2023-12-27 142.830 143.320 141.051 141.440    141.440   17288400\n",
      "2023-12-28 141.850 142.270 140.828 141.280    141.280   12192500\n",
      "2023-12-29 140.680 141.435 139.900 140.930    140.930   14872700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  59.060  59.530  58.940  59.290     59.290  2745900\n",
      "2014-01-03  59.190  59.690  59.110  59.160     59.160  1589000\n",
      "2014-01-06  58.060  58.770  58.010  58.120     58.120  3753600\n",
      "2014-01-07  58.260  59.050  58.060  58.970     58.970  2963600\n",
      "2014-01-08  59.120  59.280  58.460  58.900     58.900  3456000\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 600.800 601.860 596.000 598.750    598.750  1659800\n",
      "2023-12-26 598.920 601.690 596.500 598.260    598.260  1595100\n",
      "2023-12-27 598.600 599.790 593.710 596.080    596.080  1394900\n",
      "2023-12-28 597.440 599.040 593.630 595.520    595.520  1702600\n",
      "2023-12-29 596.090 600.750 592.940 596.600    596.600  1893900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  52.401  52.511  51.543  51.831     51.831  12325600\n",
      "2014-01-03  52.000  52.496  51.843  51.871     51.871  10817100\n",
      "2014-01-06  51.890  52.044  50.476  51.367     51.367  15501500\n",
      "2014-01-07  49.684  49.699  48.153  48.500     48.500  36167600\n",
      "2014-01-08  48.104  49.426  48.074  48.713     48.713  20001100\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 494.000 496.020 485.450 486.760    486.760   2701100\n",
      "2023-12-26 489.390 491.480 486.380 491.190    491.190   2034500\n",
      "2023-12-27 491.240 494.020 489.250 491.790    491.790   2561300\n",
      "2023-12-28 492.000 492.890 489.070 490.510    490.510   1710500\n",
      "2023-12-29 490.370 492.230 481.940 486.880    486.880   2739500\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 22.170 22.290 21.910 22.000     16.095  44377000\n",
      "2014-01-03 22.090 22.120 21.830 21.980     16.080  36328200\n",
      "2014-01-06 21.960 22.230 21.930 22.010     16.102  34150300\n",
      "2014-01-07 22.260 22.410 22.150 22.310     16.322  37368800\n",
      "2014-01-08 22.290 22.360 22.150 22.290     16.307  38362700\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 49.840 50.390 49.840 50.090     49.703  12900700\n",
      "2023-12-26 50.110 50.400 50.050 50.280     49.892   9721200\n",
      "2023-12-27 50.300 50.560 50.280 50.440     50.051  10414300\n",
      "2023-12-28 50.580 50.630 50.420 50.480     50.090   8549900\n",
      "2023-12-29 50.450 50.590 50.220 50.520     50.130  12491200\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.780 25.820 25.470 25.790     19.436  31833300\n",
      "2014-01-03 25.860 25.900 25.600 25.780     19.428  27796700\n",
      "2014-01-06 25.770 25.790 25.450 25.460     19.187  28682300\n",
      "2014-01-07 25.540 25.730 25.470 25.590     19.285  19665100\n",
      "2014-01-08 25.640 25.710 25.300 25.430     19.164  29680500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 47.250 48.160 47.200 48.000     48.000  30053700\n",
      "2023-12-26 48.920 50.520 48.710 50.500     50.500  60287400\n",
      "2023-12-27 50.630 51.280 50.190 50.760     50.760  52148000\n",
      "2023-12-28 50.810 50.870 50.160 50.390     50.390  27705200\n",
      "2023-12-29 50.300 50.570 49.770 50.250     50.250  29266500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  76.110  76.160  75.400  75.940     69.476  1355700\n",
      "2014-01-03  75.750  76.380  75.530  75.800     69.348   998800\n",
      "2014-01-06  75.800  76.000  75.470  75.740     69.293  1268700\n",
      "2014-01-07  75.880  77.350  75.740  77.060     70.501  1551000\n",
      "2014-01-08  76.890  77.050  76.010  76.440     70.106  2437600\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 622.830 625.150 617.680 624.070    623.131   820800\n",
      "2023-12-26 625.170 628.330 622.730 624.850    623.910   638300\n",
      "2023-12-27 623.990 629.800 622.260 629.120    628.174   734400\n",
      "2023-12-28 630.740 631.070 627.180 628.020    627.075   680700\n",
      "2023-12-29 628.020 630.830 622.460 625.030    624.090   724300\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.900 25.950 25.635 25.725     20.931  19522400\n",
      "2014-01-03 25.815 25.855 25.425 25.535     20.777  13371400\n",
      "2014-01-06 25.565 25.810 25.335 25.510     20.756  17987800\n",
      "2014-01-07 25.670 26.600 25.555 26.415     21.493  37161400\n",
      "2014-01-08 26.345 26.725 26.260 26.375     21.460  29731600\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 44.130 44.620 43.810 44.000     43.709  11893900\n",
      "2023-12-26 44.000 44.070 43.500 43.930     43.639   9624300\n",
      "2023-12-27 43.900 44.170 43.710 43.990     43.699   9253800\n",
      "2023-12-28 43.970 44.410 43.890 44.120     43.828   9023400\n",
      "2023-12-29 44.090 44.140 43.560 43.850     43.560  13694900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  43.440  43.500  42.880  43.100     32.954  6959200\n",
      "2014-01-03  43.120  43.460  42.970  43.290     33.100  4693300\n",
      "2014-01-06  43.250  43.280  42.850  42.930     32.824  4446300\n",
      "2014-01-07  42.980  43.110  42.640  42.700     32.648  5078900\n",
      "2014-01-08  42.960  43.320  42.620  43.290     33.100  6353500\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 167.260 168.920 166.820 168.240    168.240  3492400\n",
      "2023-12-26 168.940 171.530 168.450 170.810    170.810  3202200\n",
      "2023-12-27 171.220 171.620 170.330 171.230    171.230  3264900\n",
      "2023-12-28 172.000 172.310 170.710 171.720    171.720  3023000\n",
      "2023-12-29 171.540 171.700 169.920 170.460    170.460  2920600\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  17.690  17.690  17.370  17.550     15.280  7785900\n",
      "2014-01-03  17.540  17.700  17.470  17.510     15.245  6773000\n",
      "2014-01-06  17.500  17.510  17.220  17.290     15.053  9975500\n",
      "2014-01-07  17.370  17.430  17.260  17.370     15.123  8133200\n",
      "2014-01-08  17.400  17.450  17.180  17.420     15.166  8026100\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 161.600 163.000 160.840 162.050    162.050  2770600\n",
      "2023-12-26 162.300 164.970 162.100 164.280    164.280  2520500\n",
      "2023-12-27 164.540 164.990 163.530 164.210    164.210  3319600\n",
      "2023-12-28 165.000 165.010 162.850 163.120    163.120  2909700\n",
      "2023-12-29 163.110 163.560 160.700 162.070    162.070  2980700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  49.330  49.740  48.880  49.250     49.250  2488000\n",
      "2014-01-03  49.110  49.500  48.780  48.900     48.900  1934200\n",
      "2014-01-06  48.980  49.290  48.290  48.550     48.550  1856500\n",
      "2014-01-07  48.890  50.100  48.630  49.680     49.680  2002500\n",
      "2014-01-08  49.500  50.550  49.050  50.240     50.240  2047400\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 243.740 244.030 240.310 242.760    242.760   719400\n",
      "2023-12-26 242.490 245.360 241.960 245.070    245.070   595000\n",
      "2023-12-27 245.360 245.880 244.380 245.110    245.110   771900\n",
      "2023-12-28 245.630 245.850 244.020 244.910    244.910   537200\n",
      "2023-12-29 243.720 245.400 242.790 243.480    243.480   721400\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.850   3.980   3.840   3.950      3.950  20548400\n",
      "2014-01-03   3.980   4.000   3.880   4.000      4.000  22887200\n",
      "2014-01-06   4.010   4.180   3.990   4.130      4.130  42398300\n",
      "2014-01-07   4.190   4.250   4.110   4.180      4.180  42932100\n",
      "2014-01-08   4.230   4.260   4.140   4.180      4.180  30678700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 140.480 140.700 138.310 139.600    139.600  35370400\n",
      "2023-12-26 140.070 143.850 139.920 143.410    143.410  47157400\n",
      "2023-12-27 144.720 146.250 143.180 146.070    146.070  49033400\n",
      "2023-12-28 146.800 150.410 145.950 148.760    148.760  63800700\n",
      "2023-12-29 149.500 151.050 147.200 147.410    147.410  62028200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  73.610  73.770  73.260  73.320     54.740  10110200\n",
      "2014-01-03  73.330  73.480  72.440  72.890     54.419   7970400\n",
      "2014-01-06  73.080  73.200  72.550  72.700     54.277   7696200\n",
      "2014-01-07  72.800  73.310  72.600  73.240     54.680   5902700\n",
      "2014-01-08  73.150  73.680  72.680  73.680     55.008   8976900\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 143.250 144.400 142.750 143.490    143.490   4658300\n",
      "2023-12-26 144.170 146.050 143.960 145.460    145.460   4381200\n",
      "2023-12-27 145.850 146.220 145.040 145.720    145.720   4470300\n",
      "2023-12-28 146.180 146.890 145.730 145.860    145.860   4928800\n",
      "2023-12-29 145.410 145.620 143.790 144.630    144.630   4838400\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 21.680 21.790 21.270 21.660     21.293  26413500\n",
      "2014-01-03 21.200 21.430 20.900 20.970     20.615  34590200\n",
      "2014-01-06 20.970 20.970 20.640 20.670     20.320  38180500\n",
      "2014-01-07 20.890 21.940 20.890 21.730     21.362  67904500\n",
      "2014-01-08 24.200 24.500 23.560 23.870     23.466  93499500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 86.150 87.490 85.620 86.490     86.374  22519000\n",
      "2023-12-26 86.700 87.870 86.430 87.060     86.944  11203900\n",
      "2023-12-27 87.480 87.490 86.220 86.660     86.544   9186300\n",
      "2023-12-28 86.750 86.750 85.840 86.000     85.885   9606200\n",
      "2023-12-29 85.840 86.140 85.030 85.340     85.340   8546000\n",
      "\n",
      "[2516 rows x 6 columns]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "\n",
    "# Loading the data\n",
    "stk_tickers = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 'TSLA', 'META', 'GOOG', 'ADBE', 'NFLX', 'CSCO', 'INTC', 'INTU', 'CMCSA', 'TXN', 'AMAT', 'ADSK', 'AMD', 'QCOM', 'MU']\n",
    "\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for stk_symbol in stk_tickers:\n",
    "    stk_file = f\"{data_dir}{stk_symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index('Date')\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([stk_symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {stk_symbol} from yfin and write to {stk_file} completely!\")\n",
    "    ticks_data.append(_stk_data)\n",
    "\n",
    "print(ticks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/USB/work/fin-ml/runs/BuySellSignalPrediction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "validation_size = 0.2\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 0 #2\n",
    "pin_memory = True\n",
    "log_dir_base = f'{os.getcwd()}/runs/{get_filename_of_ipynb()}'\n",
    "log_dir = log_dir_base\n",
    "print(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat([stk_data.ta.sma(close='Adj Close', length=10), stk_data.ta.sma(close='Adj Close', length=60)], axis=1).dropna()\n",
    "    buy_signal = (sma['SMA_10'] > sma['SMA_60'])\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal['Signal'] = (buy_signal).astype('int')\n",
    "    # buy_sell_signal['SELL'] = (~buy_signal).astype('float32')\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "def gen_analysis_data(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    data = pd.concat([\n",
    "        stk_data.ta.adosc(),\n",
    "        stk_data.ta.kvo(),\n",
    "        stk_data.ta.rsi(close='Adj Close', length=10)/100,\n",
    "        stk_data.ta.rsi(close='Adj Close', length=30)/100,\n",
    "        stk_data.ta.rsi(close='Adj Close', length=200)/100,\n",
    "        stk_data.ta.stoch(k=10)/100,\n",
    "        stk_data.ta.stoch(k=30)/100,\n",
    "        stk_data.ta.stoch(k=200)/100,\n",
    "        gen_buy_sell_signal(stk_data)\n",
    "        ], \n",
    "        axis=1)\n",
    "\n",
    "    data = data.dropna().astype('float32')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ADOSC_3_10  KVO_34_55_13  KVOs_34_55_13  RSI_10  RSI_30  RSI_200  STOCHk_10_3_3  STOCHd_10_3_3  STOCHk_30_3_3  STOCHd_30_3_3  STOCHk_200_3_3  STOCHd_200_3_3  Signal\n",
      "Date                                                                                                                                                                               \n",
      "2014-10-22  -45839036.000  -3324833.250  -14716448.000   0.637   0.574    0.578          0.814          0.609          0.794          0.582           0.936           0.882   1.000\n",
      "2014-10-23   22155916.000   2109229.750  -12312780.000   0.690   0.599    0.582          0.927          0.793          0.926          0.776           0.974           0.932   1.000\n",
      "2014-10-24   74158712.000   4986560.500   -9841446.000   0.700   0.604    0.583          0.942          0.894          0.942          0.887           0.984           0.965   1.000\n",
      "2014-10-27   90922280.000   6489589.000   -7508440.500   0.693   0.602    0.583          0.972          0.947          0.972          0.947           0.992           0.983   1.000\n",
      "2014-10-28  150682864.000   9032005.000   -5145520.000   0.737   0.623    0.587          0.979          0.964          0.979          0.964           0.994           0.990   1.000\n",
      "2014-10-29  224504816.000  11721696.000   -2735917.500   0.751   0.631    0.589          0.987          0.979          0.987          0.979           0.996           0.994   1.000\n",
      "2014-10-30  259631424.000   6142394.500   -1467587.250   0.725   0.623    0.588          0.987          0.984          0.989          0.985           0.996           0.995   1.000\n",
      "2014-10-31  301736096.000   8368171.500     -62478.855   0.752   0.636    0.590          0.985          0.986          0.987          0.988           0.996           0.996   1.000\n",
      "2014-11-03  305768224.000  11029874.000    1522143.000   0.785   0.653    0.594          0.953          0.975          0.968          0.981           0.989           0.994   1.000\n",
      "2014-11-04  279321632.000   5382455.000    2073616.125   0.724   0.636    0.591          0.892          0.943          0.942          0.966           0.978           0.987   1.000\n",
      "2014-11-05  254919280.000   7009240.500    2778705.250   0.732   0.639    0.591          0.821          0.889          0.911          0.940           0.966           0.978   1.000\n",
      "2014-11-06  258175120.000   2267891.250    2705731.750   0.741   0.643    0.592          0.762          0.825          0.896          0.916           0.960           0.968   1.000\n",
      "2014-11-07  244389984.000   3837648.250    2867434.250   0.751   0.647    0.593          0.759          0.781          0.905          0.904           0.964           0.963   1.000\n",
      "2014-11-10  198702960.000     38128.344    2463247.750   0.733   0.642    0.592          0.732          0.751          0.904          0.901           0.963           0.963   1.000\n",
      "2014-11-11  194288176.000   1266829.000    2292330.750   0.763   0.654    0.595          0.779          0.756          0.926          0.911           0.972           0.966   1.000\n",
      "2014-11-12  224123552.000   4052795.250    2543825.750   0.806   0.674    0.598          0.845          0.785          0.951          0.927           0.981           0.972   1.000\n",
      "2014-11-13  240114400.000   7655808.000    3274108.750   0.839   0.692    0.602          0.910          0.845          0.972          0.949           0.989           0.981   1.000\n",
      "2014-11-14  280532000.000   9590901.000    4176507.750   0.862   0.707    0.606          0.955          0.903          0.985          0.969           0.994           0.988   1.000\n",
      "2014-11-17  232696848.000  11560663.000    5231387.000   0.844   0.702    0.605          0.851          0.905          0.939          0.965           0.971           0.984   1.000\n",
      "2014-11-18  234500112.000  13111714.000    6357148.000   0.868   0.718    0.609          0.821          0.876          0.923          0.949           0.962           0.976   1.000\n",
      "2014-11-19  208425872.000   7115728.500    6465516.500   0.794   0.697    0.606          0.730          0.801          0.884          0.915           0.943           0.959   1.000\n",
      "2014-11-20  203816096.000   8958157.000    6821608.000   0.827   0.714    0.610          0.808          0.786          0.919          0.908           0.960           0.955   1.000\n",
      "2014-11-21  152213056.000  11796450.000    7532300.000   0.830   0.716    0.610          0.832          0.790          0.930          0.911           0.965           0.956   1.000\n",
      "2014-11-24  169792096.000  13519033.000    8387547.500   0.865   0.737    0.615          0.919          0.853          0.967          0.938           0.983           0.969   1.000\n",
      "2014-11-25   85360984.000  16890982.000    9602324.000   0.780   0.711    0.611          0.886          0.879          0.952          0.950           0.975           0.975   1.000\n",
      "2014-11-26   86776752.000  17516052.000   10732857.000   0.808   0.725    0.615          0.897          0.901          0.959          0.959           0.978           0.979   1.000\n",
      "2014-11-28   89058616.000  16681132.000   11582610.000   0.803   0.723    0.614          0.870          0.884          0.949          0.953           0.973           0.976   1.000\n",
      "2014-12-01   76816264.000   6576986.500   10867521.000   0.558   0.635    0.600          0.755          0.841          0.905          0.938           0.955           0.969   1.000\n",
      "2014-12-02   83960120.000   -525951.438    9239882.000   0.537   0.626    0.598          0.583          0.736          0.823          0.892           0.924           0.951   1.000\n",
      "2014-12-03   96929960.000   1795444.000    8176391.000   0.587   0.641    0.602          0.465          0.601          0.761          0.830           0.903           0.927   1.000\n",
      "2014-12-04   51119184.000   3815020.500    7553338.000   0.564   0.632    0.600          0.481          0.510          0.745          0.777           0.906           0.911   1.000\n",
      "2014-12-05    3346461.250  -1244890.875    6296448.000   0.538   0.622    0.598          0.496          0.480          0.734          0.747           0.908           0.906   1.000\n",
      "2014-12-08  -52107064.000  -7500967.500    4325389.000   0.422   0.571    0.589          0.357          0.445          0.645          0.708           0.883           0.899   1.000\n",
      "2014-12-09     999797.188 -13391562.000    1794395.750   0.501   0.594    0.593          0.344          0.399          0.603          0.661           0.873           0.888   1.000\n",
      "2014-12-10  -20033808.000  -9739297.000     146725.406   0.421   0.555    0.586          0.284          0.328          0.519          0.589           0.852           0.869   1.000\n",
      "2014-12-11  -67514872.000 -13767440.000   -1841012.500   0.410   0.550    0.584          0.314          0.314          0.486          0.536           0.846           0.857   1.000\n",
      "2014-12-12 -142767904.000 -18655358.000   -4243061.500   0.351   0.519    0.578          0.174          0.257          0.350          0.452           0.815           0.838   1.000\n",
      "2014-12-15 -184854160.000 -24020442.000   -7068402.000   0.311   0.496    0.573          0.146          0.212          0.251          0.363           0.788           0.816   1.000\n",
      "2014-12-16 -243194736.000 -28298348.000  -10101251.000   0.277   0.474    0.568          0.085          0.135          0.126          0.243           0.753           0.785   1.000\n",
      "2014-12-17 -196331920.000 -22341214.000  -11849817.000   0.407   0.513    0.574          0.169          0.133          0.137          0.171           0.751           0.764   1.000\n",
      "2014-12-18  -84213608.000 -16399969.000  -12499839.000   0.523   0.555    0.582          0.328          0.194          0.248          0.170           0.783           0.762   1.000\n",
      "2014-12-19 -126143144.000  -8442795.000  -11920261.000   0.494   0.542    0.579          0.527          0.341          0.372          0.252           0.819           0.784   1.000\n",
      "2014-12-22 -115944064.000  -4899229.000  -10917257.000   0.533   0.556    0.582          0.690          0.515          0.459          0.360           0.844           0.815   1.000\n",
      "2014-12-23 -128265944.000  -7769984.000  -10467646.000   0.518   0.550    0.581          0.717          0.645          0.457          0.429           0.843           0.835   1.000\n",
      "2014-12-24 -140074752.000  -9374273.000  -10311450.000   0.497   0.542    0.579          0.757          0.722          0.462          0.459           0.845           0.844   1.000\n",
      "2014-12-26 -107378504.000  -6682256.000   -9792994.000   0.568   0.567    0.584          0.810          0.761          0.488          0.469           0.852           0.847   1.000\n",
      "2014-12-29 -105578784.000  -4745831.500   -9071971.000   0.565   0.566    0.584          0.866          0.811          0.522          0.491           0.862           0.853   1.000\n",
      "2014-12-30 -116046096.000  -7902472.500   -8904900.000   0.503   0.544    0.579          0.857          0.844          0.535          0.515           0.866           0.860   1.000\n",
      "2014-12-31 -156338080.000 -11743903.000   -9310471.000   0.424   0.512    0.572          0.694          0.806          0.446          0.501           0.840           0.856   1.000\n",
      "2015-01-02 -160760880.000 -16224032.000  -10298123.000   0.391   0.497    0.569          0.483          0.678          0.332          0.438           0.807           0.838   1.000\n",
      "2015-01-05 -187349536.000 -21213974.000  -11857530.000   0.311   0.457    0.559          0.268          0.482          0.197          0.325           0.762           0.803   1.000\n",
      "2015-01-06 -167437184.000 -25837020.000  -13854600.000   0.311   0.457    0.559          0.172          0.308          0.131          0.220           0.733           0.768   1.000\n",
      "2015-01-07 -123651904.000 -20910090.000  -14862527.000   0.386   0.479    0.562          0.186          0.209          0.124          0.151           0.722           0.739   1.000\n",
      "2015-01-08  -30331892.000 -14737193.000  -14844622.000   0.541   0.533    0.572          0.395          0.251          0.265          0.173           0.762           0.739   1.000\n",
      "2015-01-09   23191868.000  -9580393.000  -14092590.000   0.544   0.534    0.573          0.584          0.388          0.395          0.261           0.803           0.762   1.000\n",
      "2015-01-12   -5363126.500 -13730959.000  -14040928.000   0.452   0.498    0.564          0.633          0.537          0.431          0.364           0.814           0.793   0.000\n",
      "2015-01-13  -44435124.000  -7470047.000  -13102231.000   0.486   0.510    0.566          0.595          0.604          0.398          0.408           0.802           0.806   0.000\n",
      "2015-01-14  -37279728.000 -11709497.000  -12903269.000   0.472   0.504    0.565          0.552          0.594          0.369          0.399           0.786           0.801   0.000\n",
      "2015-01-15 -100156232.000 -16487816.000  -13415347.000   0.384   0.467    0.556          0.485          0.544          0.323          0.363           0.769           0.785   0.000\n",
      "2015-01-16 -150361968.000 -22384192.000  -14696610.000   0.363   0.458    0.553          0.337          0.458          0.231          0.308           0.738           0.764   0.000\n",
      "2015-01-20 -106578496.000 -16692551.000  -14981745.000   0.469   0.493    0.560          0.295          0.373          0.213          0.256           0.731           0.746   0.000\n",
      "2015-01-21  -84074448.000 -11597225.000  -14498242.000   0.497   0.504    0.562          0.391          0.341          0.316          0.253           0.750           0.740   0.000\n",
      "2015-01-22   -2242609.500  -6493498.000  -13354707.000   0.582   0.537    0.569          0.636          0.441          0.533          0.354           0.796           0.759   0.000\n",
      "2015-01-23   49552168.000  -2468443.500  -11799527.000   0.597   0.543    0.570          0.782          0.603          0.686          0.512           0.826           0.791   0.000\n",
      "2015-01-26   22677226.000   1980410.625   -9830964.000   0.601   0.545    0.570          0.889          0.769          0.804          0.674           0.852           0.825   0.000\n",
      "2015-01-27 -104255192.000  -6937408.500   -9417599.000   0.459   0.495    0.558          0.734          0.802          0.699          0.730           0.828           0.836   0.000\n",
      "2015-01-28 -333129760.000   5715597.000   -7255714.000   0.616   0.560    0.572          0.692          0.772          0.691          0.731           0.845           0.842   0.000\n",
      "2015-01-29 -306181888.000  11890064.000   -4520602.500   0.676   0.591    0.580          0.731          0.719          0.739          0.709           0.887           0.853   0.000\n",
      "2015-01-30 -352929216.000  17420304.000   -1386187.500   0.623   0.571    0.575          0.857          0.760          0.862          0.764           0.942           0.891   1.000\n",
      "2015-02-02 -288027616.000   9856933.000     219972.578   0.649   0.584    0.578          0.895          0.828          0.902          0.834           0.964           0.931   1.000\n",
      "2015-02-03 -208505312.000  12787877.000    2015387.500   0.650   0.584    0.578          0.864          0.872          0.879          0.881           0.960           0.955   1.000\n",
      "2015-02-04 -144843904.000  16989460.000    4154540.750   0.667   0.592    0.580          0.900          0.886          0.921          0.901           0.973           0.965   1.000\n",
      "2015-02-05  -83686888.000  18383070.000    6187188.000   0.683   0.599    0.582          0.918          0.894          0.939          0.913           0.979           0.971   1.000\n",
      "2015-02-06  -77019120.000  12245219.000    7052621.000   0.642   0.586    0.579          0.910          0.909          0.935          0.932           0.976           0.976   1.000\n",
      "2015-02-09  -26218104.000  13719486.000    8005030.500   0.660   0.593    0.581          0.915          0.914          0.938          0.937           0.976           0.977   1.000\n",
      "2015-02-10   65555964.000  17013468.000    9291950.000   0.707   0.614    0.585          0.925          0.916          0.948          0.940           0.979           0.977   1.000\n",
      "2015-02-11  187669296.000  20963866.000   10959367.000   0.755   0.637    0.591          0.969          0.936          0.980          0.955           0.992           0.983   1.000\n",
      "2015-02-12  214112288.000  24590950.000   12906736.000   0.777   0.649    0.595          0.962          0.952          0.982          0.970           0.991           0.987   1.000\n",
      "2015-02-13  257588448.000  26106826.000   14792463.000   0.785   0.654    0.596          0.957          0.963          0.979          0.980           0.989           0.991   1.000\n",
      "2015-02-17  246215952.000  28192914.000   16706813.000   0.796   0.659    0.597          0.927          0.949          0.965          0.975           0.982           0.987   1.000\n",
      "2015-02-18  271160352.000  28466814.000   18386814.000   0.808   0.666    0.599          0.952          0.946          0.978          0.974           0.988           0.986   1.000\n",
      "2015-02-19  225464816.000  28012374.000   19761894.000   0.792   0.662    0.598          0.946          0.942          0.975          0.973           0.987           0.986   1.000\n",
      "2015-02-20  248572336.000  28535122.000   21015212.000   0.808   0.670    0.601          0.977          0.958          0.990          0.981           0.995           0.990   1.000\n",
      "2015-02-23  325758624.000  30840080.000   22418764.000   0.852   0.696    0.607          0.982          0.968          0.992          0.986           0.996           0.992   1.000\n",
      "2015-02-24  312128928.000  32723112.000   23890814.000   0.804   0.683    0.605          0.965          0.974          0.983          0.988           0.991           0.994   1.000\n",
      "2015-02-25  218368912.000  22030656.000   23625076.000   0.641   0.633    0.595          0.820          0.922          0.927          0.967           0.959           0.982   1.000\n",
      "2015-02-26  251799120.000  10843505.000   21799138.000   0.676   0.646    0.598          0.688          0.824          0.889          0.933           0.937           0.962   1.000\n",
      "2015-02-27  178561264.000   3159290.250   19136302.000   0.598   0.619    0.592          0.508          0.672          0.846          0.887           0.912           0.936   1.000\n",
      "2015-03-02  120000712.000   5604608.500   17203204.000   0.614   0.624    0.594          0.437          0.545          0.849          0.862           0.914           0.921   1.000\n",
      "2015-03-03  122494304.000    452442.062   14810238.000   0.621   0.627    0.594          0.367          0.438          0.835          0.843           0.907           0.911   1.000\n",
      "2015-03-04   86376112.000  -3705603.750   12165117.000   0.584   0.615    0.592          0.341          0.382          0.828          0.837           0.907           0.909   1.000\n",
      "2015-03-05   23091636.000  -9608537.000    9054595.000   0.499   0.586    0.586          0.251          0.320          0.784          0.816           0.887           0.900   1.000\n",
      "2015-03-06  -77334416.000  -3867847.500    7208532.000   0.506   0.588    0.586          0.155          0.249          0.741          0.784           0.866           0.886   1.000\n",
      "2015-03-09 -120131536.000 -12472378.000    4396973.500   0.527   0.593    0.587          0.145          0.184          0.720          0.748           0.855           0.869   1.000\n",
      "2015-03-10 -177967360.000 -18583860.000    1113997.250   0.428   0.557    0.580          0.147          0.149          0.694          0.718           0.841           0.854   1.000\n",
      "2015-03-11 -264613184.000 -24109456.000   -2489353.250   0.362   0.529    0.573          0.116          0.136          0.582          0.665           0.807           0.834   1.000\n",
      "2015-03-12 -231392144.000 -19029604.000   -4852246.500   0.453   0.552    0.578          0.140          0.135          0.501          0.592           0.784           0.811   1.000\n",
      "2015-03-13 -215478944.000 -14075284.000   -6169823.000   0.427   0.541    0.575          0.186          0.148          0.433          0.505           0.774           0.788   1.000\n",
      "2015-03-16 -143721808.000 -10909571.000   -6846930.000   0.480   0.555    0.578          0.320          0.215          0.476          0.470           0.792           0.784   1.000\n",
      "2015-03-17  -58138220.000  -6710624.000   -6827457.500   0.551   0.576    0.582          0.442          0.316          0.508          0.473           0.812           0.793   1.000\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None,):\n",
    "    print(gen_analysis_data(ticks_data[0]).head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (np.array(self.X.iloc[idx, :]), int(self.Y.iloc[idx]))\n",
    "\n",
    "        # return (torch.tensor(np.array(self.X.iloc[idx, :]), dtype=torch.float32),\n",
    "        #         int(self.Y.iloc[idx]))\n",
    "\n",
    "\n",
    "def prepare_dataloader():\n",
    "    dataset = pd.concat([gen_analysis_data(d) for d in ticks_data])\n",
    "\n",
    "    test_size = int(dataset.shape[0] * validation_size)\n",
    "    random.seed(42)\n",
    "    test_data_idx = random.sample(range(0, dataset.shape[0]), test_size)\n",
    "    mask = np.full(len(dataset), False)\n",
    "    mask[test_data_idx] = True\n",
    "    train_data = dataset[~mask]\n",
    "    test_data = dataset[mask]\n",
    "\n",
    "    X_train_data = train_data.iloc[:, :-1]\n",
    "    Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "    X_test_data = test_data.iloc[:, :-1]\n",
    "    Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "    features = [\n",
    "        ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "    ]\n",
    "    features.extend([([column], None) for column in X_train_data.columns[3:].values])\n",
    "    # print(features)\n",
    "    X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "    X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "    X_test_data = X_dfm.transform(X_test_data)\n",
    " \n",
    "    train_loader = DataLoader(\n",
    "        SignalDataset(X_train_data, Y_train_data),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=pin_memory,\n",
    "        # pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        SignalDataset(X_test_data, Y_test_data),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=pin_memory,\n",
    "        # pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, X_train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class BuySellSignalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, activation_type):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for i in range(1, num_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if (out_features <= 2):\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU() if activation_type == 1 else\n",
    "                          nn.Sigmoid()) if activation_type == 2 else nn.Tanh()\n",
    "            in_features = out_features\n",
    "        layers.append(nn.Linear(in_features, 2))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        # self.soft_max = nn.LogSoftMax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x)\n",
    "        # output = self.soft_max(output)\n",
    "        return output\n",
    "    \n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        'epoch_num': epoch_num,\n",
    "        'time': str(datetime.now),\n",
    "        'model_state': model.state_dict(),\n",
    "        'input_size': model.input_size,\n",
    "        'hyper_parameters': hyper_parameters\n",
    "    }\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict['hyper_parameters']\n",
    "    model = BuySellSignalClassifier(\n",
    "        data_dict['input_size'],\n",
    "        num_layers=hyper_parameters['num_layers'],\n",
    "        activation_type=hyper_parameters['activation_type'])\n",
    "    model.load_state_dict(data_dict['model_state'])\n",
    "    return model, hyper_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval_dl_method(model, dl, device=device):\n",
    "    model.eval()\n",
    "    y_gt = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    f1 = 0\n",
    "    for x, y in dl:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += y.shape[0]\n",
    "        correct += int((predicted == y).sum())\n",
    "        f1 += f1_score(y.cpu().detach(), predicted.cpu().detach())\n",
    "\n",
    "    accuracy = correct/total\n",
    "    f1 = f1/len(dl)\n",
    "    # print(f\"Accuracy: {accuracy:.3f}, F1 score:{f1:.3f}\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "_best_loss = 999  \n",
    "_best_accuracy = 0\n",
    "def do_train(model, optimizer, train_dl, test_dl, id_str, config):\n",
    "    global _best_loss\n",
    "    global _best_accuracy\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "  \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    for epoch in tqdm(range(epoch_num), leave=True):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            accuracy, f1  = eval_dl_method(model, test_dl)\n",
    "            if accuracy > _best_accuracy:\n",
    "                _best_accuracy = accuracy\n",
    "\n",
    "            if loss.item() < _best_loss:\n",
    "                _best_loss = loss.item()\n",
    "                save_model(model, config, model_name)\n",
    "\n",
    "            train.report({\"mean_accuracy\":accuracy})\n",
    "  \n",
    "        # writer.add_scalars('Loss vs. Validation Accuracy',\n",
    "        #                     { 'Loss' : running_loss / len(train_dl), 'Validation Accuracy' : accuracy, 'F1': f1 },\n",
    "        #                     epoch + 1)\n",
    "\n",
    "        total_loss += running_loss / len(train_dl)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "        # writer.flush()\n",
    "\n",
    "    return {'Train loss':total_loss/epoch_num, 'Validation Accuracy': total_accuracy/epoch_num}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(config):\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    activation_type = config[\"activation_type\"]\n",
    "\n",
    "    id_str = f\"{lr}_{momentum}_{optim_type}_{num_layers}_{activation_type}\"\n",
    "\n",
    "    # writer = SummaryWriter(f\"{log_dir}/{id_str}\")    \n",
    "\n",
    "    train_loader, test_loader, features_size = prepare_dataloader()\n",
    "    model = BuySellSignalClassifier(features_size, num_layers, activation_type)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) if optim_type == 1 else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    #metric_dict = do_train(model, optimizer, train_loader, test_loader, id_str, config, writer)\n",
    "    do_train(model, optimizer, train_loader, test_loader, id_str, config)\n",
    "    # writer.add_hparams(\n",
    "    #     {\n",
    "    #         \"lr\": lr,\n",
    "    #         \"momentum\": momentum,\n",
    "    #         \"optim_type\": optim_type,\n",
    "    #         \"num_layers\": num_layers,\n",
    "    #         \"activation_type\": activation_type\n",
    "    #     },\n",
    "    #     metric_dict\n",
    "    # )\n",
    "    # writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pin_memory=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'SignalDataset' on <module '__main__' (built-in)>\n",
      "  0%|          | 0/100 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpin_memory=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpin_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElasped time:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# pin_memory=False\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(f\"pin_memory={pin_memory}\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# start = datetime.now()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# train_classifier(config)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(f\"Elasped time:{datetime.now() - start}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr) \u001b[38;5;28;01mif\u001b[39;00m optim_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#metric_dict = do_train(model, optimizer, train_loader, test_loader, id_str, config, writer)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36mdo_train\u001b[0;34m(model, optimizer, train_dl, test_dl, id_str, config)\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/py_env/ml4t/lib/python3.8/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_env/ml4t/lib/python3.8/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_env/ml4t/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.8/3.8.18/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_loader, test_loader, features_size = prepare_dataloader()\n",
    "# model = BuySellSignalClassifier(features_size, 3, 1)\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# time_str = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "# log_dir = f'{log_dir_base}/{time_str}'\n",
    "# id_str = f\"0.01_4_test\"\n",
    "config={\n",
    "    \"lr\":0.001,\n",
    "    \"momentum\":0.32400661960813776,\n",
    "    \"optim_type\":2,\n",
    "    \"num_layers\":4,\n",
    "    \"activation_type\":2\n",
    "}\n",
    "\n",
    "# writer = SummaryWriter(f\"{log_dir}/{id_str}\")\n",
    "# print(f\"num_workers={num_workers}\")\n",
    "pin_memory=True\n",
    "print(f\"pin_memory={pin_memory}\")\n",
    "start = datetime.now()\n",
    "train_classifier(config)\n",
    "print(f\"Elasped time:{datetime.now() - start}\")\n",
    "\n",
    "# pin_memory=False\n",
    "# print(f\"pin_memory={pin_memory}\")\n",
    "# start = datetime.now()\n",
    "# train_classifier(config)\n",
    "# print(f\"Elasped time:{datetime.now() - start}\")\n",
    "\n",
    "\n",
    "# num_workers  = 3\n",
    "# print(f\"num_workers={num_workers}\")\n",
    "# start = datetime.now()\n",
    "# train_classifier(config)\n",
    "# print(f\"Elasped time:{datetime.now() - start}\")\n",
    "\n",
    "# num_workers  = 4\n",
    "# print(f\"num_workers={num_workers}\")\n",
    "# start = datetime.now()\n",
    "# train_classifier(config)\n",
    "# print(f\"Elasped time:{datetime.now() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "log_dir = f'{log_dir_base}/{time_str}'\n",
    "config={\n",
    "    \"lr\": tune.grid_search([0.01]),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([2]),\n",
    "    \"num_layers\": tune.grid_search([3]), #[3, 4, 8]), #[1, 2, 4, 8]\n",
    "    \"activation_type\": tune.grid_search([1])# , 2, 3]) #, 2, 3])\n",
    "}\n",
    "\n",
    "train_LSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lr', 'optim_type', 'num_layers', 'activation_type']\n",
      "Total count of configs = 48\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.grid_search([0.1, 0.01]),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([1, 2]),\n",
    "    \"num_layers\": tune.grid_search([4, 8, 16, 32]), # , 48, 64, 72\n",
    "    \"activation_type\": tune.grid_search([1, 2, 3]) #, 2, 3])\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if type(v).__name__ == 'dict' and list(v.keys())[0] == 'grid_search' and len(list(v.values())[0]) > 1:\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 22:40:50,643\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-27 22:45:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:41.40        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.0/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0.9600000000000004/32 CPUs, 0.9600000000000004/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  activation_type</th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  optim_type</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_classifier_2d446_00000</td><td>RUNNING </td><td>192.168.0.125:2171667</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.599319</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.937203</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.246</td></tr>\n",
       "<tr><td>train_classifier_2d446_00001</td><td>RUNNING </td><td>192.168.0.125:2171668</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.427529</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.939364</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         274.368</td></tr>\n",
       "<tr><td>train_classifier_2d446_00002</td><td>RUNNING </td><td>192.168.0.125:2171669</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.541638</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.940661</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         277.517</td></tr>\n",
       "<tr><td>train_classifier_2d446_00003</td><td>RUNNING </td><td>192.168.0.125:2171672</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.448901</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.937743</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.821</td></tr>\n",
       "<tr><td>train_classifier_2d446_00004</td><td>RUNNING </td><td>192.168.0.125:2171671</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.335573</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.943147</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         272.96 </td></tr>\n",
       "<tr><td>train_classifier_2d446_00005</td><td>RUNNING </td><td>192.168.0.125:2171670</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.858763</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.939364</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         277.472</td></tr>\n",
       "<tr><td>train_classifier_2d446_00006</td><td>RUNNING </td><td>192.168.0.125:2171673</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.710885</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.937527</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.753</td></tr>\n",
       "<tr><td>train_classifier_2d446_00007</td><td>RUNNING </td><td>192.168.0.125:2171674</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.212091</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.941742</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         273.794</td></tr>\n",
       "<tr><td>train_classifier_2d446_00008</td><td>RUNNING </td><td>192.168.0.125:2171675</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.794774</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.889754</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.322</td></tr>\n",
       "<tr><td>train_classifier_2d446_00009</td><td>RUNNING </td><td>192.168.0.125:2171676</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.489945</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.94131 </td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         277.116</td></tr>\n",
       "<tr><td>train_classifier_2d446_00010</td><td>RUNNING </td><td>192.168.0.125:2171677</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.815642</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.940013</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         273.966</td></tr>\n",
       "<tr><td>train_classifier_2d446_00011</td><td>RUNNING </td><td>192.168.0.125:2171680</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.739884</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.937203</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.5  </td></tr>\n",
       "<tr><td>train_classifier_2d446_00012</td><td>RUNNING </td><td>192.168.0.125:2171682</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.440171</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.927583</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         277.503</td></tr>\n",
       "<tr><td>train_classifier_2d446_00013</td><td>RUNNING </td><td>192.168.0.125:2171685</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.117975</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.943688</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         274.73 </td></tr>\n",
       "<tr><td>train_classifier_2d446_00014</td><td>RUNNING </td><td>192.168.0.125:2171688</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.314942</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.943256</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         277.323</td></tr>\n",
       "<tr><td>train_classifier_2d446_00015</td><td>RUNNING </td><td>192.168.0.125:2171690</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.533307</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">0.941094</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         276.398</td></tr>\n",
       "<tr><td>train_classifier_2d446_00016</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.606783</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00017</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.30631 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00018</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.211485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00019</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.767944</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00020</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.887522</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00021</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.520552</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00022</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.237343</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00023</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.317846</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00024</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.114713</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00025</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.831439</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00026</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.194201</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00027</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.561213</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00028</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.319244</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00029</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.543342</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00030</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.621136</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00031</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.763793</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00032</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.265137</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00033</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.108797</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00034</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.209509</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00035</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.820015</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00036</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.799112</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00037</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.57793 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00038</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.580413</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00039</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.632029</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00040</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.240297</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00041</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.83153 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00042</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.435016</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00043</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.406511</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00044</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.1 </td><td style=\"text-align: right;\">  0.515134</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00045</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.137573</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00046</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.233027</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_classifier_2d446_00047</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.690427</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  mean_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_classifier_2d446_00000</td><td style=\"text-align: right;\">       0.937203</td></tr>\n",
       "<tr><td>train_classifier_2d446_00001</td><td style=\"text-align: right;\">       0.939364</td></tr>\n",
       "<tr><td>train_classifier_2d446_00002</td><td style=\"text-align: right;\">       0.940661</td></tr>\n",
       "<tr><td>train_classifier_2d446_00003</td><td style=\"text-align: right;\">       0.937743</td></tr>\n",
       "<tr><td>train_classifier_2d446_00004</td><td style=\"text-align: right;\">       0.943147</td></tr>\n",
       "<tr><td>train_classifier_2d446_00005</td><td style=\"text-align: right;\">       0.939364</td></tr>\n",
       "<tr><td>train_classifier_2d446_00006</td><td style=\"text-align: right;\">       0.937527</td></tr>\n",
       "<tr><td>train_classifier_2d446_00007</td><td style=\"text-align: right;\">       0.941742</td></tr>\n",
       "<tr><td>train_classifier_2d446_00008</td><td style=\"text-align: right;\">       0.889754</td></tr>\n",
       "<tr><td>train_classifier_2d446_00009</td><td style=\"text-align: right;\">       0.94131 </td></tr>\n",
       "<tr><td>train_classifier_2d446_00010</td><td style=\"text-align: right;\">       0.940013</td></tr>\n",
       "<tr><td>train_classifier_2d446_00011</td><td style=\"text-align: right;\">       0.937203</td></tr>\n",
       "<tr><td>train_classifier_2d446_00012</td><td style=\"text-align: right;\">       0.927583</td></tr>\n",
       "<tr><td>train_classifier_2d446_00013</td><td style=\"text-align: right;\">       0.943688</td></tr>\n",
       "<tr><td>train_classifier_2d446_00014</td><td style=\"text-align: right;\">       0.943256</td></tr>\n",
       "<tr><td>train_classifier_2d446_00015</td><td style=\"text-align: right;\">       0.941094</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 22:45:32,051\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-01-27 22:45:36,585\tINFO tune.py:1042 -- Total run time: 285.94 seconds (281.38 seconds for the tuning loop).\n",
      "2024-01-27 22:45:36,585\tWARNING tune.py:1057 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-01-27 22:45:36,603\tWARNING experiment_analysis.py:193 -- Failed to fetch metrics for 32 trial(s):\n",
      "- train_classifier_2d446_00016: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00016: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00016_16_activation_type=2,lr=0.0100,momentum=0.6068,num_layers=16,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00017: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00017: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00017_17_activation_type=3,lr=0.0100,momentum=0.3063,num_layers=16,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00018: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00018: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00018_18_activation_type=1,lr=0.1000,momentum=0.2115,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00019: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00019: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00019_19_activation_type=2,lr=0.1000,momentum=0.7679,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00020: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00020: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00020_20_activation_type=3,lr=0.1000,momentum=0.8875,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00021: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00021: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00021_21_activation_type=1,lr=0.0100,momentum=0.5206,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00022: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00022: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00022_22_activation_type=2,lr=0.0100,momentum=0.2373,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00023: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00023: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00023_23_activation_type=3,lr=0.0100,momentum=0.3178,num_layers=32,optim_type=1_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00024: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00024: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00024_24_activation_type=1,lr=0.1000,momentum=0.1147,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00025: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00025: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00025_25_activation_type=2,lr=0.1000,momentum=0.8314,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00026: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00026: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00026_26_activation_type=3,lr=0.1000,momentum=0.1942,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00027: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00027: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00027_27_activation_type=1,lr=0.0100,momentum=0.5612,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00028: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00028: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00028_28_activation_type=2,lr=0.0100,momentum=0.3192,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00029: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00029: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00029_29_activation_type=3,lr=0.0100,momentum=0.5433,num_layers=4,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00030: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00030: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00030_30_activation_type=1,lr=0.1000,momentum=0.6211,num_layers=8,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00031: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00031: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00031_31_activation_type=2,lr=0.1000,momentum=0.7638,num_layers=8,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00032: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00032: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00032_32_activation_type=3,lr=0.1000,momentum=0.2651,num_layers=8,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00033: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00033: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00033_33_activation_type=1,lr=0.0100,momentum=0.1088,num_layers=8,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00034: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00034: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00034_34_activation_type=2,lr=0.0100,momentum=0.2095,num_layers=8,optim_type=2_2024-01-27_22-40-51')\n",
      "- train_classifier_2d446_00035: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00035: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00035_35_activation_type=3,lr=0.0100,momentum=0.8200,num_layers=8,optim_type=2_2024-01-27_22-40-52')\n",
      "- train_classifier_2d446_00036: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00036: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00036_36_activation_type=1,lr=0.1000,momentum=0.7991,num_layers=16,optim_type=2_2024-01-27_22-40-52')\n",
      "- train_classifier_2d446_00037: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00037: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00037_37_activation_type=2,lr=0.1000,momentum=0.5779,num_layers=16,optim_type=2_2024-01-27_22-40-52')\n",
      "- train_classifier_2d446_00038: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00038: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00038_38_activation_type=3,lr=0.1000,momentum=0.5804,num_layers=16,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00039: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00039: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00039_39_activation_type=1,lr=0.0100,momentum=0.6320,num_layers=16,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00040: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00040: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00040_40_activation_type=2,lr=0.0100,momentum=0.2403,num_layers=16,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00041: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00041: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00041_41_activation_type=3,lr=0.0100,momentum=0.8315,num_layers=16,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00042: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00042: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00042_42_activation_type=1,lr=0.1000,momentum=0.4350,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00043: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00043: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00043_43_activation_type=2,lr=0.1000,momentum=0.4065,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00044: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00044: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00044_44_activation_type=3,lr=0.1000,momentum=0.5151,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00045: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00045: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00045_45_activation_type=1,lr=0.0100,momentum=0.1376,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00046: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00046: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00046_46_activation_type=2,lr=0.0100,momentum=0.2330,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n",
      "- train_classifier_2d446_00047: FileNotFoundError('Could not fetch metrics for train_classifier_2d446_00047: both result.json and progress.csv were not found at /home/skchen/ray_results/train_classifier_2024-01-27_22-40-50/train_classifier_2d446_00047_47_activation_type=3,lr=0.0100,momentum=0.6904,num_layers=32,optim_type=2_2024-01-27_22-40-53')\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "log_dir = f'{log_dir_base}/{time_str}'\n",
    "os.makedirs(log_dir)\n",
    "analysis = tune.run(train_classifier, \n",
    "                    config=search_space,\n",
    "                    resources_per_trial={'cpu':0.1, 'gpu':0.1},\n",
    "                    metric=\"mean_accuracy\",\n",
    "                    mode=\"max\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_accuracy     trial_id\n",
      "0           0.933  9dbc7_00000\n",
      "1           0.933  9dbc7_00001\n",
      "2           0.930  9dbc7_00002\n",
      "3           0.935  9dbc7_00003\n",
      "4           0.937  9dbc7_00004\n",
      "..            ...          ...\n",
      "79          0.917  9dbc7_00079\n",
      "80          0.910  9dbc7_00080\n",
      "81          0.911  9dbc7_00081\n",
      "82          0.821  9dbc7_00082\n",
      "83          0.901  9dbc7_00083\n",
      "\n",
      "[84 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "trial_list = list(analysis.trial_dataframes.values())\n",
    "for i, trial in enumerate(trial_list):\n",
    "    if trial.empty == False:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": trial.describe().loc['mean', 'mean_accuracy'], \"trial_id\": trial.loc[0:0,'trial_id'] })\n",
    "    else:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": [np.NaN], \"trial_id\": [np.NaN]})\n",
    "    accuracy_list.append(d)\n",
    "accuracy_df = pd.concat(accuracy_list)\n",
    "accuracy_df = accuracy_df.reset_index().loc[:, [\"mean_accuracy\", \"trial_id\"]]\n",
    "print(accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lr  momentum  optim_type  num_layers  activation_type\n",
      "0  0.100     0.247           1           4                1\n",
      "1  0.100     0.343           1           4                2\n",
      "2  0.100     0.520           1           4                3\n",
      "3  0.010     0.446           1           4                1\n",
      "4  0.010     0.333           1           4                2\n",
      "..   ...       ...         ...         ...              ...\n",
      "79 0.100     0.717           2          72                2\n",
      "80 0.100     0.495           2          72                3\n",
      "81 0.010     0.518           2          72                1\n",
      "82 0.010     0.442           2          72                2\n",
      "83 0.010     0.120           2          72                3\n",
      "\n",
      "[84 rows x 5 columns]\n",
      "    mean_accuracy     trial_id    lr  momentum  optim_type  num_layers  activation_type\n",
      "0           0.933  9dbc7_00000 0.100     0.247           1           4                1\n",
      "1           0.933  9dbc7_00001 0.100     0.343           1           4                2\n",
      "2           0.930  9dbc7_00002 0.100     0.520           1           4                3\n",
      "3           0.935  9dbc7_00003 0.010     0.446           1           4                1\n",
      "4           0.937  9dbc7_00004 0.010     0.333           1           4                2\n",
      "..            ...          ...   ...       ...         ...         ...              ...\n",
      "79          0.917  9dbc7_00079 0.100     0.717           2          72                2\n",
      "80          0.910  9dbc7_00080 0.100     0.495           2          72                3\n",
      "81          0.911  9dbc7_00081 0.010     0.518           2          72                1\n",
      "82          0.821  9dbc7_00082 0.010     0.442           2          72                2\n",
      "83          0.901  9dbc7_00083 0.010     0.120           2          72                3\n",
      "\n",
      "[84 rows x 7 columns]\n",
      "    mean_accuracy     trial_id    lr  momentum  optim_type  num_layers  activation_type\n",
      "19          0.937  9dbc7_00019 0.100     0.873           1          32                2\n",
      "29          0.937  9dbc7_00029 0.010     0.307           1          48                3\n",
      "10          0.937  9dbc7_00010 0.010     0.728           1           8                2\n",
      "40          0.937  9dbc7_00040 0.010     0.837           1          72                2\n",
      "22          0.937  9dbc7_00022 0.010     0.178           1          32                2\n",
      "..            ...          ...   ...       ...         ...         ...              ...\n",
      "76          0.835  9dbc7_00076 0.010     0.671           2          64                2\n",
      "46          0.828  9dbc7_00046 0.010     0.317           2           4                2\n",
      "82          0.821  9dbc7_00082 0.010     0.442           2          72                2\n",
      "70          0.803  9dbc7_00070 0.010     0.360           2          48                2\n",
      "64          0.779  9dbc7_00064 0.010     0.193           2          32                2\n",
      "\n",
      "[84 rows x 7 columns]\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/BuySellSignalPrediction/2024-01-26_22.45.10/0.1_0.8725056264596475_1_32_2.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BuySellSignalPrediction.pt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "config_df = pd.DataFrame(analysis.get_all_configs().values())\n",
    "print(config_df)\n",
    "\n",
    "results = pd.concat([accuracy_df, config_df], axis=1)\n",
    "print(results)\n",
    "\n",
    "sorted_results = results.sort_values(by=\"mean_accuracy\", ascending=False)\n",
    "print(sorted_results.head(100))\n",
    "sorted_results_file = f\"{get_filename_of_ipynb()}_sorted_results.csv\"\n",
    "sorted_results.to_csv(sorted_results_file)\n",
    "\n",
    "best_config = config_df.iloc[sorted_results.index[0]]\n",
    "id_str = \"_\".join(str(v) if v < 1 else f'{v:g}' for v in best_config.to_list())\n",
    "best_model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "print(best_model_name)\n",
    "shutil.copy(best_model_name, f\"{get_filename_of_ipynb()}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLsAAADTCAYAAABp7hHfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3SUlEQVR4nO3deXhU5fn/8c9kDyGEJYEQCAlrkF2iYBChKmWVL6BQXBNAUSu0KIIV2hIEL0L1S4CKBb2+EERAEI2gglhWtQUXNi22bGEvCXuAgIQk8/z+8JfRMQvJZCYzmbxf13UumHOec879PGdyn5l7zpyxGGOMAAAAAAAAAC/g4+4AAAAAAAAAAGeh2AUAAAAAAACvQbELAAAAAAAAXoNiFwAAAAAAALwGxS4AAAAAAAB4DYpdAAAAAAAA8BoUuwAAAAAAAOA1KHYBAAAAAADAa1DsAgAAAAAAgNeg2AU4YMSIEYqNjXXLvqdOnSqLxeKWfQNAdUbuB4DqifwPVD0Uu4ASnDp1SlOnTtWePXsqfd/Xrl3T1KlTtXXr1krfd1nNmDFDq1evdncY5ZKTk6Pk5GT17dtXdevWlcVi0eLFi90dFgAPQu4vXVXM/d98843Gjh2rtm3bKiQkRE2aNNFvfvMbHThwwN2hAfAg5P/SVcX8//3332vYsGFq1qyZatSoofDwcPXo0UMfffSRu0NDJbAYY4y7gwA80Y4dO3T77bcrLS1NI0aMsFuWl5cnq9WqwMBAl+z73LlzioiIUHJysqZOnWq3LD8/X/n5+QoKCnLJvsuqZs2aGjp0aJUqFh09elRNmzZVkyZN1KxZM23durXY4wug+iL3l64q5v6hQ4fqn//8p4YNG6YOHTooKytL8+bNU05Ojr788ku1a9fO3SEC8ADk/9JVxfy/bt06/fWvf1VCQoKioqJ07do1vf/++/riiy/0xhtv6Mknn3R3iHAhP3cHAFRF/v7+btu3n5+f/Pz403VEw4YNlZmZqcjISNsLGgAoK3J/1TR+/HgtX75cAQEBtnnDhw9X+/btNXPmTC1dutSN0QGoCsj/VVP//v3Vv39/u3ljx45VfHy8UlNTKXZ5Ob7GCK9y7NgxPfPMM4qLi1NwcLDq1aunYcOG6ejRo0XaZmdn67nnnlNsbKwCAwPVuHFjJSYm6ty5c9q6dautEDJy5EhZLBa7r7z9/Hv7eXl5qlu3rkaOHFlkH5cvX1ZQUJAmTJggSbpx44amTJmi+Ph4hYWFKSQkRHfddZe2bNliW+fo0aOKiIiQJL300ku2fRd+ylPc9/bz8/M1ffp0NW/eXIGBgYqNjdXkyZOVm5tr1y42Nlb33Xef/vGPf6hLly4KCgpSs2bNtGTJknKNs8Vi0dWrV/XWW2/Z4hsxYoS2bNkii8WiDz74oMg6y5cvl8Vi0fbt221jWLNmTR0+fFh9+vRRSEiIoqKiNG3aNP3yglOr1ao5c+aobdu2CgoKUoMGDfTUU0/p4sWL5Yo7MDBQkZGR5VoHgOcj95P7S9OtWze7QpcktWzZUm3bttV//vOfcm0LgGch/5P/y8vX11fR0dHKzs6u8Lbg4QzgRVatWmU6duxopkyZYt58800zefJkU6dOHRMTE2OuXr1qa3flyhXTrl074+vra0aPHm3mz59vpk+fbm6//Xaze/duk5WVZaZNm2YkmSeffNK8/fbb5u233zYZGRnGGGOSkpJMTEyMbXujRo0ytWvXNrm5uXbxvPXWW0aS+eabb4wxxpw9e9Y0bNjQjB8/3syfP9+88sorJi4uzvj7+5vdu3cbY4zJyckx8+fPN5LMkCFDbPv+9ttvjTHGJCcnm1/+6SYlJRlJZujQoeb11183iYmJRpIZPHiwXbuYmBgTFxdnGjRoYCZPnmzmzZtnOnfubCwWi9m7d2+Zx/ntt982gYGB5q677rLFt23bNmO1Wk10dLR54IEHiqzTv39/07x5c7uYg4KCTMuWLc1jjz1m5s2bZ+677z4jyfz5z3+2W/eJJ54wfn5+ZvTo0WbBggXmD3/4gwkJCTG33367uXHjRpnj/rlvvvnGSDJpaWkOrQ/Ac5D7yf3lZbVaTaNGjUzv3r0rtB0A7kX+J/+XRU5Ojjl79qw5dOiQSU1NNb6+vubhhx8u93ZQtVDsgle5du1akXnbt283ksySJUts86ZMmWIkmfT09CLtrVarMab0YsgvT3iffvqpkWQ++ugju3b9+/c3zZo1sz3Oz88vclK8ePGiadCggRk1apRt3tmzZ40kk5ycXGTfvzzh7dmzx0gyTzzxhF27CRMmGElm8+bNtnkxMTFGkvn8889t886cOWMCAwPN888/X2RfpQkJCTFJSUlF5k+aNMkEBgaa7Oxsu334+fnZ9afwJP273/3ONs9qtZoBAwaYgIAAc/bsWWOMMV988YWRZJYtW2a3n/Xr1xc7v6wodgHeg9z/E3J/2bz99ttGklm4cGGFtgPAvcj/PyH/l+ypp54ykowk4+PjY4YOHWouXLhQ7u2gauFrjPAqwcHBtv/n5eXp/PnzatGihWrXrq1du3bZlr3//vvq2LGjhgwZUmQbjvy07z333KPw8HCtXLnSNu/ixYvasGGDhg8fbpvn6+tr+yqF1WrVhQsXlJ+fr9tuu80uvvJYt26dpB/vSfJzzz//vCRp7dq1dvPbtGmju+66y/Y4IiJCcXFxOnz4sEP7/6XExETl5ubqvffes81buXKl8vPz9eijjxZpP3bsWNv/LRaLxo4dqxs3bmjjxo2SpFWrViksLEy//vWvde7cOdsUHx+vmjVr2l0GDqB6Ivf/hNx/c/v27dOYMWOUkJCgpKQkh7cDwP3I/z8h/5fs2Wef1YYNG/TWW2+pX79+Kigo0I0bNxzoLaoSil3wKj/88IOmTJmi6OhoBQYGKjw8XBEREcrOztalS5ds7TIyMpz660t+fn564IEHtGbNGtt35dPT05WXl2d3wpOkt956Sx06dFBQUJDq1auniIgIrV271i6+8jh27Jh8fHzUokULu/mRkZGqXbu2jh07Zje/SZMmRbZRp04dp3wHXpJat26t22+/XcuWLbPNW7Zsme64444iMfr4+KhZs2Z281q1aiVJtnstHDx4UJcuXVL9+vUVERFhN+Xk5OjMmTNOiRtA1UXu/wm5v3RZWVkaMGCAwsLC9N5778nX19eh7QDwDOT/n5D/S4+xV69eSkxM1Mcff6ycnBwNHDiwyL3C4F34WQd4ld/97ndKS0vTs88+q4SEBIWFhclisejBBx+U1Wp16b4ffPBBvfHGG/rkk080ePBgvfvuu2rdurU6duxoa7N06VKNGDFCgwcP1sSJE1W/fn35+voqJSVFGRkZFdp/WT+VKumFvTOTfWJiosaNG6eTJ08qNzdXX375pebNm+fQtqxWq+rXr293Av25wht6Aqi+yP03R+6XLl26pH79+ik7O1tffPGFoqKiHIoNgOcg/98c+b+ooUOH6qmnntKBAwcUFxdX4e3BM1Hsgld57733lJSUpFmzZtnmXb9+vcivbTRv3lx79+4tdVvlvaS5R48eatiwoVauXKnu3btr8+bN+uMf/1gkvmbNmik9Pd1u+8nJyQ7vOyYmRlarVQcPHtQtt9xim3/69GllZ2crJiamXP0oq9JifPDBBzV+/Hi98847+uGHH+Tv71/kUy7px5PZ4cOHbZ/oSNKBAwckyfaLN82bN9fGjRt155132l2qDgCFyP3k/pu5fv26Bg4cqAMHDmjjxo1q06ZNhbcJwP3I/+R/R/zwww+S5PDVdaga+BojvIqvr2+RTylee+01FRQU2M174IEH9O233xb7M7mF64eEhEhSmX+W1sfHR0OHDtVHH32kt99+W/n5+UWSfOEnKz+P8auvvrL9JG+hGjVqlHnf/fv3lyTNmTPHbn5qaqokacCAAWWKv7xCQkJKjC88PFz9+vXT0qVLtWzZMvXt21fh4eHFtv35pz7GGM2bN0/+/v669957JUm/+c1vVFBQoOnTpxdZNz8/n58NBkDu/xlyf1EFBQUaPny4tm/frlWrVikhIaHM6wLwbOT/n5D/iyruK495eXlasmSJgoOD+eDDy3FlF7zKfffdp7ffflthYWFq06aNtm/fro0bN6pevXp27SZOnKj33ntPw4YN06hRoxQfH68LFy7oww8/1IIFC9SxY0c1b95ctWvX1oIFCxQaGqqQkBB17dpVTZs2LXH/w4cP12uvvabk5GS1b9/e7tOWwvjS09M1ZMgQDRgwQEeOHNGCBQvUpk0b5eTk2NoVJt+VK1eqVatWqlu3rtq1a1fsvQY6duyopKQkvfnmm8rOzlbPnj319ddf66233tLgwYN19913V3BUixcfH6+NGzcqNTVVUVFRatq0qbp27WpbnpiYqKFDh0pSsScrSQoKCtL69euVlJSkrl276pNPPtHatWs1efJk2yXKPXv21FNPPaWUlBTt2bNHvXv3lr+/vw4ePKhVq1Zp7ty5tv2Uxbx585Sdna1Tp05Jkj766COdPHlS0o+XwoeFhTk0HgDch9xP7i/N888/rw8//FADBw7UhQsXtHTpUrvlxd1AGUDVQP4n/5fmqaee0uXLl9WjRw81atRIWVlZWrZsmfbt26dZs2apZs2aFRwVeLTK/wFIwHUuXrxoRo4cacLDw03NmjVNnz59zL59+0xMTEyRn8o9f/68GTt2rGnUqJEJCAgwjRs3NklJSebcuXO2NmvWrDFt2rQxfn5+dj9F/MufHy5ktVpNdHS0kWRefvnlYpfPmDHDxMTEmMDAQHPrrbeajz/+uNjtbdu2zcTHx5uAgAC7nyL+5c8PG2NMXl6eeemll0zTpk2Nv7+/iY6ONpMmTTLXr1+3axcTE2MGDBhQJK6ePXuanj17Fj+oJdi3b5/p0aOHCQ4ONpKKjG9ubq6pU6eOCQsLMz/88EOR9ZOSkkxISIjJyMgwvXv3NjVq1DANGjQwycnJpqCgoEj7N99808THx5vg4GATGhpq2rdvb1544QVz6tSpcsVd+BPMxU1Hjhwp17YAeAZyP7m/ND179iwx7/NSGKjayP/k/9K88847plevXqZBgwbGz8/P1KlTx/Tq1cusWbOmXH1H1WQxhp8gAOB8+fn5ioqK0sCBA7Vw4cIiy0eMGKH33nvP7lMtAEDVRu4HgOqJ/A9Pwz27ALjE6tWrdfbsWSUmJro7FABAJSH3A0D1RP6Hp+GeXQDsZGVllbo8ODi41PtaffXVV/ruu+80ffp03XrrrerZs6ezQywiJyfnpp8SRURElPjTywBQ3ZH7AaB6Iv/DW1HsAmCnYcOGpS5PSkrS4sWLS1w+f/58LV26VJ06dSq1nTP97//+r1566aVS2xw5csT2k8YAAHvkfgConsj/8FbcswuAnY0bN5a6PCoqyuN+pvfw4cM6fPhwqW26d++uoKCgSooIAKoWcj8AVE/kf3gril0AAAAAAADwGtygHgAAAAAAAF6De3YVw2q16tSpUwoNDZXFYnF3OACAEhhjdOXKFUVFRcnHp2Kf35D7AaDqIP8DQPVU1vxPsasYp06dUnR0tLvDAACU0YkTJ9S4ceMKbYPcDwBVD/kfAKqnm+V/il3FCA0NlfTj4NWqVcvN0QAASnL58mVFR0fb8nZFkPsBoOog/wNA9VTW/E+xqxiFly/XqlWLEx4AVAHO+NoJuR8Aqh7yPwBUTzfL/9ygHgAAAAAAAF6DYhcAAAAAAAC8BsUuAAAAAAAAeA2KXQAAAAAAAPAa3KAeAAAAgNPFvri2xGVHZw6oxEgAANUNV3YBAAAAAADAa1DsAgAAAAAAgNeg2AUAAAAAAACvQbELAAAAAAAAXoNiFwAAAAAAALwGxS4AAAAAAAB4DYpdAAAAAAAA8BpuLXZ9/vnnGjhwoKKiomSxWLR69Wq75cYYTZkyRQ0bNlRwcLB69eqlgwcP3nS7r7/+umJjYxUUFKSuXbvq66+/dlEPAAAAAAAA4EncWuy6evWqOnbsqNdff73Y5a+88or++te/asGCBfrqq68UEhKiPn366Pr16yVuc+XKlRo/frySk5O1a9cudezYUX369NGZM2dc1Q0AAAAAAAB4CLcWu/r166eXX35ZQ4YMKbLMGKM5c+boT3/6kwYNGqQOHTpoyZIlOnXqVJErwH4uNTVVo0eP1siRI9WmTRstWLBANWrU0KJFi1zYEwAAAAAAAHgCj71n15EjR5SVlaVevXrZ5oWFhalr167avn17sevcuHFDO3futFvHx8dHvXr1KnEdScrNzdXly5ftJgCAdyP3A0D1RP4HAO/nscWurKwsSVKDBg3s5jdo0MC27JfOnTungoKCcq0jSSkpKQoLC7NN0dHRFYweAODpyP0AUD2R/wHA+3lssasyTZo0SZcuXbJNJ06ccHdIAAAXI/cDQPVE/gcA7+fn7gBKEhkZKUk6ffq0GjZsaJt/+vRpderUqdh1wsPD5evrq9OnT9vNP336tG17xQkMDFRgYGDFgwYAVBnkfgConsj/AOD9PPbKrqZNmyoyMlKbNm2yzbt8+bK++uorJSQkFLtOQECA4uPj7daxWq3atGlTiesAAAAAAADAe7j1yq6cnBwdOnTI9vjIkSPas2eP6tatqyZNmujZZ5/Vyy+/rJYtW6pp06b685//rKioKA0ePNi2zr333qshQ4Zo7NixkqTx48crKSlJt912m7p06aI5c+bo6tWrGjlyZGV3DwAAAAAAAJXMrcWuHTt26O6777Y9Hj9+vCQpKSlJixcv1gsvvKCrV6/qySefVHZ2trp3767169crKCjItk5GRobOnTtnezx8+HCdPXtWU6ZMUVZWljp16qT169cXuWk9AAAAAAAAvI/FGGPcHYSnuXz5ssLCwnTp0iXVqlXL3eEAAErgzHxN7gcA54p9cW2Jy47OHFChbZP/AaB6KmvO9th7dgEAAAAAAADlRbELAAAAAAAAXoNiFwAAAAAAALwGxS4AAAAAAAB4DYpdAAAAAAAA8BoUuwAAAAAAAOA1KHYBAAAAAADAa1DsAgAAAAAAgNeg2AUAAAAAAACvQbELAAAAAAAAXoNiFwAAAAAAALwGxS4AAAAAAAB4DYpdAAAAAAAA8BoUuwAAAAAAAOA1HCp2HT582NlxAAAAAAAAABXmULGrRYsWuvvuu7V06VJdv37d2TEBAAAAAAAADnGo2LVr1y516NBB48ePV2RkpJ566il9/fXXzo4NAAAAAAAAKBeHil2dOnXS3LlzderUKS1atEiZmZnq3r272rVrp9TUVJ09e9bZcQIAAAAAAAA3VaEb1Pv5+en+++/XqlWr9Je//EWHDh3ShAkTFB0drcTERGVmZjorTgAAAAAAAOCmKlTs2rFjh5555hk1bNhQqampmjBhgjIyMrRhwwadOnVKgwYNclacAAAAAAAAwE35ObJSamqq0tLStH//fvXv319LlixR//795ePzY+2sadOmWrx4sWJjY50ZKwAAAAAAAFAqh67smj9/vh5++GEdO3ZMq1ev1n333WcrdBWqX7++Fi5cWOEAY2NjZbFYikxjxowptv3ixYuLtA0KCqpwHAAAAAAAAPB8Dl3ZdfDgwZu2CQgIUFJSkiObt/PNN9+ooKDA9njv3r369a9/rWHDhpW4Tq1atbR//37bY4vFUuE4AAAAAAAA4PkcKnalpaWpZs2aRQpOq1at0rVr15xS5CoUERFh93jmzJlq3ry5evbsWeI6FotFkZGRTosBAAAAAAAAVYNDX2NMSUlReHh4kfn169fXjBkzKhxUSW7cuKGlS5dq1KhRpV6tlZOTo5iYGEVHR2vQoEH6/vvvS91ubm6uLl++bDcBALwbuR8AqifyPwB4P4eKXcePH1fTpk2LzI+JidHx48crHFRJVq9erezsbI0YMaLENnFxcVq0aJHWrFmjpUuXymq1qlu3bjp58mSJ66SkpCgsLMw2RUdHuyB6AIAnIfcDQPVE/gcA7+dQsat+/fr67rvvisz/9ttvVa9evQoHVZKFCxeqX79+ioqKKrFNQkKCEhMT1alTJ/Xs2VPp6emKiIjQG2+8UeI6kyZN0qVLl2zTiRMnXBE+AMCDkPsBoHoi/wOA93Ponl0PPfSQfv/73ys0NFQ9evSQJH322WcaN26cHnzwQacGWOjYsWPauHGj0tPTy7Wev7+/br31Vh06dKjENoGBgQoMDKxoiACAKoTcDwDVE/kfALyfQ8Wu6dOn6+jRo7r33nvl5/fjJqxWqxITE112z660tDTVr19fAwYMKNd6BQUF+te//qX+/fu7JC4AAAAAAAB4DoeKXQEBAVq5cqWmT5+ub7/9VsHBwWrfvr1iYmKcHZ+kHwtpaWlpSkpKshXXCiUmJqpRo0ZKSUmRJE2bNk133HGHWrRooezsbL366qs6duyYnnjiCZfEBgAAAAAAAM/hULGrUKtWrdSqVStnxVKijRs36vjx4xo1alSRZcePH5ePz0+3Hrt48aJGjx6trKws1alTR/Hx8dq2bZvatGnj8jgBAAAAAADgXg4VuwoKCrR48WJt2rRJZ86ckdVqtVu+efNmpwRXqHfv3jLGFLts69atdo9nz56t2bNnO3X/AAAAAAAAqBocKnaNGzdOixcv1oABA9SuXTtZLBZnxwUAAAAAAACUm0PFrhUrVujdd9/lpu8AAAAAAADwKD43b1JUQECAWrRo4exYAAAAAAAAgApxqNj1/PPPa+7cuSXeRwsAAAAAAABwB4e+xviPf/xDW7Zs0SeffKK2bdvK39/fbnl6erpTggMAAAAAAADKw6FiV+3atTVkyBBnxwIAAAAAAABUiEPFrrS0NGfHAQAAAAAAAFSYQ/fskqT8/Hxt3LhRb7zxhq5cuSJJOnXqlHJycpwWHAAAAAAAAFAeDl3ZdezYMfXt21fHjx9Xbm6ufv3rXys0NFR/+ctflJubqwULFjg7TgAAAAAAAOCmHLqya9y4cbrtttt08eJFBQcH2+YPGTJEmzZtclpwAAAAAAAAQHk4dGXXF198oW3btikgIMBufmxsrP773/86JTAAAAAAAACgvBy6sstqtaqgoKDI/JMnTyo0NLTCQQEAAAAAAACOcKjY1bt3b82ZM8f22GKxKCcnR8nJyerfv7+zYgMAAAAAAADKxaGvMc6aNUt9+vRRmzZtdP36dT388MM6ePCgwsPD9c477zg7RgAAAAAAAKBMHCp2NW7cWN9++61WrFih7777Tjk5OXr88cf1yCOP2N2wHgAAAAAAAKhMDhW7JMnPz0+PPvqoM2MBAAAAAAAAKsShYteSJUtKXZ6YmOhQMAAAAAAAAEBFOFTsGjdunN3jvLw8Xbt2TQEBAapRowbFLgAAAAAAALiFQ7/GePHiRbspJydH+/fvV/fu3blBPQAAAAAAANzGoWJXcVq2bKmZM2cWueoLAAAAAAAAqCxOK3ZJP960/tSpU07b3tSpU2WxWOym1q1bl7rOqlWr1Lp1awUFBal9+/Zat26d0+IBAAAAAACAZ3Ponl0ffvih3WNjjDIzMzVv3jzdeeedTgmsUNu2bbVx40bbYz+/kkPetm2bHnroIaWkpOi+++7T8uXLNXjwYO3atUvt2rVzalwAAAAAAADwPA4VuwYPHmz32GKxKCIiQvfcc49mzZrljLhs/Pz8FBkZWaa2c+fOVd++fTVx4kRJ0vTp07VhwwbNmzdPCxYscGpcAAAAAAAA8DwOFbusVquz4yjRwYMHFRUVpaCgICUkJCglJUVNmjQptu327ds1fvx4u3l9+vTR6tWrS91Hbm6ucnNzbY8vX75c4bgBAJ6N3A8A1RP5HwC8n0PFrsrStWtXLV68WHFxccrMzNRLL72ku+66S3v37lVoaGiR9llZWWrQoIHdvAYNGigrK6vU/aSkpOill15yauwAUN3Evri2xGVHZw6oxEjKhtwPANUT+R8AvJ9Dxa5fXj1VmtTUVEd2IUnq16+f7f8dOnRQ165dFRMTo3fffVePP/64w9v9pUmTJtn16fLly4qOjnba9gEAnofcDwDVE/kfALyfQ8Wu3bt3a/fu3crLy1NcXJwk6cCBA/L19VXnzp1t7SwWi3Oi/P9q166tVq1a6dChQ8Uuj4yM1OnTp+3mnT59+qb3/AoMDFRgYKDT4gQAeD5yPwBUT+R/APB+Po6sNHDgQPXo0UMnT57Url27tGvXLp04cUJ333237rvvPm3ZskVbtmzR5s2bnRpsTk6OMjIy1LBhw2KXJyQkaNOmTXbzNmzYoISEBKfGAQAAAAAAAM/kULFr1qxZSklJUZ06dWzz6tSpo5dfftmpv8Y4YcIEffbZZzp69Ki2bdumIUOGyNfXVw899JAkKTExUZMmTbK1HzdunNavX69Zs2Zp3759mjp1qnbs2KGxY8c6LSYAAAAAAAB4Loe+xnj58mWdPXu2yPyzZ8/qypUrFQ6q0MmTJ/XQQw/p/PnzioiIUPfu3fXll18qIiJCknT8+HH5+PxUr+vWrZuWL1+uP/3pT5o8ebJatmyp1atXq127dk6LCQAAAAAAAJ7LoWLXkCFDNHLkSM2aNUtdunSRJH311VeaOHGi7r//fqcFt2LFilKXb926tci8YcOGadiwYU6LAQAAAAAAAFWHQ8WuBQsWaMKECXr44YeVl5f344b8/PT444/r1VdfdWqAAAAAAAAAQFk5VOyqUaOG/va3v+nVV19VRkaGJKl58+YKCQlxanAAAAAAAABAeTh0g/pCmZmZyszMVMuWLRUSEiJjjLPiAgAAAAAAAMrNoWLX+fPnde+996pVq1bq37+/MjMzJUmPP/64nn/+eacGCAAAAAAAAJSVQ8Wu5557Tv7+/jp+/Lhq1Khhmz98+HCtX7/eacEBAAAAAAAA5eHQPbv+/ve/69NPP1Xjxo3t5rds2VLHjh1zSmAAAAAAAABAeTl0ZdfVq1ftrugqdOHCBQUGBlY4KAAAAAAAAMARDhW77rrrLi1ZssT22GKxyGq16pVXXtHdd9/ttOAAAAAAAACA8nDoa4yvvPKK7r33Xu3YsUM3btzQCy+8oO+//14XLlzQP//5T2fHCAAAAAAAAJSJQ1d2tWvXTgcOHFD37t01aNAgXb16Vffff792796t5s2bOztGAAAAAAAAoEzKfWVXXl6e+vbtqwULFuiPf/yjK2ICAAAAAAAAHFLuK7v8/f313XffuSIWAAAAAAAAoEIc+hrjo48+qoULFzo7FgAAAAAAAKBCHLpBfX5+vhYtWqSNGzcqPj5eISEhdstTU1OdEhwAAAAAAABQHuUqdh0+fFixsbHau3evOnfuLEk6cOCAXRuLxeK86AAAAAAAAIByKFexq2XLlsrMzNSWLVskScOHD9df//pXNWjQwCXBAQAAAAAAAOVRrnt2GWPsHn/yySe6evWqUwMCAAAAAAAAHOXQDeoL/bL4BQAAAAAAALhTub7GaLFYityTi3t0AQAAANVX7Itr3R0CAOAXfp6bj84c4MZI3KNcxS5jjEaMGKHAwEBJ0vXr1/X0008X+TXG9PR050UIAAAAAAAAlFG5il1JSUl2jx999FGnBgMAAAAAAABURLmKXWlpaa6Ko1gpKSlKT0/Xvn37FBwcrG7duukvf/mL4uLiSlxn8eLFGjlypN28wMBAXb9+3dXhAgAAAAAAwM0qdIN6V/vss880ZswYffnll9qwYYPy8vLUu3fvm/4CZK1atZSZmWmbjh07VkkRAwAAAAAAwJ3KdWVXZVu/fr3d48WLF6t+/frauXOnevToUeJ6FotFkZGRrg4PAAAAAAAAHsaji12/dOnSJUlS3bp1S22Xk5OjmJgYWa1Wde7cWTNmzFDbtm1LbJ+bm6vc3Fzb48uXLzsnYACAxyL3A0D1RP4HAO/n0V9j/Dmr1apnn31Wd955p9q1a1diu7i4OC1atEhr1qzR0qVLZbVa1a1bN508ebLEdVJSUhQWFmaboqOjXdEFAIAHIfcDQPVE/gcA71dlil1jxozR3r17tWLFilLbJSQkKDExUZ06dVLPnj2Vnp6uiIgIvfHGGyWuM2nSJF26dMk2nThxwtnhAwA8DLkfAKon8j8AeL8q8TXGsWPH6uOPP9bnn3+uxo0bl2tdf39/3XrrrTp06FCJbQIDAxUYGFjRMAEAVQi5HwCqJ/I/AHg/j76yyxijsWPH6oMPPtDmzZvVtGnTcm+joKBA//rXv9SwYUMXRAgAAAAAAABP4tFXdo0ZM0bLly/XmjVrFBoaqqysLElSWFiYgoODJUmJiYlq1KiRUlJSJEnTpk3THXfcoRYtWig7O1uvvvqqjh07pieeeMJt/QAAAAAAAEDl8Ohi1/z58yVJv/rVr+zmp6WlacSIEZKk48ePy8fnpwvULl68qNGjRysrK0t16tRRfHy8tm3bpjZt2lRW2AAAAAAAAHATjy52GWNu2mbr1q12j2fPnq3Zs2e7KCJUVbEvri1x2dGZAyoxEsA1SnuOO5sjfzP8DQKoLNUt35TUX2/sKwBUltgX11aJPFp4Djg6c4BdzJX53sBTefQ9uwAAAAAAAIDyoNgFAAAAAAAAr0GxCwAAAAAAAF6DYhcAAAAAAAC8BsUuAAAAAAAAeA2KXQAAAAAAAPAaFLsAAAAAAADgNSh2AQAAAAAAwGtQ7AIAAAAAAIDXoNgFAAAAAAAAr0GxCwAAAAAAAF6DYhcAAAAAAAC8hp+7A/BmsS+uLXb+0ZkDKmU/rtiXJ8RQ2r4qaz8l9cnR2LzxODmiOv3NwHtV1eeXp8ft7JxcWp8qKxdVJvKra3jyc8XTX5MAgLPFvrj2pjmssE1hjvzl/2+2bqGyvI4obHOz7f8yX/+8XXF9Km57lfUeuSrhyi4AAAAAAAB4DYpdAAAAAAAA8BoUuwAAAAAAAOA1KHYBAAAAAADAa1DsAgAAAAAAgNeg2AUAAAAAAACvQbELAAAAAAAAXqNKFLtef/11xcbGKigoSF27dtXXX39davtVq1apdevWCgoKUvv27bVu3bpKihQAAAAAAADu5PHFrpUrV2r8+PFKTk7Wrl271LFjR/Xp00dnzpwptv22bdv00EMP6fHHH9fu3bs1ePBgDR48WHv37q3kyAEAAAAAAFDZPL7YlZqaqtGjR2vkyJFq06aNFixYoBo1amjRokXFtp87d6769u2riRMn6pZbbtH06dPVuXNnzZs3r5IjBwAAAAAAQGXzc3cApblx44Z27typSZMm2eb5+PioV69e2r59e7HrbN++XePHj7eb16dPH61evbrE/eTm5io3N9f2+NKlS5Kky5cvVyB6yZp7rdj5Fd1uWffjin15Qgyl7csRJcXnSJ8cjc0bj5MjqtPfjLM5+++iNJ70/C9c1xhT7nUrO/c7Y9uu5OlxOzsnl9anyspFlak65VdPeE1Smc+Vynpd5OwYKjpGnpj/AbiXNffaTf+OC9sU5q1f/r+0bf0815XldURhm+K2X9J2yxPHz2P/5bqlbbOqK3P+Nx7sv//9r5Fktm3bZjd/4sSJpkuXLsWu4+/vb5YvX2437/XXXzf169cvcT/JyclGEhMTExNTFZ1OnDhR7nMMuZ+JiYmp6k/kfyYmJqbqOd0s/3v0lV2VZdKkSXZXg1mtVl24cEH16tWTxWJxY2Tuc/nyZUVHR+vEiROqVauWu8OpMhi38mPMyo8x+4kxRleuXFFUVFS513Uk93vb2HtbfyTv6xP98Xze1qeq0h9X5v+qMgZVAWPpPIylczCOzuOusSxr/vfoYld4eLh8fX11+vRpu/mnT59WZGRksetERkaWq70kBQYGKjAw0G5e7dq1HQvay9SqVYsk4ADGrfwYs/JjzH4UFhbm0HoVyf3eNvbe1h/J+/pEfzyft/WpKvTH1fm/KoxBVcFYOg9j6RyMo/O4YyzLkv89+gb1AQEBio+P16ZNm2zzrFarNm3apISEhGLXSUhIsGsvSRs2bCixPQAAAAAAALyHR1/ZJUnjx49XUlKSbrvtNnXp0kVz5szR1atXNXLkSElSYmKiGjVqpJSUFEnSuHHj1LNnT82aNUsDBgzQihUrtGPHDr355pvu7AYAAAAAAAAqgccXu4YPH66zZ89qypQpysrKUqdOnbR+/Xo1aNBAknT8+HH5+Px0gVq3bt20fPly/elPf9LkyZPVsmVLrV69Wu3atXNXF6qkwMBAJScnF7nEG6Vj3MqPMSs/xsx9vG3sva0/kvf1if54Pm/rk7f1xxGMgfMwls7DWDoH4+g8nj6WFmMc+L1eAAAAAAAAwAN59D27AAAAAAAAgPKg2AUAAAAAAACvQbELAAAAAAAAXoNiFwAAAAAAALwGxS4AAAAAAAB4DYpd1cjrr7+u2NhYBQUFqWvXrvr6669LbJuXl6dp06apefPmCgoKUseOHbV+/foKbbMqcvaYff755xo4cKCioqJksVi0evVqF/eg8jl7zFJSUnT77bcrNDRU9evX1+DBg7V//35Xd6PSOXvc5s+frw4dOqhWrVqqVauWEhIS9Mknn7i6Gx6vPOP8q1/9ShaLpcg0YMAAW5villssFr366qu2NrGxsUWWz5w50y19kqQ5c+YoLi5OwcHBio6O1nPPPafr16+Xa5vXr1/XmDFjVK9ePdWsWVMPPPCATp8+7ZH9KUsOKe5YP/300x7Zn6lTpxaJtXXr1nbbqErHp7i/D4vFojFjxtjauPL4lLdPznp95CnHyFnnYVcfI2dwdv4fMWJEkeV9+/atjK64lTvOOd7KHecHb8X7XOdw9ji6/TlpUC2sWLHCBAQEmEWLFpnvv//ejB492tSuXducPn262PYvvPCCiYqKMmvXrjUZGRnmb3/7mwkKCjK7du1yeJtVjSvGbN26deaPf/yjSU9PN5LMBx98UEm9qRyuGLM+ffqYtLQ0s3fvXrNnzx7Tv39/06RJE5OTk1NZ3XI5V4zbhx9+aNauXWsOHDhg9u/fbyZPnmz8/f3N3r17K6tbHqe843z+/HmTmZlpm/bu3Wt8fX1NWlqarc3Pl2dmZppFixYZi8ViMjIybG1iYmLMtGnT7No56/lb3j4tW7bMBAYGmmXLlpkjR46YTz/91DRs2NA899xz5drm008/baKjo82mTZvMjh07zB133GG6devmkf0pSw7p2bOnGT16tN0xunTpkkf2Jzk52bRt29Yu1rNnz9ptpyodnzNnztj1ZcOGDUaS2bJli62Nq46PI31y1usjTzlGzjoPu/IYOYMr8n9SUpLp27evXbsLFy5UUo/cw13nHG/krvODN+J9rnO4Yhzd/Zyk2FVNdOnSxYwZM8b2uKCgwERFRZmUlJRi2zds2NDMmzfPbt79999vHnnkEYe3WdW4Ysx+zhuLXa4eM2N+fGMkyXz22WfOCdoDVMa4GWNMnTp1zP/93/9VPOAqqqI5a/bs2SY0NLTUQtWgQYPMPffcYzcvJibGzJ4926GYb6a8fRozZkyR+MaPH2/uvPPOMm8zOzvb+Pv7m1WrVtna/Oc//zGSzPbt2z2uP79UXA7p2bOnGTduXIViL44r+pOcnGw6duxY4j6r+vEZN26cad68ubFarbZ5rjo+xrjn9ZEnHSNnnYddeYycwRX5PykpyQwaNMjZoXo0d5xzvJU7zg/eive5zuGKcXT3c5KvMVYDN27c0M6dO9WrVy/bPB8fH/Xq1Uvbt28vdp3c3FwFBQXZzQsODtY//vEPh7dZlbhizLxdZY3ZpUuXJEl169Z1QtTuVxnjVlBQoBUrVujq1atKSEhwXvBViDNy1sKFC/Xggw8qJCSk2OWnT5/W2rVr9fjjjxdZNnPmTNWrV0+33nqrXn31VeXn5zvWkZ9xpE/dunXTzp07bZelHz58WOvWrVP//v3LvM2dO3cqLy/Prk3r1q3VpEmTCuV/V/SnOCXlkGXLlik8PFzt2rXTpEmTdO3aNYf74ur+HDx4UFFRUWrWrJkeeeQRHT9+3LasKh+fGzduaOnSpRo1apQsFovdMmcfH0f75IzXR550jJx5HnbFMXIGV+b/rVu3qn79+oqLi9Nvf/tbnT9/3qmxexJ3nXO8kbvOD96I97nO4crzhzufk36Vtie4zblz51RQUKAGDRrYzW/QoIH27dtX7Dp9+vRRamqqevTooebNm2vTpk1KT09XQUGBw9usSlwxZt6uMsbMarXq2Wef1Z133ql27do5vQ/u4Mpx+9e//qWEhARdv35dNWvW1AcffKA2bdq4rC+erKI56+uvv9bevXu1cOHCEtu89dZbCg0N1f333283//e//706d+6sunXratu2bZo0aZIyMzOVmprqWGf+P0f69PDDD+vcuXPq3r27jDHKz8/X008/rcmTJ5d5m1lZWQoICFDt2rWLtMnKyvKo/vxSSTnk4YcfVkxMjKKiovTdd9/pD3/4g/bv36/09HSP60/Xrl21ePFixcXFKTMzUy+99JLuuusu7d27V6GhoVX6+KxevVrZ2dkaMWJEke04+/g42idnvD7ypGPkrPOwq46RM7gq//ft21f333+/mjZtqoyMDE2ePFn9+vXT9u3b5evr69Q+eAJ3nXO8kbvOD96I97nO4arzh7ufk1zZhWLNnTtXLVu2VOvWrRUQEKCxY8dq5MiR8vHhKVMSxqz8yjtmY8aM0d69e7VixYpKjtSzlHXc4uLitGfPHn311Vf67W9/q6SkJP373/92U9RV28KFC9W+fXt16dKlxDaLFi3SI488UuRTrvHjx+tXv/qVOnTooKefflqzZs3Sa6+9ptzcXFeHXcTWrVs1Y8YM/e1vf9OuXbuUnp6utWvXavr06ZUeizOUtz8l5ZAnn3xSffr0Ufv27fXII49oyZIl+uCDD5SRkVEZ3bApS3/69eunYcOGqUOHDurTp4/WrVun7Oxsvfvuu5Uaa1mU9/gsXLhQ/fr1U1RUlN18Tzk+kved6511HvakY+RsJeX/Bx98UP/zP/+j9u3ba/Dgwfr444/1zTffaOvWre4J1AN52znHnbzt/OBO3pbH3aUs4+ju5yRHtBoIDw+Xr69vkV/5OX36tCIjI4tdJyIiQqtXr9bVq1d17Ngx7du3TzVr1lSzZs0c3mZV4oox83auHrOxY8fq448/1pYtW9S4cWOX9MEdXDluAQEBatGiheLj45WSkqKOHTtq7ty5LuuLJ6tIzrp69apWrFhR7NcTC33xxRfav3+/nnjiiZvG0rVrV+Xn5+vo0aNlir0kjvTpz3/+sx577DE98cQTat++vYYMGaIZM2YoJSVFVqu1TNuMjIzUjRs3lJ2dXeb9uqs/P1eeHNK1a1dJ0qFDhzy2P4Vq166tVq1a2WKtqsfn2LFj2rhxY5n/hqSKHR/Jfa+PPOkYueo87Kxj5Ayuzv+FmjVrpvDwcI/osyu465zjjdx1fvBGvM91jsp671vZz0mKXdVAQECA4uPjtWnTJts8q9WqTZs23fT+PUFBQWrUqJHy8/P1/vvva9CgQRXeZlXgijHzdq4aM2OMxo4dqw8++ECbN29W06ZNXdYHd6jM55rVanXL1USeoCLjvGrVKuXm5urRRx8tsc3ChQsVHx+vjh073jSWPXv2yMfHR/Xr1y97B4rhSJ+uXbtW5JPLwq/bGGPKtM34+Hj5+/vbtdm/f7+OHz9eofzviv4U/lveHLJnzx5JUsOGDR3piiTX9eeXcnJylJGRYYu1qh2fQmlpaapfv74GDBhw01iccXwk970+8qRjdLP+SO77G3IWV+f/QidPntT58+c9os+u4K5zjjdy1/nBG/E+1zkq6/1IpT8n3XBTfLjBihUrTGBgoFm8eLH597//bZ588klTu3Ztk5WVZYwx5rHHHjMvvviirf2XX35p3n//fZORkWE+//xzc88995imTZuaixcvlnmbVZ0rxuzKlStm9+7dZvfu3UaSSU1NNbt37zbHjh2r7O65hCvG7Le//a0JCwszW7dutfvZ2mvXrlV291zGFeP24osvms8++8wcOXLEfPfdd+bFF180FovF/P3vf6/s7nmM8o5zoe7du5vhw4eXuN1Lly6ZGjVqmPnz5xdZtm3bNjN79myzZ88ek5GRYZYuXWoiIiJMYmKiW/qUnJxsQkNDzTvvvGMOHz5s/v73v5vmzZub3/zmN2XepjHGPP3006ZJkyZm8+bNZseOHSYhIcEkJCR4ZH9ulkMOHTpkpk2bZnbs2GGOHDli1qxZY5o1a2Z69Ojhkf15/vnnzdatW82RI0fMP//5T9OrVy8THh5uzpw5Y2tTlY6PMT/+4lOTJk3MH/7whyL7dOXxcaRPznp95CnHyBnnYVcfI2dwdv6/cuWKmTBhgtm+fbs5cuSI2bhxo+ncubNp2bKluX79usv74y7uOud4I3edH7wR73OdwxXj6O7nJMWuauS1114zTZo0MQEBAaZLly7myy+/tC3r2bOnSUpKsj3eunWrueWWW0xgYKCpV6+eeeyxx8x///vfcm3TGzh7zLZs2WIkFZl+vp2qztljVtx4STJpaWmV1KPK4exxGzVqlImJiTEBAQEmIiLC3HvvvdW60FWoPONsjDH79u0zkkoduzfeeMMEBweb7OzsIst27txpunbtasLCwkxQUJC55ZZbzIwZM5z6Zqg8fcrLyzNTp041zZs3N0FBQSY6Oto888wzdi9MbrZNY4z54YcfzDPPPGPq1KljatSoYYYMGWIyMzM9sj83yyHHjx83PXr0MHXr1jWBgYGmRYsWZuLEiebSpUse2Z/hw4ebhg0bmoCAANOoUSMzfPhwc+jQIbt9VqXjY4wxn376qZFk9u/fX2R/rj4+5e2Ts14fecoxcsZ5uDKOkTM4M/9fu3bN9O7d20RERBh/f38TExNjRo8e7dVvhAu545zjrdxxfvBWvM91DmePo7ufkxZjSrjuEQAAAAAAAKhiuGcXAAAAAAAAvAbFLgAAAAAAAHgNil0AAAAAAADwGhS7AAAAAAAA4DUodgEAAAAAAMBrUOwCAAAAAACA16DYBQAAAAAAAK9BsQsAAAAAAABeg2IXAAAAAAAAvAbFLgAAAAAAAHgNil0AAAAAAADwGv8Pgf1OGKPAghwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_desc = sorted_results['mean_accuracy'].astype('float32').describe()\n",
    "xlimit_range = [accuracy_desc['min'] - accuracy_desc['std'], accuracy_desc['max'] + accuracy_desc['std']]\n",
    "for hperparameter_name in turning_parameters:\n",
    "    parameter_group = sorted_results.groupby(hperparameter_name)\n",
    "    fix, axs = pyplot.subplots(1, len(parameter_group), layout='constrained', sharex=False, sharey=True, figsize=(12,2))\n",
    "    for i, g in enumerate(parameter_group):\n",
    "        g[1]['mean_accuracy'].astype('float32').plot(kind='hist',bins=50, subplots=True,sharex=False,sharey=True,ax=axs[i])\n",
    "        axs[i].set_title(f\"{hperparameter_name}_{g[0]}\")\n",
    "        \n",
    "pyplot.xlim(xlimit_range)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                         19\n",
      "mean_accuracy      0.9370654993514915\n",
      "trial_id                  9dbc7_00019\n",
      "lr                                0.1\n",
      "momentum           0.8725056264596475\n",
      "optim_type                          1\n",
      "num_layers                         32\n",
      "activation_type                     2\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sorted_results_file = f\"{get_filename_of_ipynb()}_sorted_results.csv\"\n",
    "sorted_results = pd.read_csv(sorted_results_file, dtype='str')\n",
    "best_config = sorted_results.loc[0]\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.95\n",
      "Test Accuracy: 0.94358\n",
      "Train F1: 0.96\n",
      "Test F1: 0.95426\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.precision', 5)\n",
    "\n",
    "model, config = load_model(f\"{get_filename_of_ipynb()}.pt\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader, features_size = prepare_dataloader()\n",
    "model.eval()\n",
    "\n",
    "(trainAccuracy, trainF1)= eval_dl_method(model, train_loader, device=device)\n",
    "(testAccuracy, testF1)  = eval_dl_method(model, test_loader, device=device)\n",
    "print(f\"Train Accuracy: {trainAccuracy:.2f}\\nTest Accuracy: {testAccuracy:.5f}\")\n",
    "print(f\"Train F1: {trainF1:.2f}\\nTest F1: {testF1:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
