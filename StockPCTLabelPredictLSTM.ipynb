{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockPCTLabelPredictLSTM\n",
      "/mnt/AIWorkSpace/work/fin-ml/data/\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#logging\n",
    "from myutil.logconf import logging\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%run 'nb_utils.ipynb'\n",
    "task_name = get_filename_of_ipynb()\n",
    "print(task_name)\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/{task_name}'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')\n",
    "\n",
    "return_period = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 11:36:00,440\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9db08f3dfe430aab5d327dbc017d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.8.18</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.9.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.18', ray_version='2.9.1', ray_commit='cfbf98c315cfb2710c56039a3c96477d196de049', protocol_version=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read /mnt/AIWorkSpace/work/fin-ml/data/AAPL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MSFT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMZN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NVDA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOGL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TSLA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/META.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOG.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADBE.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NFLX.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CSCO.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTC.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTU.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CMCSA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TXN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMAT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADSK.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMD.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/QCOM.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MU.csv completely!\n",
      "[              Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  19.846  19.894  19.715  19.755     17.319  234684800\n",
      "2014-01-03  19.745  19.775  19.301  19.321     16.938  392467600\n",
      "2014-01-06  19.195  19.529  19.057  19.426     17.031  412610800\n",
      "2014-01-07  19.440  19.499  19.211  19.287     16.909  317209200\n",
      "2014-01-08  19.243  19.484  19.239  19.409     17.016  258529600\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 195.180 195.410 192.970 193.600    193.600   37122800\n",
      "2023-12-26 193.610 193.890 192.830 193.050    193.050   28919300\n",
      "2023-12-27 192.490 193.500 191.090 193.150    193.150   48087700\n",
      "2023-12-28 194.140 194.660 193.170 193.580    193.580   34049900\n",
      "2023-12-29 193.900 194.400 191.730 192.530    192.530   42628800\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  37.350  37.400  37.100  37.160     31.291  30632200\n",
      "2014-01-03  37.200  37.220  36.600  36.910     31.080  31134800\n",
      "2014-01-06  36.850  36.890  36.110  36.130     30.423  43603700\n",
      "2014-01-07  36.330  36.490  36.210  36.410     30.659  35802800\n",
      "2014-01-08  36.000  36.140  35.580  35.760     30.112  59971700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 373.680 375.180 372.710 374.580    374.580  17091100\n",
      "2023-12-26 375.000 376.940 373.500 374.660    374.660  12673100\n",
      "2023-12-27 373.690 375.060 372.810 374.070    374.070  14905400\n",
      "2023-12-28 375.370 376.460 374.160 375.280    375.280  14327000\n",
      "2023-12-29 376.000 377.160 373.480 376.040    376.040  18723000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  19.940  19.968  19.701  19.899     19.899  42756000\n",
      "2014-01-03  19.914  20.135  19.811  19.822     19.822  44204000\n",
      "2014-01-06  19.792  19.850  19.421  19.681     19.681  63412000\n",
      "2014-01-07  19.752  19.924  19.715  19.902     19.902  38320000\n",
      "2014-01-08  19.924  20.150  19.802  20.096     20.096  46330000\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 153.770 154.350 152.710 153.420    153.420  29480100\n",
      "2023-12-26 153.560 153.980 153.030 153.410    153.410  25067200\n",
      "2023-12-27 153.560 154.780 153.120 153.340    153.340  31434700\n",
      "2023-12-28 153.720 154.080 152.950 153.380    153.380  27057000\n",
      "2023-12-29 153.100 153.890 151.030 151.940    151.940  39789000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.980   3.995   3.930   3.965      3.741  26009200\n",
      "2014-01-03   3.973   3.980   3.905   3.918      3.696  25933200\n",
      "2014-01-06   3.957   4.000   3.920   3.970      3.745  40949200\n",
      "2014-01-07   4.010   4.050   3.983   4.035      3.807  33328800\n",
      "2014-01-08   4.050   4.110   4.035   4.090      3.859  30819200\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 491.950 493.830 484.670 488.300    488.300  25213900\n",
      "2023-12-26 489.680 496.000 489.600 492.790    492.790  24420000\n",
      "2023-12-27 495.110 496.800 490.850 494.170    494.170  23364800\n",
      "2023-12-28 496.430 498.840 494.120 495.220    495.220  24658700\n",
      "2023-12-29 498.130 499.970 487.510 495.220    495.220  38869000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.914  27.972  27.734  27.856     27.856   72783144\n",
      "2014-01-03  27.903  27.951  27.651  27.653     27.653   66601332\n",
      "2014-01-06  27.853  27.999  27.689  27.961     27.961   70701228\n",
      "2014-01-07  28.153  28.521  28.057  28.500     28.500  102001896\n",
      "2014-01-08  28.679  28.712  28.361  28.559     28.559   89610300\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 140.770 141.990 140.710 141.490    141.490   26514600\n",
      "2023-12-26 141.590 142.680 141.190 141.520    141.520   16780300\n",
      "2023-12-27 141.590 142.080 139.890 140.370    140.370   19628600\n",
      "2023-12-28 140.780 141.140 139.750 140.230    140.230   16045700\n",
      "2023-12-29 139.630 140.360 138.780 139.690    139.690   18727200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02   9.987  10.165   9.770  10.007     10.007   92826000\n",
      "2014-01-03  10.000  10.146   9.907   9.971      9.971   70425000\n",
      "2014-01-06  10.000  10.027   9.683   9.800      9.800   80416500\n",
      "2014-01-07   9.841  10.027   9.683   9.957      9.957   75511500\n",
      "2014-01-08   9.923  10.247   9.917  10.085     10.085   92448000\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 256.760 258.220 251.370 252.540    252.540   93249800\n",
      "2023-12-26 254.490 257.970 252.910 256.610    256.610   86892400\n",
      "2023-12-27 258.350 263.340 257.520 261.440    261.440  106494400\n",
      "2023-12-28 263.660 265.130 252.710 253.180    253.180  113619900\n",
      "2023-12-29 255.100 255.190 247.430 248.480    248.480  100615300\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  54.830  55.220  54.190  54.710     54.710  43195500\n",
      "2014-01-03  55.020  55.650  54.530  54.560     54.560  38246200\n",
      "2014-01-06  54.420  57.260  54.050  57.200     57.200  68852600\n",
      "2014-01-07  57.700  58.550  57.220  57.920     57.920  77207400\n",
      "2014-01-08  57.600  58.410  57.230  58.230     58.230  56682400\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 355.580 357.200 351.220 353.390    353.390  11764200\n",
      "2023-12-26 354.990 356.980 353.450 354.830    354.830   9898600\n",
      "2023-12-27 356.070 359.000 355.310 357.830    357.830  13207900\n",
      "2023-12-28 359.700 361.900 357.810 358.320    358.320  11798800\n",
      "2023-12-29 358.990 360.000 351.820 353.960    353.960  14980500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.782  27.839  27.603  27.724     27.724   73129082\n",
      "2014-01-03  27.771  27.819  27.520  27.522     27.522   66917888\n",
      "2014-01-06  27.721  27.867  27.558  27.829     27.829   71037271\n",
      "2014-01-07  28.020  28.386  27.924  28.365     28.365  102486711\n",
      "2014-01-08  28.543  28.576  28.226  28.424     28.424   90036218\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 142.130 143.250 142.055 142.720    142.720   18494700\n",
      "2023-12-26 142.980 143.945 142.500 142.820    142.820   11170100\n",
      "2023-12-27 142.830 143.320 141.051 141.440    141.440   17288400\n",
      "2023-12-28 141.850 142.270 140.828 141.280    141.280   12192500\n",
      "2023-12-29 140.680 141.435 139.900 140.930    140.930   14872700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  59.060  59.530  58.940  59.290     59.290  2745900\n",
      "2014-01-03  59.190  59.690  59.110  59.160     59.160  1589000\n",
      "2014-01-06  58.060  58.770  58.010  58.120     58.120  3753600\n",
      "2014-01-07  58.260  59.050  58.060  58.970     58.970  2963600\n",
      "2014-01-08  59.120  59.280  58.460  58.900     58.900  3456000\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 600.800 601.860 596.000 598.750    598.750  1659800\n",
      "2023-12-26 598.920 601.690 596.500 598.260    598.260  1595100\n",
      "2023-12-27 598.600 599.790 593.710 596.080    596.080  1394900\n",
      "2023-12-28 597.440 599.040 593.630 595.520    595.520  1702600\n",
      "2023-12-29 596.090 600.750 592.940 596.600    596.600  1893900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  52.401  52.511  51.543  51.831     51.831  12325600\n",
      "2014-01-03  52.000  52.496  51.843  51.871     51.871  10817100\n",
      "2014-01-06  51.890  52.044  50.476  51.367     51.367  15501500\n",
      "2014-01-07  49.684  49.699  48.153  48.500     48.500  36167600\n",
      "2014-01-08  48.104  49.426  48.074  48.713     48.713  20001100\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 494.000 496.020 485.450 486.760    486.760   2701100\n",
      "2023-12-26 489.390 491.480 486.380 491.190    491.190   2034500\n",
      "2023-12-27 491.240 494.020 489.250 491.790    491.790   2561300\n",
      "2023-12-28 492.000 492.890 489.070 490.510    490.510   1710500\n",
      "2023-12-29 490.370 492.230 481.940 486.880    486.880   2739500\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 22.170 22.290 21.910 22.000     16.095  44377000\n",
      "2014-01-03 22.090 22.120 21.830 21.980     16.080  36328200\n",
      "2014-01-06 21.960 22.230 21.930 22.010     16.102  34150300\n",
      "2014-01-07 22.260 22.410 22.150 22.310     16.322  37368800\n",
      "2014-01-08 22.290 22.360 22.150 22.290     16.307  38362700\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 49.840 50.390 49.840 50.090     49.703  12900700\n",
      "2023-12-26 50.110 50.400 50.050 50.280     49.892   9721200\n",
      "2023-12-27 50.300 50.560 50.280 50.440     50.051  10414300\n",
      "2023-12-28 50.580 50.630 50.420 50.480     50.090   8549900\n",
      "2023-12-29 50.450 50.590 50.220 50.520     50.130  12491200\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.780 25.820 25.470 25.790     19.436  31833300\n",
      "2014-01-03 25.860 25.900 25.600 25.780     19.428  27796700\n",
      "2014-01-06 25.770 25.790 25.450 25.460     19.187  28682300\n",
      "2014-01-07 25.540 25.730 25.470 25.590     19.285  19665100\n",
      "2014-01-08 25.640 25.710 25.300 25.430     19.164  29680500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 47.250 48.160 47.200 48.000     48.000  30053700\n",
      "2023-12-26 48.920 50.520 48.710 50.500     50.500  60287400\n",
      "2023-12-27 50.630 51.280 50.190 50.760     50.760  52148000\n",
      "2023-12-28 50.810 50.870 50.160 50.390     50.390  27705200\n",
      "2023-12-29 50.300 50.570 49.770 50.250     50.250  29266500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  76.110  76.160  75.400  75.940     69.476  1355700\n",
      "2014-01-03  75.750  76.380  75.530  75.800     69.348   998800\n",
      "2014-01-06  75.800  76.000  75.470  75.740     69.293  1268700\n",
      "2014-01-07  75.880  77.350  75.740  77.060     70.501  1551000\n",
      "2014-01-08  76.890  77.050  76.010  76.440     70.107  2437600\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 622.830 625.150 617.680 624.070    623.131   820800\n",
      "2023-12-26 625.170 628.330 622.730 624.850    623.910   638300\n",
      "2023-12-27 623.990 629.800 622.260 629.120    628.174   734400\n",
      "2023-12-28 630.740 631.070 627.180 628.020    627.075   680700\n",
      "2023-12-29 628.020 630.830 622.460 625.030    624.090   724300\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.900 25.950 25.635 25.725     20.931  19522400\n",
      "2014-01-03 25.815 25.855 25.425 25.535     20.777  13371400\n",
      "2014-01-06 25.565 25.810 25.335 25.510     20.756  17987800\n",
      "2014-01-07 25.670 26.600 25.555 26.415     21.493  37161400\n",
      "2014-01-08 26.345 26.725 26.260 26.375     21.460  29731600\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 44.130 44.620 43.810 44.000     43.709  11893900\n",
      "2023-12-26 44.000 44.070 43.500 43.930     43.639   9624300\n",
      "2023-12-27 43.900 44.170 43.710 43.990     43.699   9253800\n",
      "2023-12-28 43.970 44.410 43.890 44.120     43.828   9023400\n",
      "2023-12-29 44.090 44.140 43.560 43.850     43.560  13694900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  43.440  43.500  42.880  43.100     32.954  6959200\n",
      "2014-01-03  43.120  43.460  42.970  43.290     33.100  4693300\n",
      "2014-01-06  43.250  43.280  42.850  42.930     32.824  4446300\n",
      "2014-01-07  42.980  43.110  42.640  42.700     32.648  5078900\n",
      "2014-01-08  42.960  43.320  42.620  43.290     33.100  6353500\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 167.260 168.920 166.820 168.240    168.240  3492400\n",
      "2023-12-26 168.940 171.530 168.450 170.810    170.810  3202200\n",
      "2023-12-27 171.220 171.620 170.330 171.230    171.230  3264900\n",
      "2023-12-28 172.000 172.310 170.710 171.720    171.720  3023000\n",
      "2023-12-29 171.540 171.700 169.920 170.460    170.460  2920600\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  17.690  17.690  17.370  17.550     15.280  7785900\n",
      "2014-01-03  17.540  17.700  17.470  17.510     15.245  6773000\n",
      "2014-01-06  17.500  17.510  17.220  17.290     15.053  9975500\n",
      "2014-01-07  17.370  17.430  17.260  17.370     15.123  8133200\n",
      "2014-01-08  17.400  17.450  17.180  17.420     15.166  8026100\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 161.600 163.000 160.840 162.050    162.050  2770600\n",
      "2023-12-26 162.300 164.970 162.100 164.280    164.280  2520500\n",
      "2023-12-27 164.540 164.990 163.530 164.210    164.210  3319600\n",
      "2023-12-28 165.000 165.010 162.850 163.120    163.120  2909700\n",
      "2023-12-29 163.110 163.560 160.700 162.070    162.070  2980700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  49.330  49.740  48.880  49.250     49.250  2488000\n",
      "2014-01-03  49.110  49.500  48.780  48.900     48.900  1934200\n",
      "2014-01-06  48.980  49.290  48.290  48.550     48.550  1856500\n",
      "2014-01-07  48.890  50.100  48.630  49.680     49.680  2002500\n",
      "2014-01-08  49.500  50.550  49.050  50.240     50.240  2047400\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 243.740 244.030 240.310 242.760    242.760   719400\n",
      "2023-12-26 242.490 245.360 241.960 245.070    245.070   595000\n",
      "2023-12-27 245.360 245.880 244.380 245.110    245.110   771900\n",
      "2023-12-28 245.630 245.850 244.020 244.910    244.910   537200\n",
      "2023-12-29 243.720 245.400 242.790 243.480    243.480   721400\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.850   3.980   3.840   3.950      3.950  20548400\n",
      "2014-01-03   3.980   4.000   3.880   4.000      4.000  22887200\n",
      "2014-01-06   4.010   4.180   3.990   4.130      4.130  42398300\n",
      "2014-01-07   4.190   4.250   4.110   4.180      4.180  42932100\n",
      "2014-01-08   4.230   4.260   4.140   4.180      4.180  30678700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 140.480 140.700 138.310 139.600    139.600  35370400\n",
      "2023-12-26 140.070 143.850 139.920 143.410    143.410  47157400\n",
      "2023-12-27 144.720 146.250 143.180 146.070    146.070  49033400\n",
      "2023-12-28 146.800 150.410 145.950 148.760    148.760  63800700\n",
      "2023-12-29 149.500 151.050 147.200 147.410    147.410  62028200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  73.610  73.770  73.260  73.320     54.740  10110200\n",
      "2014-01-03  73.330  73.480  72.440  72.890     54.419   7970400\n",
      "2014-01-06  73.080  73.200  72.550  72.700     54.277   7696200\n",
      "2014-01-07  72.800  73.310  72.600  73.240     54.680   5902700\n",
      "2014-01-08  73.150  73.680  72.680  73.680     55.008   8976900\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 143.250 144.400 142.750 143.490    143.490   4658300\n",
      "2023-12-26 144.170 146.050 143.960 145.460    145.460   4381200\n",
      "2023-12-27 145.850 146.220 145.040 145.720    145.720   4470300\n",
      "2023-12-28 146.180 146.890 145.730 145.860    145.860   4928800\n",
      "2023-12-29 145.410 145.620 143.790 144.630    144.630   4838400\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 21.680 21.790 21.270 21.660     21.293  26413500\n",
      "2014-01-03 21.200 21.430 20.900 20.970     20.615  34590200\n",
      "2014-01-06 20.970 20.970 20.640 20.670     20.320  38180500\n",
      "2014-01-07 20.890 21.940 20.890 21.730     21.362  67904500\n",
      "2014-01-08 24.200 24.500 23.560 23.870     23.466  93499500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 86.150 87.490 85.620 86.490     86.374  22519000\n",
      "2023-12-26 86.700 87.870 86.430 87.060     86.944  11203900\n",
      "2023-12-27 87.480 87.490 86.220 86.660     86.544   9186300\n",
      "2023-12-28 86.750 86.750 85.840 86.000     85.885   9606200\n",
      "2023-12-29 85.840 86.140 85.030 85.340     85.340   8546000\n",
      "\n",
      "[2516 rows x 6 columns]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "\n",
    "# Loading the data\n",
    "stk_tickers = [\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"AMZN\",\n",
    "    \"NVDA\",\n",
    "    \"GOOGL\",\n",
    "    \"TSLA\",\n",
    "    \"META\",\n",
    "    \"GOOG\",\n",
    "    \"ADBE\",\n",
    "    \"NFLX\",\n",
    "    \"CSCO\",\n",
    "    \"INTC\",\n",
    "    \"INTU\",\n",
    "    \"CMCSA\",\n",
    "    \"TXN\",\n",
    "    \"AMAT\",\n",
    "    \"ADSK\",\n",
    "    \"AMD\",\n",
    "    \"QCOM\",\n",
    "    \"MU\",\n",
    "]\n",
    "\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for stk_symbol in stk_tickers:\n",
    "    stk_file = f\"{data_dir}{stk_symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index(\"Date\")\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([stk_symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {stk_symbol} from yfin and write to {stk_file} completely!\")\n",
    "    ticks_data.append(_stk_data)\n",
    "\n",
    "print(ticks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name:cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device_name = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_name)\n",
    "seq_len = 3\n",
    "validation_size = 0.2\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "shuffle = True\n",
    "print(f\"device_name:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes = 3\n",
    "# 0: PCT <= -0.05\n",
    "# 1: 0.05 < PCT < -0.05\n",
    "# 2: PCT >= -0.05\n",
    "num_classes = 3\n",
    "pct_threshold = 0.05\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "\n",
    "def gen_pct_label(stk_data, _return_period):\n",
    "    max_price_period = (\n",
    "        stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "    )\n",
    "    max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "    pct_label = max_pct_period.apply(\n",
    "        lambda x: 2 if x >= pct_threshold else 0 if x <= -pct_threshold else 1\n",
    "    ).astype(\"int8\")\n",
    "    pct_label.name = \"label\"\n",
    "    return pct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=10),\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=60),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    buy_signal = sma[\"SMA_10\"] > sma[\"SMA_60\"]\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal[\"Signal\"] = (buy_signal).astype(\"int\")\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "\n",
    "def gen_analysis_data(stk_data, _return_period):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.adosc(),\n",
    "            stk_data.ta.kvo(),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=10) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=30) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=200) / 100,\n",
    "            stk_data.ta.stoch(k=10) / 100,\n",
    "            stk_data.ta.stoch(k=30) / 100,\n",
    "            stk_data.ta.stoch(k=200) / 100,\n",
    "            gen_buy_sell_signal(stk_data),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data = pd.concat(\n",
    "        [data.astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AAPL  MSFT  AMZN  NVDA  GOOGL  TSLA  META  GOOG  ADBE  NFLX  CSCO  INTC  INTU  CMCSA   TXN  AMAT  ADSK   AMD  QCOM    MU\n",
      "0     6     7    14    32      9    55    17     8    13    26     7    17    10      8     8    25    17    50    20    41\n",
      "1  2022  2092  1929  1574   2069  1475  1915  2058  1957  1735  2141  1975  1981   2162  2085  1779  1873  1374  1933  1590\n",
      "2   285   214   370   707    235   783   381   247   343   552   165   321   322    143   220   509   423   889   360   682\n",
      "class 0: 0.843%,   390\n",
      "class 1: 81.537%, 37719\n",
      "class 2: 17.620%,  8151\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_df = pd.DataFrame()\n",
    "for i, stk_data in enumerate(ticks_data):\n",
    "    label_stat = gen_analysis_data(stk_data, return_period).groupby(\"label\").size()\n",
    "    label_stat.name = stk_tickers[i]\n",
    "    classes_df = pd.concat([classes_df, label_stat], axis=1)\n",
    "print(classes_df)\n",
    "classes_count = [classes_df.iloc[i].sum() for i in range(num_classes)]\n",
    "total_recs = sum(classes_count)\n",
    "for i, v in enumerate(classes_count):\n",
    "    print(f\"class {i}: {v*100/total_recs:.3f}%, {v:>5d}\")\n",
    "pyplot.bar([\"<= -0.5%\", \" between \", \">= 0.05%\"], classes_count)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, ticks_data_X, ticks_data_Y, seq_len):\n",
    "        self.ticks_data_X = ticks_data_X\n",
    "        self.ticks_data_Y = ticks_data_Y\n",
    "        self.seq_len = seq_len\n",
    "        len_array = [len(d) - self.seq_len + 1 for d in ticks_data_X]\n",
    "        self.idx_boundary = [len_array[0]]\n",
    "\n",
    "        for i in range(1, len(len_array)):\n",
    "            self.idx_boundary.append(len_array[i] + self.idx_boundary[i - 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(f\"len of dataset:{self.idx_boundary[-1]}\")\n",
    "        return self.idx_boundary[-1]  # len(self.X) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for ticks_data_idx in range(len(self.ticks_data_X)):\n",
    "            if self.idx_boundary[ticks_data_idx] > idx:\n",
    "                break\n",
    "        offset = (\n",
    "            idx if ticks_data_idx == 0 else idx - self.idx_boundary[ticks_data_idx - 1]\n",
    "        )\n",
    "        # print(f\"{ticks_data_idx}, {offset}\")\n",
    "        return (\n",
    "            np.array(self.ticks_data_X[ticks_data_idx][offset : offset + self.seq_len]),\n",
    "            int(self.ticks_data_Y[ticks_data_idx].iloc[offset + self.seq_len - 1, :]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_dataloader(_return_period, _seq_len):\n",
    "    ticks_dataset = [gen_analysis_data(d, _return_period) for d in ticks_data]\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for dataset in ticks_dataset:\n",
    "        test_size = int(dataset.shape[0] * validation_size)\n",
    "        # random.seed(42)\n",
    "        test_data_idx = random.sample(range(0, dataset.shape[0]), test_size)\n",
    "        mask = np.full(len(dataset), False)\n",
    "        mask[test_data_idx] = True\n",
    "        train_data = dataset[~mask]\n",
    "        test_data = dataset[mask]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[3:].values]\n",
    "        )\n",
    "        # print(features)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_train_data, ticks_Y_train_data, _seq_len),\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_test_data, ticks_Y_test_data, _seq_len),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, ticks_X_train_data[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class StockPCTLabelPredictLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_fc_layers,\n",
    "            activation_type,\n",
    "        )\n",
    "\n",
    "    def __init__(self, input_size, config):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_fc_layers=config[\"num_fc_layers\"],\n",
    "            activation_type=config[\"activation_type\"],\n",
    "        )\n",
    "\n",
    "    def setup_model(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \"\"\"\n",
    "            input_size    : The number of expected features in the input x\n",
    "            hidden_size   : The number of features in the hidden state h\n",
    "            num_layers    : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "            bias          : If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            batch_first   : If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "            dropout       : If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            bidirectional : If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size     : If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        layers = []\n",
    "        in_features = self.hidden_size\n",
    "        for i in range(1, num_fc_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if out_features <= num_classes:\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            (\n",
    "                layers.append(nn.ReLU() if activation_type == 1 else nn.Sigmoid())\n",
    "                if activation_type == 2\n",
    "                else nn.Tanh()\n",
    "            )\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange = 0.5\n",
    "            nn.init.uniform_(m.weight, -initrange, initrange)\n",
    "            nn.init.zeros_(m.bias)\n",
    "            # print(f\"{m.in_features},{m.out_features}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        out, (h_out, _) = self.rnn(x, (h_0, c_0))\n",
    "\n",
    "        fc_input = h_out[-1].view(-1, self.hidden_size)\n",
    "        return self.fc(fc_input)\n",
    "\n",
    "\n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        \"epoch_num\": epoch_num,\n",
    "        \"time\": str(datetime.now),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hyper_parameters\": hyper_parameters,\n",
    "    }\n",
    "    # print(f\"save model:{file_path}\")\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict[\"hyper_parameters\"]\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=data_dict[\"input_size\"],\n",
    "        hidden_size=int(hyper_parameters[\"hidden_size\"]),\n",
    "        num_layers=int(hyper_parameters[\"num_layers\"]),\n",
    "        num_fc_layers=int(hyper_parameters[\"num_fc_layers\"]),\n",
    "        activation_type=int(hyper_parameters[\"activation_type\"]),\n",
    "    )\n",
    "    model.load_state_dict(data_dict[\"model_state\"])\n",
    "    return model, hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "METRICS_LABEL_NDX = 0  # ground_truth\n",
    "METRICS_PBTY_NDX = 1  # Probability of predicition\n",
    "METRICS_PRED_NDX = 2  # class(label) of predicition\n",
    "METRICS_LOSS_NDX = 3\n",
    "METRICS_SIZE = 4\n",
    "softmax = nn.Softmax(dim=1)\n",
    "totalTrainingSamples_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:55:18.647675: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-01 13:55:18.648776: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 13:55:18.670171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 13:55:18.991524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def logMetrics(\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5,\n",
    "):\n",
    "    log.info(\n",
    "        \"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            task_name,\n",
    "        )\n",
    "    )\n",
    "    F1_rec = namedtuple(\n",
    "        \"f1_rec\",\n",
    "        \"pos_correct neg_correct pos_count neg_count pos_loss neg_loss precision recall F1\",\n",
    "    )\n",
    "    F1_metrics = []\n",
    "    for target_class in range(num_classes):\n",
    "        posLabel_mask = metrics_t[METRICS_LABEL_NDX] == target_class\n",
    "        pos_count = posLabel_mask.sum()\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] != target_class\n",
    "        neg_count = negLabel_mask.sum()\n",
    "\n",
    "        posPred_mask = metrics_t[METRICS_PRED_NDX] == target_class\n",
    "        threshold_mask = metrics_t[METRICS_PBTY_NDX] > classificationThreshold\n",
    "        # TP, truePos_count\n",
    "        TP = pos_correct = int((posLabel_mask & posPred_mask & threshold_mask).sum())\n",
    "\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] != target_class\n",
    "        # TN, trueNeg_count\n",
    "        TN = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "\n",
    "        # FP, falsePos_count\n",
    "        FP = neg_count - neg_correct\n",
    "        # FN, falseNeg_count\n",
    "        FN = pos_count - pos_correct\n",
    "\n",
    "        # precision = TP / (TP + FP)\n",
    "        precision = 0.0 if (TP + FP) == 0 else TP / np.float32(TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        recall = 0.0 if (TP + FN) == 0 else TP / np.float32(TP + FN)\n",
    "        # F1 = 2 * precision * recall / (precision + recall)\n",
    "        F1 = (\n",
    "            0.0\n",
    "            if (precision + recall) == 0.0\n",
    "            else (2 * precision * recall) / np.float32(precision + recall)\n",
    "        )\n",
    "        F1_metrics.append(\n",
    "            F1_rec(\n",
    "                pos_correct,\n",
    "                neg_correct,\n",
    "                pos_count,\n",
    "                neg_count,\n",
    "                metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean(),\n",
    "                metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean(),\n",
    "                precision,\n",
    "                recall,\n",
    "                F1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"loss/all\"] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "    log.info(\n",
    "        (\"E{} {:8} {loss/all:.4f} loss\").format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            **metrics_dict,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for target_class, rec in enumerate(F1_metrics):\n",
    "        metrics_dict[f\"{target_class} loss/pos\"] = rec.pos_loss\n",
    "        metrics_dict[f\"{target_class} loss/neg\"] = rec.neg_loss\n",
    "        metrics_dict[f\"{target_class} correct/all\"] = (\n",
    "            (rec.pos_correct + rec.neg_correct) / metrics_t.shape[1] * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} correct/neg\"] = (\n",
    "            (rec.neg_correct) / rec.neg_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} correct/pos\"] = (\n",
    "            (rec.pos_correct) / rec.pos_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} pr/precision\"] = rec.precision\n",
    "        metrics_dict[f\"{target_class} pr/recall\"] = rec.recall\n",
    "        metrics_dict[f\"{target_class} pr/f1_score\"] = rec.F1\n",
    "\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} class {} {\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" correct/all:-5.1f}% correct, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" pr/precision:.4f} precision, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" pr/recall:.4f} recall, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" pr/f1_score:.4f} f1 score\"\n",
    "            ).format(epoch_ndx, mode_str, target_class, **metrics_dict)\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} class {} {\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" loss/neg:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_neg\",\n",
    "                target_class,\n",
    "                neg_correct=rec.neg_correct,\n",
    "                neg_count=rec.neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} class {} {\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" loss/pos:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class}\"\n",
    "                + \" correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_pos\",\n",
    "                target_class,\n",
    "                pos_correct=rec.pos_correct,\n",
    "                pos_count=rec.pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir + f\"-{mode_str}_cls\")\n",
    "    for key, value in metrics_dict.items():\n",
    "        writer.add_scalar(key, value, totalTrainingSamples_count)\n",
    "\n",
    "    writer.add_pr_curve(\n",
    "        \"pr\",\n",
    "        metrics_t[METRICS_LABEL_NDX],\n",
    "        metrics_t[METRICS_PRED_NDX],\n",
    "        totalTrainingSamples_count,\n",
    "    )\n",
    "\n",
    "    writer.close()\n",
    "    # bins = [x / 50.0 for x in range(51)]\n",
    "\n",
    "    # negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
    "    # posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
    "\n",
    "    # if negHist_mask.any():\n",
    "    #     writer.add_histogram(\n",
    "    #         \"is_neg\",\n",
    "    #         metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "    #         self.totalTrainingSamples_count,\n",
    "    #         bins=bins,\n",
    "    #     )\n",
    "    # if posHist_mask.any():\n",
    "    #     writer.add_histogram(\n",
    "    #         \"is_pos\",\n",
    "    #         metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
    "    #         self.totalTrainingSamples_count,\n",
    "    #         bins=bins,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBatchLoss(model, loss_fn, x, y, metrics_g, batch_idx):\n",
    "    x_g = x.to(device)\n",
    "    y_g = y.to(device)\n",
    "    outputs = model(x_g)\n",
    "    loss_g = loss_fn(outputs, y_g)\n",
    "    probability_g, predition_g = torch.max(softmax(outputs), dim=1)\n",
    "\n",
    "    start_ndx = batch_idx * batch_size\n",
    "    end_ndx = start_ndx + y.size(0)\n",
    "\n",
    "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = y_g\n",
    "    metrics_g[METRICS_PBTY_NDX, start_ndx:end_ndx] = probability_g\n",
    "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predition_g\n",
    "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
    "\n",
    "    return loss_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutil.util import enumerateWithEstimate\n",
    "import torch\n",
    "\n",
    "\n",
    "def doTraining(model, optimizer, loss_fn, epoch_ndx, train_dl):\n",
    "    global totalTrainingSamples_count\n",
    "    model.train()\n",
    "    trnMetrics_g = torch.zeros(\n",
    "        METRICS_SIZE,\n",
    "        len(train_dl.dataset),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, (x, y) in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = computeBatchLoss(\n",
    "            model,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            trnMetrics_g,\n",
    "            batch_ndx,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    totalTrainingSamples_count += len(train_dl.dataset)\n",
    "    return trnMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(model, loss_fn, epoch_ndx, val_dl):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, (x, y) in batch_iter:\n",
    "            computeBatchLoss(model, loss_fn, x, y, valMetrics_g, batch_ndx)\n",
    "\n",
    "    return valMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(config):\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    print(id_str)\n",
    "\n",
    "    train_loader, test_loader, features_size = prepare_dataloader(\n",
    "        config[\"return_period\"], config[\"seq_len\"]\n",
    "    )\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        logMetrics(epoch_ndx, \"trn\", trnMetrics_t, classificationThreshold)\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        logMetrics(epoch_ndx, \"val\", valMetrics_t, classificationThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-02-01_13.56.51\n",
      "5_5_0.01_0.11646759543664197_1_4_64_1_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:56:51,781 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.001033 secs\n",
      "2024-02-01 13:56:51,783 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000789 secs\n",
      "2024-02-01 13:56:51,784 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000812 secs\n",
      "2024-02-01 13:56:51,787 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.00037 secs\n",
      "2024-02-01 13:56:51,788 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.00034 secs\n",
      "2024-02-01 13:56:51,789 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000357 secs\n",
      "2024-02-01 13:56:51,793 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000825 secs\n",
      "2024-02-01 13:56:51,794 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000812 secs\n",
      "2024-02-01 13:56:51,796 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000817 secs\n",
      "2024-02-01 13:56:51,799 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000352 secs\n",
      "2024-02-01 13:56:51,800 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000355 secs\n",
      "2024-02-01 13:56:51,800 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000335 secs\n",
      "2024-02-01 13:56:51,805 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000829 secs\n",
      "2024-02-01 13:56:51,806 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000767 secs\n",
      "2024-02-01 13:56:51,807 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000771 secs\n",
      "2024-02-01 13:56:51,810 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000355 secs\n",
      "2024-02-01 13:56:51,811 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000343 secs\n",
      "2024-02-01 13:56:51,812 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000369 secs\n",
      "2024-02-01 13:56:51,816 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000838 secs\n",
      "2024-02-01 13:56:51,817 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000744 secs\n",
      "2024-02-01 13:56:51,819 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000795 secs\n",
      "2024-02-01 13:56:51,822 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000358 secs\n",
      "2024-02-01 13:56:51,823 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.00033 secs\n",
      "2024-02-01 13:56:51,824 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000342 secs\n",
      "2024-02-01 13:56:51,828 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000842 secs\n",
      "2024-02-01 13:56:51,829 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000763 secs\n",
      "2024-02-01 13:56:51,831 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000806 secs\n",
      "2024-02-01 13:56:51,834 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000362 secs\n",
      "2024-02-01 13:56:51,835 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000332 secs\n",
      "2024-02-01 13:56:51,835 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.00036 secs\n",
      "2024-02-01 13:56:51,840 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000879 secs\n",
      "2024-02-01 13:56:51,842 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.00087 secs\n",
      "2024-02-01 13:56:51,843 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000775 secs\n",
      "2024-02-01 13:56:51,846 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000362 secs\n",
      "2024-02-01 13:56:51,847 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000382 secs\n",
      "2024-02-01 13:56:51,848 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000462 secs\n",
      "2024-02-01 13:56:51,855 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.001543 secs\n",
      "2024-02-01 13:56:51,856 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000797 secs\n",
      "2024-02-01 13:56:51,858 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000833 secs\n",
      "2024-02-01 13:56:51,861 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000351 secs\n",
      "2024-02-01 13:56:51,862 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000332 secs\n",
      "2024-02-01 13:56:51,862 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000329 secs\n",
      "2024-02-01 13:56:51,870 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000847 secs\n",
      "2024-02-01 13:56:51,871 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000812 secs\n",
      "2024-02-01 13:56:51,872 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.001091 secs\n",
      "2024-02-01 13:56:51,876 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000355 secs\n",
      "2024-02-01 13:56:51,877 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000457 secs\n",
      "2024-02-01 13:56:51,877 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000334 secs\n",
      "2024-02-01 13:56:51,882 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.001043 secs\n",
      "2024-02-01 13:56:51,884 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.001056 secs\n",
      "2024-02-01 13:56:51,885 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000885 secs\n",
      "2024-02-01 13:56:51,889 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.00037 secs\n",
      "2024-02-01 13:56:51,890 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000332 secs\n",
      "2024-02-01 13:56:51,891 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000681 secs\n",
      "2024-02-01 13:56:51,896 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000828 secs\n",
      "2024-02-01 13:56:51,897 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.00081 secs\n",
      "2024-02-01 13:56:51,898 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.00074 secs\n",
      "2024-02-01 13:56:51,901 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000356 secs\n",
      "2024-02-01 13:56:51,902 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.00043 secs\n",
      "2024-02-01 13:56:51,903 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000349 secs\n",
      "2024-02-01 13:56:51,907 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000796 secs\n",
      "2024-02-01 13:56:51,908 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000814 secs\n",
      "2024-02-01 13:56:51,909 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000788 secs\n",
      "2024-02-01 13:56:51,913 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000344 secs\n",
      "2024-02-01 13:56:51,913 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000346 secs\n",
      "2024-02-01 13:56:51,914 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000342 secs\n",
      "2024-02-01 13:56:51,919 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000797 secs\n",
      "2024-02-01 13:56:51,920 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000875 secs\n",
      "2024-02-01 13:56:51,921 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000748 secs\n",
      "2024-02-01 13:56:51,924 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000342 secs\n",
      "2024-02-01 13:56:51,925 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000329 secs\n",
      "2024-02-01 13:56:51,925 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000327 secs\n",
      "2024-02-01 13:56:51,930 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000811 secs\n",
      "2024-02-01 13:56:51,932 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.001437 secs\n",
      "2024-02-01 13:56:51,933 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.001101 secs\n",
      "2024-02-01 13:56:51,937 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000388 secs\n",
      "2024-02-01 13:56:51,938 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000346 secs\n",
      "2024-02-01 13:56:51,938 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000354 secs\n",
      "2024-02-01 13:56:51,943 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.00081 secs\n",
      "2024-02-01 13:56:51,944 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000818 secs\n",
      "2024-02-01 13:56:51,945 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000728 secs\n",
      "2024-02-01 13:56:51,948 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000342 secs\n",
      "2024-02-01 13:56:51,949 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000327 secs\n",
      "2024-02-01 13:56:51,950 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000346 secs\n",
      "2024-02-01 13:56:51,954 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000825 secs\n",
      "2024-02-01 13:56:51,955 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000823 secs\n",
      "2024-02-01 13:56:51,956 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000765 secs\n",
      "2024-02-01 13:56:51,960 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000343 secs\n",
      "2024-02-01 13:56:51,960 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000336 secs\n",
      "2024-02-01 13:56:51,961 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.00034 secs\n",
      "2024-02-01 13:56:51,969 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.001521 secs\n",
      "2024-02-01 13:56:51,971 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.001494 secs\n",
      "2024-02-01 13:56:51,973 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.001307 secs\n",
      "2024-02-01 13:56:51,978 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000388 secs\n",
      "2024-02-01 13:56:51,978 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000349 secs\n",
      "2024-02-01 13:56:51,979 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.00037 secs\n",
      "2024-02-01 13:56:51,983 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000866 secs\n",
      "2024-02-01 13:56:51,985 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.00085 secs\n",
      "2024-02-01 13:56:51,986 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000766 secs\n",
      "2024-02-01 13:56:51,989 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000345 secs\n",
      "2024-02-01 13:56:51,990 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000488 secs\n",
      "2024-02-01 13:56:51,991 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000342 secs\n",
      "2024-02-01 13:56:51,995 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000861 secs\n",
      "2024-02-01 13:56:51,996 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000819 secs\n",
      "2024-02-01 13:56:51,998 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000762 secs\n",
      "2024-02-01 13:56:52,001 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000371 secs\n",
      "2024-02-01 13:56:52,002 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000349 secs\n",
      "2024-02-01 13:56:52,002 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000362 secs\n",
      "2024-02-01 13:56:52,007 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.001008 secs\n",
      "2024-02-01 13:56:52,009 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.001002 secs\n",
      "2024-02-01 13:56:52,010 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000755 secs\n",
      "2024-02-01 13:56:52,013 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000344 secs\n",
      "2024-02-01 13:56:52,014 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000321 secs\n",
      "2024-02-01 13:56:52,014 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000322 secs\n",
      "2024-02-01 13:56:52,019 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['ADOSC_3_10']: 0.000816 secs\n",
      "2024-02-01 13:56:52,020 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVO_34_55_13']: 0.000912 secs\n",
      "2024-02-01 13:56:52,021 INFO     pid:9731 sklearn_pandas:343:_transform [FIT_TRANSFORM] ['KVOs_34_55_13']: 0.000797 secs\n",
      "2024-02-01 13:56:52,025 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['ADOSC_3_10']: 0.000353 secs\n",
      "2024-02-01 13:56:52,025 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVO_34_55_13']: 0.000337 secs\n",
      "2024-02-01 13:56:52,026 INFO     pid:9731 sklearn_pandas:353:_transform [TRANSFORM] ['KVOs_34_55_13']: 0.000328 secs\n",
      "2024-02-01 13:56:52,043 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E0 Training ----/1155, starting\n",
      "2024-02-01 13:56:52,138 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Training   16/1155, done at 2024-02-01 13:56:56, 0:00:04\n",
      "2024-02-01 13:56:52,195 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Training   64/1155, done at 2024-02-01 13:56:54, 0:00:02\n",
      "2024-02-01 13:56:52,410 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Training  256/1155, done at 2024-02-01 13:56:53, 0:00:01\n",
      "2024-02-01 13:56:53,402 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Training 1024/1155, done at 2024-02-01 13:56:53, 0:00:01\n",
      "2024-02-01 13:56:53,564 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E0 Training 1155/1155, done at 2024-02-01 13:56:53\n",
      "2024-02-01 13:56:53,564 INFO     pid:9731 __main__:011:logMetrics E0 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:53,566 INFO     pid:9731 __main__:068:logMetrics E0 trn      0.5073 loss\n",
      "2024-02-01 13:56:53,567 INFO     pid:9731 __main__:092:logMetrics E0 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:53,567 INFO     pid:9731 __main__:108:logMetrics E0 trn_neg  class 0 0.4718 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:56:53,567 INFO     pid:9731 __main__:125:logMetrics E0 trn_pos  class 0 4.8070 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:56:53,567 INFO     pid:9731 __main__:092:logMetrics E0 trn      class 1  81.6% correct, 0.8167 precision, 0.9991 recall, 0.8988 f1 score\n",
      "2024-02-01 13:56:53,567 INFO     pid:9731 __main__:108:logMetrics E0 trn_neg  class 1 1.8577 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:56:53,568 INFO     pid:9731 __main__:125:logMetrics E0 trn_pos  class 1 0.2045 loss,  99.9% correct (30149 of 30175)\n",
      "2024-02-01 13:56:53,568 INFO     pid:9731 __main__:092:logMetrics E0 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:53,568 INFO     pid:9731 __main__:108:logMetrics E0 trn_neg  class 2 0.2501 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:56:53,568 INFO     pid:9731 __main__:125:logMetrics E0 trn_pos  class 2 1.7199 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:56:53,570 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E0 Validation  ----/287, starting\n",
      "2024-02-01 13:56:53,664 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Validation    16/287, done at 2024-02-01 13:56:53, 0:00:00\n",
      "2024-02-01 13:56:53,712 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Validation    64/287, done at 2024-02-01 13:56:53, 0:00:00\n",
      "2024-02-01 13:56:53,898 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E0 Validation   256/287, done at 2024-02-01 13:56:53, 0:00:00\n",
      "2024-02-01 13:56:53,937 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E0 Validation  287/287, done at 2024-02-01 13:56:53\n",
      "2024-02-01 13:56:53,938 INFO     pid:9731 __main__:011:logMetrics E0 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:53,939 INFO     pid:9731 __main__:068:logMetrics E0 val      0.5122 loss\n",
      "2024-02-01 13:56:53,939 INFO     pid:9731 __main__:092:logMetrics E0 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:53,939 INFO     pid:9731 __main__:108:logMetrics E0 val_neg  class 0 0.4698 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:56:53,939 INFO     pid:9731 __main__:125:logMetrics E0 val_pos  class 0 4.8884 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:56:53,939 INFO     pid:9731 __main__:092:logMetrics E0 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:56:53,940 INFO     pid:9731 __main__:108:logMetrics E0 val_neg  class 1 1.9327 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:56:53,940 INFO     pid:9731 __main__:125:logMetrics E0 val_pos  class 1 0.1791 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:56:53,940 INFO     pid:9731 __main__:092:logMetrics E0 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:53,940 INFO     pid:9731 __main__:108:logMetrics E0 val_neg  class 2 0.2343 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:56:53,941 INFO     pid:9731 __main__:125:logMetrics E0 val_pos  class 2 1.7752 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:56:53,945 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E1 Training ----/1155, starting\n",
      "2024-02-01 13:56:54,022 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Training   16/1155, done at 2024-02-01 13:56:55, 0:00:01\n",
      "2024-02-01 13:56:54,081 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Training   64/1155, done at 2024-02-01 13:56:55, 0:00:01\n",
      "2024-02-01 13:56:54,280 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Training  256/1155, done at 2024-02-01 13:56:55, 0:00:01\n",
      "2024-02-01 13:56:55,297 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Training 1024/1155, done at 2024-02-01 13:56:55, 0:00:01\n",
      "2024-02-01 13:56:55,452 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E1 Training 1155/1155, done at 2024-02-01 13:56:55\n",
      "2024-02-01 13:56:55,462 INFO     pid:9731 __main__:011:logMetrics E1 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:55,464 INFO     pid:9731 __main__:068:logMetrics E1 trn      0.5023 loss\n",
      "2024-02-01 13:56:55,465 INFO     pid:9731 __main__:092:logMetrics E1 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:55,465 INFO     pid:9731 __main__:108:logMetrics E1 trn_neg  class 0 0.4672 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:56:55,465 INFO     pid:9731 __main__:125:logMetrics E1 trn_pos  class 0 4.7713 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:56:55,466 INFO     pid:9731 __main__:092:logMetrics E1 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:56:55,466 INFO     pid:9731 __main__:108:logMetrics E1 trn_neg  class 1 1.8486 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:56:55,466 INFO     pid:9731 __main__:125:logMetrics E1 trn_pos  class 1 0.2005 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:56:55,467 INFO     pid:9731 __main__:092:logMetrics E1 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:55,467 INFO     pid:9731 __main__:108:logMetrics E1 trn_neg  class 2 0.2458 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:56:55,467 INFO     pid:9731 __main__:125:logMetrics E1 trn_pos  class 2 1.7121 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:56:55,470 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E1 Validation  ----/287, starting\n",
      "2024-02-01 13:56:55,515 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Validation    16/287, done at 2024-02-01 13:56:55, 0:00:00\n",
      "2024-02-01 13:56:55,559 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Validation    64/287, done at 2024-02-01 13:56:55, 0:00:00\n",
      "2024-02-01 13:56:55,772 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E1 Validation   256/287, done at 2024-02-01 13:56:55, 0:00:00\n",
      "2024-02-01 13:56:55,813 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E1 Validation  287/287, done at 2024-02-01 13:56:55\n",
      "2024-02-01 13:56:55,814 INFO     pid:9731 __main__:011:logMetrics E1 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:55,815 INFO     pid:9731 __main__:068:logMetrics E1 val      0.5134 loss\n",
      "2024-02-01 13:56:55,816 INFO     pid:9731 __main__:092:logMetrics E1 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:55,816 INFO     pid:9731 __main__:108:logMetrics E1 val_neg  class 0 0.4734 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:56:55,816 INFO     pid:9731 __main__:125:logMetrics E1 val_pos  class 0 4.6303 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:56:55,817 INFO     pid:9731 __main__:092:logMetrics E1 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:56:55,817 INFO     pid:9731 __main__:108:logMetrics E1 val_neg  class 1 1.6926 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:56:55,817 INFO     pid:9731 __main__:125:logMetrics E1 val_pos  class 1 0.2368 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:56:55,817 INFO     pid:9731 __main__:092:logMetrics E1 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:55,817 INFO     pid:9731 __main__:108:logMetrics E1 val_neg  class 2 0.2883 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:56:55,818 INFO     pid:9731 __main__:125:logMetrics E1 val_pos  class 2 1.5361 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:56:55,820 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E2 Training ----/1155, starting\n",
      "2024-02-01 13:56:55,911 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Training   16/1155, done at 2024-02-01 13:56:57, 0:00:01\n",
      "2024-02-01 13:56:55,966 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Training   64/1155, done at 2024-02-01 13:56:57, 0:00:01\n",
      "2024-02-01 13:56:56,184 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Training  256/1155, done at 2024-02-01 13:56:57, 0:00:01\n",
      "2024-02-01 13:56:57,314 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Training 1024/1155, done at 2024-02-01 13:56:57, 0:00:01\n",
      "2024-02-01 13:56:57,458 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E2 Training 1155/1155, done at 2024-02-01 13:56:57\n",
      "2024-02-01 13:56:57,468 INFO     pid:9731 __main__:011:logMetrics E2 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:57,470 INFO     pid:9731 __main__:068:logMetrics E2 trn      0.4998 loss\n",
      "2024-02-01 13:56:57,470 INFO     pid:9731 __main__:092:logMetrics E2 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:57,471 INFO     pid:9731 __main__:108:logMetrics E2 trn_neg  class 0 0.4645 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:56:57,471 INFO     pid:9731 __main__:125:logMetrics E2 trn_pos  class 0 4.7856 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:56:57,471 INFO     pid:9731 __main__:092:logMetrics E2 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:56:57,471 INFO     pid:9731 __main__:108:logMetrics E2 trn_neg  class 1 1.8366 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:56:57,472 INFO     pid:9731 __main__:125:logMetrics E2 trn_pos  class 1 0.2002 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:56:57,472 INFO     pid:9731 __main__:092:logMetrics E2 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:57,472 INFO     pid:9731 __main__:108:logMetrics E2 trn_neg  class 2 0.2456 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:56:57,472 INFO     pid:9731 __main__:125:logMetrics E2 trn_pos  class 2 1.6988 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:56:57,475 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E2 Validation  ----/287, starting\n",
      "2024-02-01 13:56:57,520 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Validation    16/287, done at 2024-02-01 13:56:57, 0:00:00\n",
      "2024-02-01 13:56:57,593 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Validation    64/287, done at 2024-02-01 13:56:57, 0:00:00\n",
      "2024-02-01 13:56:57,771 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E2 Validation   256/287, done at 2024-02-01 13:56:57, 0:00:00\n",
      "2024-02-01 13:56:57,806 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E2 Validation  287/287, done at 2024-02-01 13:56:57\n",
      "2024-02-01 13:56:57,807 INFO     pid:9731 __main__:011:logMetrics E2 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:57,808 INFO     pid:9731 __main__:068:logMetrics E2 val      0.5133 loss\n",
      "2024-02-01 13:56:57,808 INFO     pid:9731 __main__:092:logMetrics E2 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:57,809 INFO     pid:9731 __main__:108:logMetrics E2 val_neg  class 0 0.4744 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:56:57,809 INFO     pid:9731 __main__:125:logMetrics E2 val_pos  class 0 4.5182 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:56:57,809 INFO     pid:9731 __main__:092:logMetrics E2 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:56:57,809 INFO     pid:9731 __main__:108:logMetrics E2 val_neg  class 1 1.9238 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:56:57,810 INFO     pid:9731 __main__:125:logMetrics E2 val_pos  class 1 0.1825 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:56:57,810 INFO     pid:9731 __main__:092:logMetrics E2 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:57,810 INFO     pid:9731 __main__:108:logMetrics E2 val_neg  class 2 0.2333 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:56:57,810 INFO     pid:9731 __main__:125:logMetrics E2 val_pos  class 2 1.7856 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:56:57,813 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E3 Training ----/1155, starting\n",
      "2024-02-01 13:56:57,865 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Training   16/1155, done at 2024-02-01 13:56:59, 0:00:01\n",
      "2024-02-01 13:56:57,921 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Training   64/1155, done at 2024-02-01 13:56:59, 0:00:01\n",
      "2024-02-01 13:56:58,168 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Training  256/1155, done at 2024-02-01 13:56:59, 0:00:01\n",
      "2024-02-01 13:56:59,132 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Training 1024/1155, done at 2024-02-01 13:56:59, 0:00:01\n",
      "2024-02-01 13:56:59,289 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E3 Training 1155/1155, done at 2024-02-01 13:56:59\n",
      "2024-02-01 13:56:59,300 INFO     pid:9731 __main__:011:logMetrics E3 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:59,302 INFO     pid:9731 __main__:068:logMetrics E3 trn      0.4987 loss\n",
      "2024-02-01 13:56:59,302 INFO     pid:9731 __main__:092:logMetrics E3 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:59,302 INFO     pid:9731 __main__:108:logMetrics E3 trn_neg  class 0 0.4637 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:125:logMetrics E3 trn_pos  class 0 4.7350 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:092:logMetrics E3 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:108:logMetrics E3 trn_neg  class 1 1.8303 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:125:logMetrics E3 trn_pos  class 1 0.2001 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:092:logMetrics E3 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:59,303 INFO     pid:9731 __main__:108:logMetrics E3 trn_neg  class 2 0.2450 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:56:59,304 INFO     pid:9731 __main__:125:logMetrics E3 trn_pos  class 2 1.6946 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:56:59,307 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E3 Validation  ----/287, starting\n",
      "2024-02-01 13:56:59,355 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Validation    16/287, done at 2024-02-01 13:56:59, 0:00:00\n",
      "2024-02-01 13:56:59,446 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Validation    64/287, done at 2024-02-01 13:56:59, 0:00:00\n",
      "2024-02-01 13:56:59,620 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E3 Validation   256/287, done at 2024-02-01 13:56:59, 0:00:00\n",
      "2024-02-01 13:56:59,656 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E3 Validation  287/287, done at 2024-02-01 13:56:59\n",
      "2024-02-01 13:56:59,657 INFO     pid:9731 __main__:011:logMetrics E3 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:56:59,659 INFO     pid:9731 __main__:068:logMetrics E3 val      0.5202 loss\n",
      "2024-02-01 13:56:59,659 INFO     pid:9731 __main__:092:logMetrics E3 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:59,659 INFO     pid:9731 __main__:108:logMetrics E3 val_neg  class 0 0.4767 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:56:59,659 INFO     pid:9731 __main__:125:logMetrics E3 val_pos  class 0 5.0026 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:092:logMetrics E3 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:108:logMetrics E3 val_neg  class 1 2.1072 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:125:logMetrics E3 val_pos  class 1 0.1480 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:092:logMetrics E3 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:108:logMetrics E3 val_neg  class 2 0.2049 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:56:59,660 INFO     pid:9731 __main__:125:logMetrics E3 val_pos  class 2 1.9530 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:56:59,662 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E4 Training ----/1155, starting\n",
      "2024-02-01 13:56:59,715 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Training   16/1155, done at 2024-02-01 13:57:00, 0:00:01\n",
      "2024-02-01 13:56:59,774 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Training   64/1155, done at 2024-02-01 13:57:01, 0:00:01\n",
      "2024-02-01 13:57:00,002 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Training  256/1155, done at 2024-02-01 13:57:01, 0:00:01\n",
      "2024-02-01 13:57:00,970 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Training 1024/1155, done at 2024-02-01 13:57:01, 0:00:01\n",
      "2024-02-01 13:57:01,122 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E4 Training 1155/1155, done at 2024-02-01 13:57:01\n",
      "2024-02-01 13:57:01,132 INFO     pid:9731 __main__:011:logMetrics E4 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:01,134 INFO     pid:9731 __main__:068:logMetrics E4 trn      0.4985 loss\n",
      "2024-02-01 13:57:01,135 INFO     pid:9731 __main__:092:logMetrics E4 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:01,135 INFO     pid:9731 __main__:108:logMetrics E4 trn_neg  class 0 0.4632 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:01,135 INFO     pid:9731 __main__:125:logMetrics E4 trn_pos  class 0 4.7820 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:01,136 INFO     pid:9731 __main__:092:logMetrics E4 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:01,136 INFO     pid:9731 __main__:108:logMetrics E4 trn_neg  class 1 1.8340 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:01,136 INFO     pid:9731 __main__:125:logMetrics E4 trn_pos  class 1 0.1991 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:01,136 INFO     pid:9731 __main__:092:logMetrics E4 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:01,137 INFO     pid:9731 __main__:108:logMetrics E4 trn_neg  class 2 0.2445 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:01,137 INFO     pid:9731 __main__:125:logMetrics E4 trn_pos  class 2 1.6962 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:01,139 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E4 Validation  ----/287, starting\n",
      "2024-02-01 13:57:01,184 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Validation    16/287, done at 2024-02-01 13:57:01, 0:00:00\n",
      "2024-02-01 13:57:01,263 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Validation    64/287, done at 2024-02-01 13:57:01, 0:00:00\n",
      "2024-02-01 13:57:01,438 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E4 Validation   256/287, done at 2024-02-01 13:57:01, 0:00:00\n",
      "2024-02-01 13:57:01,474 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E4 Validation  287/287, done at 2024-02-01 13:57:01\n",
      "2024-02-01 13:57:01,475 INFO     pid:9731 __main__:011:logMetrics E4 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:01,476 INFO     pid:9731 __main__:068:logMetrics E4 val      0.5111 loss\n",
      "2024-02-01 13:57:01,476 INFO     pid:9731 __main__:092:logMetrics E4 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:01,476 INFO     pid:9731 __main__:108:logMetrics E4 val_neg  class 0 0.4717 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:01,476 INFO     pid:9731 __main__:125:logMetrics E4 val_pos  class 0 4.5773 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:092:logMetrics E4 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:108:logMetrics E4 val_neg  class 1 1.7587 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:125:logMetrics E4 val_pos  class 1 0.2186 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:092:logMetrics E4 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:108:logMetrics E4 val_neg  class 2 0.2697 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:01,477 INFO     pid:9731 __main__:125:logMetrics E4 val_pos  class 2 1.6086 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:01,479 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E5 Training ----/1155, starting\n",
      "2024-02-01 13:57:01,557 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Training   16/1155, done at 2024-02-01 13:57:02, 0:00:01\n",
      "2024-02-01 13:57:01,614 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Training   64/1155, done at 2024-02-01 13:57:02, 0:00:01\n",
      "2024-02-01 13:57:01,812 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Training  256/1155, done at 2024-02-01 13:57:02, 0:00:01\n",
      "2024-02-01 13:57:02,789 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Training 1024/1155, done at 2024-02-01 13:57:02, 0:00:01\n",
      "2024-02-01 13:57:02,945 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E5 Training 1155/1155, done at 2024-02-01 13:57:02\n",
      "2024-02-01 13:57:02,956 INFO     pid:9731 __main__:011:logMetrics E5 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:02,958 INFO     pid:9731 __main__:068:logMetrics E5 trn      0.4975 loss\n",
      "2024-02-01 13:57:02,959 INFO     pid:9731 __main__:092:logMetrics E5 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:02,959 INFO     pid:9731 __main__:108:logMetrics E5 trn_neg  class 0 0.4630 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:02,960 INFO     pid:9731 __main__:125:logMetrics E5 trn_pos  class 0 4.6812 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:02,960 INFO     pid:9731 __main__:092:logMetrics E5 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:02,960 INFO     pid:9731 __main__:108:logMetrics E5 trn_neg  class 1 1.8261 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:02,960 INFO     pid:9731 __main__:125:logMetrics E5 trn_pos  class 1 0.1996 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:02,961 INFO     pid:9731 __main__:092:logMetrics E5 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:02,961 INFO     pid:9731 __main__:108:logMetrics E5 trn_neg  class 2 0.2440 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:02,961 INFO     pid:9731 __main__:125:logMetrics E5 trn_pos  class 2 1.6927 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:02,964 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E5 Validation  ----/287, starting\n",
      "2024-02-01 13:57:03,012 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Validation    16/287, done at 2024-02-01 13:57:03, 0:00:00\n",
      "2024-02-01 13:57:03,087 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Validation    64/287, done at 2024-02-01 13:57:03, 0:00:00\n",
      "2024-02-01 13:57:03,271 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E5 Validation   256/287, done at 2024-02-01 13:57:03, 0:00:00\n",
      "2024-02-01 13:57:03,310 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E5 Validation  287/287, done at 2024-02-01 13:57:03\n",
      "2024-02-01 13:57:03,311 INFO     pid:9731 __main__:011:logMetrics E5 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:03,312 INFO     pid:9731 __main__:068:logMetrics E5 val      0.5144 loss\n",
      "2024-02-01 13:57:03,312 INFO     pid:9731 __main__:092:logMetrics E5 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:03,312 INFO     pid:9731 __main__:108:logMetrics E5 val_neg  class 0 0.4696 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:125:logMetrics E5 val_pos  class 0 5.1344 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:092:logMetrics E5 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:108:logMetrics E5 val_neg  class 1 1.7139 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:125:logMetrics E5 val_pos  class 1 0.2331 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:092:logMetrics E5 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:03,313 INFO     pid:9731 __main__:108:logMetrics E5 val_neg  class 2 0.2906 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:03,314 INFO     pid:9731 __main__:125:logMetrics E5 val_pos  class 2 1.5317 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:03,316 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E6 Training ----/1155, starting\n",
      "2024-02-01 13:57:03,377 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Training   16/1155, done at 2024-02-01 13:57:04, 0:00:01\n",
      "2024-02-01 13:57:03,435 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Training   64/1155, done at 2024-02-01 13:57:04, 0:00:01\n",
      "2024-02-01 13:57:03,661 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Training  256/1155, done at 2024-02-01 13:57:04, 0:00:01\n",
      "2024-02-01 13:57:04,774 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Training 1024/1155, done at 2024-02-01 13:57:04, 0:00:01\n",
      "2024-02-01 13:57:04,923 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E6 Training 1155/1155, done at 2024-02-01 13:57:04\n",
      "2024-02-01 13:57:04,933 INFO     pid:9731 __main__:011:logMetrics E6 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:04,936 INFO     pid:9731 __main__:068:logMetrics E6 trn      0.4965 loss\n",
      "2024-02-01 13:57:04,936 INFO     pid:9731 __main__:092:logMetrics E6 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:04,936 INFO     pid:9731 __main__:108:logMetrics E6 trn_neg  class 0 0.4616 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:04,937 INFO     pid:9731 __main__:125:logMetrics E6 trn_pos  class 0 4.7356 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:04,937 INFO     pid:9731 __main__:092:logMetrics E6 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:04,938 INFO     pid:9731 __main__:108:logMetrics E6 trn_neg  class 1 1.8217 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:04,938 INFO     pid:9731 __main__:125:logMetrics E6 trn_pos  class 1 0.1994 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:04,938 INFO     pid:9731 __main__:092:logMetrics E6 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:04,939 INFO     pid:9731 __main__:108:logMetrics E6 trn_neg  class 2 0.2444 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:04,939 INFO     pid:9731 __main__:125:logMetrics E6 trn_pos  class 2 1.6856 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:04,950 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E6 Validation  ----/287, starting\n",
      "2024-02-01 13:57:04,996 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Validation    16/287, done at 2024-02-01 13:57:05, 0:00:00\n",
      "2024-02-01 13:57:05,070 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Validation    64/287, done at 2024-02-01 13:57:05, 0:00:00\n",
      "2024-02-01 13:57:05,256 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E6 Validation   256/287, done at 2024-02-01 13:57:05, 0:00:00\n",
      "2024-02-01 13:57:05,295 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E6 Validation  287/287, done at 2024-02-01 13:57:05\n",
      "2024-02-01 13:57:05,296 INFO     pid:9731 __main__:011:logMetrics E6 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:05,297 INFO     pid:9731 __main__:068:logMetrics E6 val      0.5119 loss\n",
      "2024-02-01 13:57:05,298 INFO     pid:9731 __main__:092:logMetrics E6 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:05,298 INFO     pid:9731 __main__:108:logMetrics E6 val_neg  class 0 0.4673 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:05,298 INFO     pid:9731 __main__:125:logMetrics E6 val_pos  class 0 5.1136 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:05,298 INFO     pid:9731 __main__:092:logMetrics E6 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:05,299 INFO     pid:9731 __main__:108:logMetrics E6 val_neg  class 1 1.8549 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:05,299 INFO     pid:9731 __main__:125:logMetrics E6 val_pos  class 1 0.1970 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:05,299 INFO     pid:9731 __main__:092:logMetrics E6 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:05,299 INFO     pid:9731 __main__:108:logMetrics E6 val_neg  class 2 0.2546 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:05,299 INFO     pid:9731 __main__:125:logMetrics E6 val_pos  class 2 1.6813 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:05,302 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E7 Training ----/1155, starting\n",
      "2024-02-01 13:57:05,354 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Training   16/1155, done at 2024-02-01 13:57:06, 0:00:01\n",
      "2024-02-01 13:57:05,404 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Training   64/1155, done at 2024-02-01 13:57:06, 0:00:01\n",
      "2024-02-01 13:57:05,607 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Training  256/1155, done at 2024-02-01 13:57:06, 0:00:01\n",
      "2024-02-01 13:57:06,620 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Training 1024/1155, done at 2024-02-01 13:57:06, 0:00:01\n",
      "2024-02-01 13:57:06,775 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E7 Training 1155/1155, done at 2024-02-01 13:57:06\n",
      "2024-02-01 13:57:06,785 INFO     pid:9731 __main__:011:logMetrics E7 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:06,788 INFO     pid:9731 __main__:068:logMetrics E7 trn      0.4986 loss\n",
      "2024-02-01 13:57:06,788 INFO     pid:9731 __main__:092:logMetrics E7 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:06,788 INFO     pid:9731 __main__:108:logMetrics E7 trn_neg  class 0 0.4634 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:06,788 INFO     pid:9731 __main__:125:logMetrics E7 trn_pos  class 0 4.7662 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:092:logMetrics E7 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:108:logMetrics E7 trn_neg  class 1 1.8351 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:125:logMetrics E7 trn_pos  class 1 0.1990 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:092:logMetrics E7 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:108:logMetrics E7 trn_neg  class 2 0.2442 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:06,789 INFO     pid:9731 __main__:125:logMetrics E7 trn_pos  class 2 1.6982 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:06,792 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E7 Validation  ----/287, starting\n",
      "2024-02-01 13:57:06,860 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Validation    16/287, done at 2024-02-01 13:57:07, 0:00:00\n",
      "2024-02-01 13:57:06,917 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Validation    64/287, done at 2024-02-01 13:57:07, 0:00:00\n",
      "2024-02-01 13:57:07,091 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E7 Validation   256/287, done at 2024-02-01 13:57:07, 0:00:00\n",
      "2024-02-01 13:57:07,129 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E7 Validation  287/287, done at 2024-02-01 13:57:07\n",
      "2024-02-01 13:57:07,130 INFO     pid:9731 __main__:011:logMetrics E7 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:07,131 INFO     pid:9731 __main__:068:logMetrics E7 val      0.5188 loss\n",
      "2024-02-01 13:57:07,131 INFO     pid:9731 __main__:092:logMetrics E7 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:07,132 INFO     pid:9731 __main__:108:logMetrics E7 val_neg  class 0 0.4791 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:07,132 INFO     pid:9731 __main__:125:logMetrics E7 val_pos  class 0 4.6144 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:07,132 INFO     pid:9731 __main__:092:logMetrics E7 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:07,132 INFO     pid:9731 __main__:108:logMetrics E7 val_neg  class 1 1.8410 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:07,132 INFO     pid:9731 __main__:125:logMetrics E7 val_pos  class 1 0.2088 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:07,133 INFO     pid:9731 __main__:092:logMetrics E7 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:07,133 INFO     pid:9731 __main__:108:logMetrics E7 val_neg  class 2 0.2604 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:07,133 INFO     pid:9731 __main__:125:logMetrics E7 val_pos  class 2 1.6933 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:07,135 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E8 Training ----/1155, starting\n",
      "2024-02-01 13:57:07,192 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Training   16/1155, done at 2024-02-01 13:57:08, 0:00:01\n",
      "2024-02-01 13:57:07,242 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Training   64/1155, done at 2024-02-01 13:57:08, 0:00:01\n",
      "2024-02-01 13:57:07,451 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Training  256/1155, done at 2024-02-01 13:57:08, 0:00:01\n",
      "2024-02-01 13:57:08,483 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Training 1024/1155, done at 2024-02-01 13:57:08, 0:00:01\n",
      "2024-02-01 13:57:08,632 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E8 Training 1155/1155, done at 2024-02-01 13:57:08\n",
      "2024-02-01 13:57:08,642 INFO     pid:9731 __main__:011:logMetrics E8 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:08,645 INFO     pid:9731 __main__:068:logMetrics E8 trn      0.4992 loss\n",
      "2024-02-01 13:57:08,645 INFO     pid:9731 __main__:092:logMetrics E8 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:08,645 INFO     pid:9731 __main__:108:logMetrics E8 trn_neg  class 0 0.4642 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:08,646 INFO     pid:9731 __main__:125:logMetrics E8 trn_pos  class 0 4.7447 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:08,646 INFO     pid:9731 __main__:092:logMetrics E8 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:08,646 INFO     pid:9731 __main__:108:logMetrics E8 trn_neg  class 1 1.8353 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:08,646 INFO     pid:9731 __main__:125:logMetrics E8 trn_pos  class 1 0.1996 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:08,646 INFO     pid:9731 __main__:092:logMetrics E8 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:08,647 INFO     pid:9731 __main__:108:logMetrics E8 trn_neg  class 2 0.2447 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:08,647 INFO     pid:9731 __main__:125:logMetrics E8 trn_pos  class 2 1.6994 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:08,650 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E8 Validation  ----/287, starting\n",
      "2024-02-01 13:57:08,699 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Validation    16/287, done at 2024-02-01 13:57:08, 0:00:00\n",
      "2024-02-01 13:57:08,784 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Validation    64/287, done at 2024-02-01 13:57:09, 0:00:00\n",
      "2024-02-01 13:57:08,971 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E8 Validation   256/287, done at 2024-02-01 13:57:09, 0:00:00\n",
      "2024-02-01 13:57:09,012 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E8 Validation  287/287, done at 2024-02-01 13:57:09\n",
      "2024-02-01 13:57:09,012 INFO     pid:9731 __main__:011:logMetrics E8 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:09,013 INFO     pid:9731 __main__:068:logMetrics E8 val      0.5080 loss\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:092:logMetrics E8 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:108:logMetrics E8 val_neg  class 0 0.4676 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:125:logMetrics E8 val_pos  class 0 4.6804 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:092:logMetrics E8 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:108:logMetrics E8 val_neg  class 1 1.8053 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:09,014 INFO     pid:9731 __main__:125:logMetrics E8 val_pos  class 1 0.2038 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:09,015 INFO     pid:9731 __main__:092:logMetrics E8 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:09,015 INFO     pid:9731 __main__:108:logMetrics E8 val_neg  class 2 0.2563 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:09,015 INFO     pid:9731 __main__:125:logMetrics E8 val_pos  class 2 1.6521 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:09,017 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E9 Training ----/1155, starting\n",
      "2024-02-01 13:57:09,079 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Training   16/1155, done at 2024-02-01 13:57:10, 0:00:01\n",
      "2024-02-01 13:57:09,164 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Training   64/1155, done at 2024-02-01 13:57:10, 0:00:01\n",
      "2024-02-01 13:57:09,375 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Training  256/1155, done at 2024-02-01 13:57:10, 0:00:01\n",
      "2024-02-01 13:57:10,371 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Training 1024/1155, done at 2024-02-01 13:57:10, 0:00:01\n",
      "2024-02-01 13:57:10,533 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E9 Training 1155/1155, done at 2024-02-01 13:57:10\n",
      "2024-02-01 13:57:10,546 INFO     pid:9731 __main__:011:logMetrics E9 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:10,548 INFO     pid:9731 __main__:068:logMetrics E9 trn      0.4948 loss\n",
      "2024-02-01 13:57:10,548 INFO     pid:9731 __main__:092:logMetrics E9 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:10,548 INFO     pid:9731 __main__:108:logMetrics E9 trn_neg  class 0 0.4599 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:10,549 INFO     pid:9731 __main__:125:logMetrics E9 trn_pos  class 0 4.7230 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:10,549 INFO     pid:9731 __main__:092:logMetrics E9 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:10,549 INFO     pid:9731 __main__:108:logMetrics E9 trn_neg  class 1 1.8162 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:10,549 INFO     pid:9731 __main__:125:logMetrics E9 trn_pos  class 1 0.1985 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:10,549 INFO     pid:9731 __main__:092:logMetrics E9 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:10,550 INFO     pid:9731 __main__:108:logMetrics E9 trn_neg  class 2 0.2434 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:10,550 INFO     pid:9731 __main__:125:logMetrics E9 trn_pos  class 2 1.6804 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:10,552 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E9 Validation  ----/287, starting\n",
      "2024-02-01 13:57:10,654 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Validation    16/287, done at 2024-02-01 13:57:11, 0:00:01\n",
      "2024-02-01 13:57:10,707 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Validation    64/287, done at 2024-02-01 13:57:11, 0:00:00\n",
      "2024-02-01 13:57:10,890 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E9 Validation   256/287, done at 2024-02-01 13:57:10, 0:00:00\n",
      "2024-02-01 13:57:10,926 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E9 Validation  287/287, done at 2024-02-01 13:57:10\n",
      "2024-02-01 13:57:10,927 INFO     pid:9731 __main__:011:logMetrics E9 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:10,928 INFO     pid:9731 __main__:068:logMetrics E9 val      0.5106 loss\n",
      "2024-02-01 13:57:10,928 INFO     pid:9731 __main__:092:logMetrics E9 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:108:logMetrics E9 val_neg  class 0 0.4708 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:125:logMetrics E9 val_pos  class 0 4.6196 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:092:logMetrics E9 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:108:logMetrics E9 val_neg  class 1 1.7215 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:125:logMetrics E9 val_pos  class 1 0.2267 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:10,929 INFO     pid:9731 __main__:092:logMetrics E9 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:10,930 INFO     pid:9731 __main__:108:logMetrics E9 val_neg  class 2 0.2782 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:10,930 INFO     pid:9731 __main__:125:logMetrics E9 val_pos  class 2 1.5672 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:10,932 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E10 Training ----/1155, starting\n",
      "2024-02-01 13:57:10,987 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Training   16/1155, done at 2024-02-01 13:57:12, 0:00:01\n",
      "2024-02-01 13:57:11,039 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Training   64/1155, done at 2024-02-01 13:57:12, 0:00:01\n",
      "2024-02-01 13:57:11,230 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Training  256/1155, done at 2024-02-01 13:57:12, 0:00:01\n",
      "2024-02-01 13:57:12,266 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Training 1024/1155, done at 2024-02-01 13:57:12, 0:00:01\n",
      "2024-02-01 13:57:12,422 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E10 Training 1155/1155, done at 2024-02-01 13:57:12\n",
      "2024-02-01 13:57:12,433 INFO     pid:9731 __main__:011:logMetrics E10 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:12,436 INFO     pid:9731 __main__:068:logMetrics E10 trn      0.4939 loss\n",
      "2024-02-01 13:57:12,436 INFO     pid:9731 __main__:092:logMetrics E10 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:12,436 INFO     pid:9731 __main__:108:logMetrics E10 trn_neg  class 0 0.4591 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:12,436 INFO     pid:9731 __main__:125:logMetrics E10 trn_pos  class 0 4.7209 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:12,436 INFO     pid:9731 __main__:092:logMetrics E10 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:12,437 INFO     pid:9731 __main__:108:logMetrics E10 trn_neg  class 1 1.8091 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:12,437 INFO     pid:9731 __main__:125:logMetrics E10 trn_pos  class 1 0.1991 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:12,437 INFO     pid:9731 __main__:092:logMetrics E10 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:12,437 INFO     pid:9731 __main__:108:logMetrics E10 trn_neg  class 2 0.2439 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:12,437 INFO     pid:9731 __main__:125:logMetrics E10 trn_pos  class 2 1.6730 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:12,440 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E10 Validation  ----/287, starting\n",
      "2024-02-01 13:57:12,527 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Validation    16/287, done at 2024-02-01 13:57:13, 0:00:00\n",
      "2024-02-01 13:57:12,574 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Validation    64/287, done at 2024-02-01 13:57:12, 0:00:00\n",
      "2024-02-01 13:57:12,750 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E10 Validation   256/287, done at 2024-02-01 13:57:12, 0:00:00\n",
      "2024-02-01 13:57:12,793 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E10 Validation  287/287, done at 2024-02-01 13:57:12\n",
      "2024-02-01 13:57:12,793 INFO     pid:9731 __main__:011:logMetrics E10 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:12,794 INFO     pid:9731 __main__:068:logMetrics E10 val      0.5101 loss\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:092:logMetrics E10 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:108:logMetrics E10 val_neg  class 0 0.4728 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:125:logMetrics E10 val_pos  class 0 4.3610 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:092:logMetrics E10 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:108:logMetrics E10 val_neg  class 1 1.8781 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:12,795 INFO     pid:9731 __main__:125:logMetrics E10 val_pos  class 1 0.1893 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:12,796 INFO     pid:9731 __main__:092:logMetrics E10 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:12,796 INFO     pid:9731 __main__:108:logMetrics E10 val_neg  class 2 0.2382 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:12,796 INFO     pid:9731 __main__:125:logMetrics E10 val_pos  class 2 1.7458 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:12,798 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E11 Training ----/1155, starting\n",
      "2024-02-01 13:57:12,856 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Training   16/1155, done at 2024-02-01 13:57:14, 0:00:01\n",
      "2024-02-01 13:57:12,909 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Training   64/1155, done at 2024-02-01 13:57:14, 0:00:01\n",
      "2024-02-01 13:57:13,137 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Training  256/1155, done at 2024-02-01 13:57:14, 0:00:01\n",
      "2024-02-01 13:57:14,109 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Training 1024/1155, done at 2024-02-01 13:57:14, 0:00:01\n",
      "2024-02-01 13:57:14,260 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E11 Training 1155/1155, done at 2024-02-01 13:57:14\n",
      "2024-02-01 13:57:14,271 INFO     pid:9731 __main__:011:logMetrics E11 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:14,274 INFO     pid:9731 __main__:068:logMetrics E11 trn      0.4939 loss\n",
      "2024-02-01 13:57:14,274 INFO     pid:9731 __main__:092:logMetrics E11 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:14,274 INFO     pid:9731 __main__:108:logMetrics E11 trn_neg  class 0 0.4594 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:14,275 INFO     pid:9731 __main__:125:logMetrics E11 trn_pos  class 0 4.6894 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:14,275 INFO     pid:9731 __main__:092:logMetrics E11 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:14,275 INFO     pid:9731 __main__:108:logMetrics E11 trn_neg  class 1 1.8118 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:14,275 INFO     pid:9731 __main__:125:logMetrics E11 trn_pos  class 1 0.1985 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:14,276 INFO     pid:9731 __main__:092:logMetrics E11 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:14,276 INFO     pid:9731 __main__:108:logMetrics E11 trn_neg  class 2 0.2430 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:14,276 INFO     pid:9731 __main__:125:logMetrics E11 trn_pos  class 2 1.6773 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:14,280 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E11 Validation  ----/287, starting\n",
      "2024-02-01 13:57:14,327 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Validation    16/287, done at 2024-02-01 13:57:14, 0:00:00\n",
      "2024-02-01 13:57:14,422 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Validation    64/287, done at 2024-02-01 13:57:14, 0:00:00\n",
      "2024-02-01 13:57:14,601 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E11 Validation   256/287, done at 2024-02-01 13:57:14, 0:00:00\n",
      "2024-02-01 13:57:14,637 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E11 Validation  287/287, done at 2024-02-01 13:57:14\n",
      "2024-02-01 13:57:14,637 INFO     pid:9731 __main__:011:logMetrics E11 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:14,639 INFO     pid:9731 __main__:068:logMetrics E11 val      0.5120 loss\n",
      "2024-02-01 13:57:14,639 INFO     pid:9731 __main__:092:logMetrics E11 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:14,639 INFO     pid:9731 __main__:108:logMetrics E11 val_neg  class 0 0.4747 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:14,639 INFO     pid:9731 __main__:125:logMetrics E11 val_pos  class 0 4.3509 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:14,640 INFO     pid:9731 __main__:092:logMetrics E11 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:14,640 INFO     pid:9731 __main__:108:logMetrics E11 val_neg  class 1 1.6736 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:14,640 INFO     pid:9731 __main__:125:logMetrics E11 val_pos  class 1 0.2396 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:14,641 INFO     pid:9731 __main__:092:logMetrics E11 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:14,641 INFO     pid:9731 __main__:108:logMetrics E11 val_neg  class 2 0.2878 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:14,641 INFO     pid:9731 __main__:125:logMetrics E11 val_pos  class 2 1.5310 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:14,643 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E12 Training ----/1155, starting\n",
      "2024-02-01 13:57:14,698 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Training   16/1155, done at 2024-02-01 13:57:15, 0:00:01\n",
      "2024-02-01 13:57:14,799 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Training   64/1155, done at 2024-02-01 13:57:16, 0:00:02\n",
      "2024-02-01 13:57:15,005 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Training  256/1155, done at 2024-02-01 13:57:16, 0:00:01\n",
      "2024-02-01 13:57:15,994 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Training 1024/1155, done at 2024-02-01 13:57:16, 0:00:01\n",
      "2024-02-01 13:57:16,150 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E12 Training 1155/1155, done at 2024-02-01 13:57:16\n",
      "2024-02-01 13:57:16,161 INFO     pid:9731 __main__:011:logMetrics E12 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:16,163 INFO     pid:9731 __main__:068:logMetrics E12 trn      0.4932 loss\n",
      "2024-02-01 13:57:16,163 INFO     pid:9731 __main__:092:logMetrics E12 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:108:logMetrics E12 trn_neg  class 0 0.4584 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:125:logMetrics E12 trn_pos  class 0 4.7143 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:092:logMetrics E12 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:108:logMetrics E12 trn_neg  class 1 1.8073 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:125:logMetrics E12 trn_pos  class 1 0.1986 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:16,164 INFO     pid:9731 __main__:092:logMetrics E12 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:16,165 INFO     pid:9731 __main__:108:logMetrics E12 trn_neg  class 2 0.2433 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:16,165 INFO     pid:9731 __main__:125:logMetrics E12 trn_pos  class 2 1.6715 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:16,168 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E12 Validation  ----/287, starting\n",
      "2024-02-01 13:57:16,262 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Validation    16/287, done at 2024-02-01 13:57:16, 0:00:00\n",
      "2024-02-01 13:57:16,307 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Validation    64/287, done at 2024-02-01 13:57:16, 0:00:00\n",
      "2024-02-01 13:57:16,497 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E12 Validation   256/287, done at 2024-02-01 13:57:16, 0:00:00\n",
      "2024-02-01 13:57:16,535 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E12 Validation  287/287, done at 2024-02-01 13:57:16\n",
      "2024-02-01 13:57:16,536 INFO     pid:9731 __main__:011:logMetrics E12 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:16,537 INFO     pid:9731 __main__:068:logMetrics E12 val      0.5103 loss\n",
      "2024-02-01 13:57:16,538 INFO     pid:9731 __main__:092:logMetrics E12 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:16,538 INFO     pid:9731 __main__:108:logMetrics E12 val_neg  class 0 0.4682 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:16,538 INFO     pid:9731 __main__:125:logMetrics E12 val_pos  class 0 4.8495 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:16,538 INFO     pid:9731 __main__:092:logMetrics E12 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:16,539 INFO     pid:9731 __main__:108:logMetrics E12 val_neg  class 1 1.8384 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:16,539 INFO     pid:9731 __main__:125:logMetrics E12 val_pos  class 1 0.1989 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:16,539 INFO     pid:9731 __main__:092:logMetrics E12 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:16,539 INFO     pid:9731 __main__:108:logMetrics E12 val_neg  class 2 0.2534 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:16,539 INFO     pid:9731 __main__:125:logMetrics E12 val_pos  class 2 1.6780 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:16,541 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E13 Training ----/1155, starting\n",
      "2024-02-01 13:57:16,598 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Training   16/1155, done at 2024-02-01 13:57:17, 0:00:01\n",
      "2024-02-01 13:57:16,673 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Training   64/1155, done at 2024-02-01 13:57:18, 0:00:01\n",
      "2024-02-01 13:57:16,876 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Training  256/1155, done at 2024-02-01 13:57:17, 0:00:01\n",
      "2024-02-01 13:57:17,842 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Training 1024/1155, done at 2024-02-01 13:57:18, 0:00:01\n",
      "2024-02-01 13:57:18,002 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E13 Training 1155/1155, done at 2024-02-01 13:57:18\n",
      "2024-02-01 13:57:18,012 INFO     pid:9731 __main__:011:logMetrics E13 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:18,015 INFO     pid:9731 __main__:068:logMetrics E13 trn      0.4936 loss\n",
      "2024-02-01 13:57:18,015 INFO     pid:9731 __main__:092:logMetrics E13 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:18,015 INFO     pid:9731 __main__:108:logMetrics E13 trn_neg  class 0 0.4589 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:18,015 INFO     pid:9731 __main__:125:logMetrics E13 trn_pos  class 0 4.7093 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:092:logMetrics E13 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:108:logMetrics E13 trn_neg  class 1 1.8114 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:125:logMetrics E13 trn_pos  class 1 0.1982 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:092:logMetrics E13 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:108:logMetrics E13 trn_neg  class 2 0.2429 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:18,016 INFO     pid:9731 __main__:125:logMetrics E13 trn_pos  class 2 1.6759 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:18,019 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E13 Validation  ----/287, starting\n",
      "2024-02-01 13:57:18,070 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Validation    16/287, done at 2024-02-01 13:57:18, 0:00:00\n",
      "2024-02-01 13:57:18,158 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Validation    64/287, done at 2024-02-01 13:57:18, 0:00:00\n",
      "2024-02-01 13:57:18,343 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E13 Validation   256/287, done at 2024-02-01 13:57:18, 0:00:00\n",
      "2024-02-01 13:57:18,379 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E13 Validation  287/287, done at 2024-02-01 13:57:18\n",
      "2024-02-01 13:57:18,380 INFO     pid:9731 __main__:011:logMetrics E13 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:18,381 INFO     pid:9731 __main__:068:logMetrics E13 val      0.5093 loss\n",
      "2024-02-01 13:57:18,381 INFO     pid:9731 __main__:092:logMetrics E13 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:18,382 INFO     pid:9731 __main__:108:logMetrics E13 val_neg  class 0 0.4693 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:18,382 INFO     pid:9731 __main__:125:logMetrics E13 val_pos  class 0 4.6383 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:18,382 INFO     pid:9731 __main__:092:logMetrics E13 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:18,382 INFO     pid:9731 __main__:108:logMetrics E13 val_neg  class 1 1.8636 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:18,383 INFO     pid:9731 __main__:125:logMetrics E13 val_pos  class 1 0.1917 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:18,383 INFO     pid:9731 __main__:092:logMetrics E13 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:18,383 INFO     pid:9731 __main__:108:logMetrics E13 val_neg  class 2 0.2439 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:18,383 INFO     pid:9731 __main__:125:logMetrics E13 val_pos  class 2 1.7158 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:18,385 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E14 Training ----/1155, starting\n",
      "2024-02-01 13:57:18,440 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Training   16/1155, done at 2024-02-01 13:57:19, 0:00:01\n",
      "2024-02-01 13:57:18,492 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Training   64/1155, done at 2024-02-01 13:57:19, 0:00:01\n",
      "2024-02-01 13:57:18,733 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Training  256/1155, done at 2024-02-01 13:57:19, 0:00:01\n",
      "2024-02-01 13:57:19,715 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Training 1024/1155, done at 2024-02-01 13:57:19, 0:00:01\n",
      "2024-02-01 13:57:19,863 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E14 Training 1155/1155, done at 2024-02-01 13:57:19\n",
      "2024-02-01 13:57:19,875 INFO     pid:9731 __main__:011:logMetrics E14 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:19,877 INFO     pid:9731 __main__:068:logMetrics E14 trn      0.4928 loss\n",
      "2024-02-01 13:57:19,877 INFO     pid:9731 __main__:092:logMetrics E14 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:19,878 INFO     pid:9731 __main__:108:logMetrics E14 trn_neg  class 0 0.4580 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:19,878 INFO     pid:9731 __main__:125:logMetrics E14 trn_pos  class 0 4.7082 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:19,878 INFO     pid:9731 __main__:092:logMetrics E14 trn      class 1  81.7% correct, 0.8169 precision, 1.0000 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:19,878 INFO     pid:9731 __main__:108:logMetrics E14 trn_neg  class 1 1.8083 loss,   0.0% correct (0 of 6765)\n",
      "2024-02-01 13:57:19,878 INFO     pid:9731 __main__:125:logMetrics E14 trn_pos  class 1 0.1978 loss, 100.0% correct (30175 of 30175)\n",
      "2024-02-01 13:57:19,879 INFO     pid:9731 __main__:092:logMetrics E14 trn      class 2  82.5% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:19,879 INFO     pid:9731 __main__:108:logMetrics E14 trn_neg  class 2 0.2425 loss, 100.0% correct (30477 of 30477)\n",
      "2024-02-01 13:57:19,879 INFO     pid:9731 __main__:125:logMetrics E14 trn_pos  class 2 1.6728 loss,   0.0% correct (0 of 6463)\n",
      "2024-02-01 13:57:19,882 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E14 Validation  ----/287, starting\n",
      "2024-02-01 13:57:19,956 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Validation    16/287, done at 2024-02-01 13:57:20, 0:00:00\n",
      "2024-02-01 13:57:20,004 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Validation    64/287, done at 2024-02-01 13:57:20, 0:00:00\n",
      "2024-02-01 13:57:20,180 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E14 Validation   256/287, done at 2024-02-01 13:57:20, 0:00:00\n",
      "2024-02-01 13:57:20,215 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E14 Validation  287/287, done at 2024-02-01 13:57:20\n",
      "2024-02-01 13:57:20,215 INFO     pid:9731 __main__:011:logMetrics E14 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:20,217 INFO     pid:9731 __main__:068:logMetrics E14 val      0.5127 loss\n",
      "2024-02-01 13:57:20,217 INFO     pid:9731 __main__:092:logMetrics E14 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:20,217 INFO     pid:9731 __main__:108:logMetrics E14 val_neg  class 0 0.4735 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:20,217 INFO     pid:9731 __main__:125:logMetrics E14 val_pos  class 0 4.5556 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:20,217 INFO     pid:9731 __main__:092:logMetrics E14 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:20,218 INFO     pid:9731 __main__:108:logMetrics E14 val_neg  class 1 1.6332 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:20,218 INFO     pid:9731 __main__:125:logMetrics E14 val_pos  class 1 0.2499 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:20,218 INFO     pid:9731 __main__:092:logMetrics E14 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:20,218 INFO     pid:9731 __main__:108:logMetrics E14 val_neg  class 2 0.3004 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:20,218 INFO     pid:9731 __main__:125:logMetrics E14 val_pos  class 2 1.4776 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:20,221 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E15 Training ----/1155, starting\n",
      "2024-02-01 13:57:20,285 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Training   16/1155, done at 2024-02-01 13:57:21, 0:00:01\n",
      "2024-02-01 13:57:20,336 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Training   64/1155, done at 2024-02-01 13:57:21, 0:00:01\n",
      "2024-02-01 13:57:20,546 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Training  256/1155, done at 2024-02-01 13:57:21, 0:00:01\n",
      "2024-02-01 13:57:21,503 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Training 1024/1155, done at 2024-02-01 13:57:21, 0:00:01\n",
      "2024-02-01 13:57:21,654 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E15 Training 1155/1155, done at 2024-02-01 13:57:21\n",
      "2024-02-01 13:57:21,666 INFO     pid:9731 __main__:011:logMetrics E15 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:21,668 INFO     pid:9731 __main__:068:logMetrics E15 trn      0.4919 loss\n",
      "2024-02-01 13:57:21,669 INFO     pid:9731 __main__:092:logMetrics E15 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:21,669 INFO     pid:9731 __main__:108:logMetrics E15 trn_neg  class 0 0.4573 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:21,669 INFO     pid:9731 __main__:125:logMetrics E15 trn_pos  class 0 4.6829 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:21,669 INFO     pid:9731 __main__:092:logMetrics E15 trn      class 1  81.7% correct, 0.8170 precision, 0.9994 recall, 0.8990 f1 score\n",
      "2024-02-01 13:57:21,670 INFO     pid:9731 __main__:108:logMetrics E15 trn_neg  class 1 1.7997 loss,   0.1% correct (9 of 6765)\n",
      "2024-02-01 13:57:21,670 INFO     pid:9731 __main__:125:logMetrics E15 trn_pos  class 1 0.1987 loss,  99.9% correct (30156 of 30175)\n",
      "2024-02-01 13:57:21,670 INFO     pid:9731 __main__:092:logMetrics E15 trn      class 2  82.5% correct, 0.3158 precision, 0.0009 recall, 0.0019 f1 score\n",
      "2024-02-01 13:57:21,671 INFO     pid:9731 __main__:108:logMetrics E15 trn_neg  class 2 0.2431 loss, 100.0% correct (30464 of 30477)\n",
      "2024-02-01 13:57:21,671 INFO     pid:9731 __main__:125:logMetrics E15 trn_pos  class 2 1.6649 loss,   0.1% correct (6 of 6463)\n",
      "2024-02-01 13:57:21,683 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E15 Validation  ----/287, starting\n",
      "2024-02-01 13:57:21,769 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Validation    16/287, done at 2024-02-01 13:57:22, 0:00:01\n",
      "2024-02-01 13:57:21,817 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Validation    64/287, done at 2024-02-01 13:57:22, 0:00:00\n",
      "2024-02-01 13:57:21,996 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E15 Validation   256/287, done at 2024-02-01 13:57:22, 0:00:00\n",
      "2024-02-01 13:57:22,032 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E15 Validation  287/287, done at 2024-02-01 13:57:22\n",
      "2024-02-01 13:57:22,032 INFO     pid:9731 __main__:011:logMetrics E15 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:22,034 INFO     pid:9731 __main__:068:logMetrics E15 val      0.5116 loss\n",
      "2024-02-01 13:57:22,034 INFO     pid:9731 __main__:092:logMetrics E15 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:22,034 INFO     pid:9731 __main__:108:logMetrics E15 val_neg  class 0 0.4752 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:22,035 INFO     pid:9731 __main__:125:logMetrics E15 val_pos  class 0 4.2659 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:22,035 INFO     pid:9731 __main__:092:logMetrics E15 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:22,035 INFO     pid:9731 __main__:108:logMetrics E15 val_neg  class 1 1.6788 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:22,035 INFO     pid:9731 __main__:125:logMetrics E15 val_pos  class 1 0.2379 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:22,036 INFO     pid:9731 __main__:092:logMetrics E15 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:22,036 INFO     pid:9731 __main__:108:logMetrics E15 val_neg  class 2 0.2851 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:22,036 INFO     pid:9731 __main__:125:logMetrics E15 val_pos  class 2 1.5410 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:22,038 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E16 Training ----/1155, starting\n",
      "2024-02-01 13:57:22,121 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Training   16/1155, done at 2024-02-01 13:57:23, 0:00:01\n",
      "2024-02-01 13:57:22,177 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Training   64/1155, done at 2024-02-01 13:57:23, 0:00:01\n",
      "2024-02-01 13:57:22,389 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Training  256/1155, done at 2024-02-01 13:57:23, 0:00:01\n",
      "2024-02-01 13:57:23,384 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Training 1024/1155, done at 2024-02-01 13:57:23, 0:00:01\n",
      "2024-02-01 13:57:23,545 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E16 Training 1155/1155, done at 2024-02-01 13:57:23\n",
      "2024-02-01 13:57:23,556 INFO     pid:9731 __main__:011:logMetrics E16 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:23,558 INFO     pid:9731 __main__:068:logMetrics E16 trn      0.4909 loss\n",
      "2024-02-01 13:57:23,559 INFO     pid:9731 __main__:092:logMetrics E16 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:23,559 INFO     pid:9731 __main__:108:logMetrics E16 trn_neg  class 0 0.4559 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:23,559 INFO     pid:9731 __main__:125:logMetrics E16 trn_pos  class 0 4.7340 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:23,559 INFO     pid:9731 __main__:092:logMetrics E16 trn      class 1  81.7% correct, 0.8169 precision, 0.9997 recall, 0.8991 f1 score\n",
      "2024-02-01 13:57:23,560 INFO     pid:9731 __main__:108:logMetrics E16 trn_neg  class 1 1.7998 loss,   0.1% correct (4 of 6765)\n",
      "2024-02-01 13:57:23,560 INFO     pid:9731 __main__:125:logMetrics E16 trn_pos  class 1 0.1975 loss, 100.0% correct (30166 of 30175)\n",
      "2024-02-01 13:57:23,560 INFO     pid:9731 __main__:092:logMetrics E16 trn      class 2  82.5% correct, 0.2000 precision, 0.0003 recall, 0.0006 f1 score\n",
      "2024-02-01 13:57:23,560 INFO     pid:9731 __main__:108:logMetrics E16 trn_neg  class 2 0.2424 loss, 100.0% correct (30469 of 30477)\n",
      "2024-02-01 13:57:23,561 INFO     pid:9731 __main__:125:logMetrics E16 trn_pos  class 2 1.6627 loss,   0.0% correct (2 of 6463)\n",
      "2024-02-01 13:57:23,564 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E16 Validation  ----/287, starting\n",
      "2024-02-01 13:57:23,615 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Validation    16/287, done at 2024-02-01 13:57:23, 0:00:00\n",
      "2024-02-01 13:57:23,710 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Validation    64/287, done at 2024-02-01 13:57:24, 0:00:00\n",
      "2024-02-01 13:57:23,892 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E16 Validation   256/287, done at 2024-02-01 13:57:23, 0:00:00\n",
      "2024-02-01 13:57:23,930 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E16 Validation  287/287, done at 2024-02-01 13:57:23\n",
      "2024-02-01 13:57:23,931 INFO     pid:9731 __main__:011:logMetrics E16 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:23,932 INFO     pid:9731 __main__:068:logMetrics E16 val      0.5091 loss\n",
      "2024-02-01 13:57:23,932 INFO     pid:9731 __main__:092:logMetrics E16 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:23,932 INFO     pid:9731 __main__:108:logMetrics E16 val_neg  class 0 0.4698 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:23,933 INFO     pid:9731 __main__:125:logMetrics E16 val_pos  class 0 4.5555 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:23,933 INFO     pid:9731 __main__:092:logMetrics E16 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:23,933 INFO     pid:9731 __main__:108:logMetrics E16 val_neg  class 1 1.7745 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:23,934 INFO     pid:9731 __main__:125:logMetrics E16 val_pos  class 1 0.2123 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:23,934 INFO     pid:9731 __main__:092:logMetrics E16 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:23,934 INFO     pid:9731 __main__:108:logMetrics E16 val_neg  class 2 0.2632 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:23,934 INFO     pid:9731 __main__:125:logMetrics E16 val_pos  class 2 1.6263 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:23,936 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E17 Training ----/1155, starting\n",
      "2024-02-01 13:57:24,031 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Training   16/1155, done at 2024-02-01 13:57:25, 0:00:01\n",
      "2024-02-01 13:57:24,098 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Training   64/1155, done at 2024-02-01 13:57:25, 0:00:01\n",
      "2024-02-01 13:57:24,305 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Training  256/1155, done at 2024-02-01 13:57:25, 0:00:01\n",
      "2024-02-01 13:57:25,446 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Training 1024/1155, done at 2024-02-01 13:57:25, 0:00:01\n",
      "2024-02-01 13:57:25,599 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E17 Training 1155/1155, done at 2024-02-01 13:57:25\n",
      "2024-02-01 13:57:25,610 INFO     pid:9731 __main__:011:logMetrics E17 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:25,612 INFO     pid:9731 __main__:068:logMetrics E17 trn      0.4910 loss\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:092:logMetrics E17 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:108:logMetrics E17 trn_neg  class 0 0.4564 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:125:logMetrics E17 trn_pos  class 0 4.6906 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:092:logMetrics E17 trn      class 1  81.7% correct, 0.8173 precision, 0.9994 recall, 0.8992 f1 score\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:108:logMetrics E17 trn_neg  class 1 1.7989 loss,   0.4% correct (25 of 6765)\n",
      "2024-02-01 13:57:25,613 INFO     pid:9731 __main__:125:logMetrics E17 trn_pos  class 1 0.1978 loss,  99.9% correct (30156 of 30175)\n",
      "2024-02-01 13:57:25,614 INFO     pid:9731 __main__:092:logMetrics E17 trn      class 2  82.5% correct, 0.5385 precision, 0.0032 recall, 0.0065 f1 score\n",
      "2024-02-01 13:57:25,614 INFO     pid:9731 __main__:108:logMetrics E17 trn_neg  class 2 0.2423 loss,  99.9% correct (30459 of 30477)\n",
      "2024-02-01 13:57:25,614 INFO     pid:9731 __main__:125:logMetrics E17 trn_pos  class 2 1.6638 loss,   0.3% correct (21 of 6463)\n",
      "2024-02-01 13:57:25,617 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E17 Validation  ----/287, starting\n",
      "2024-02-01 13:57:25,663 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Validation    16/287, done at 2024-02-01 13:57:25, 0:00:00\n",
      "2024-02-01 13:57:25,745 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Validation    64/287, done at 2024-02-01 13:57:26, 0:00:00\n",
      "2024-02-01 13:57:25,926 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E17 Validation   256/287, done at 2024-02-01 13:57:25, 0:00:00\n",
      "2024-02-01 13:57:25,967 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E17 Validation  287/287, done at 2024-02-01 13:57:25\n",
      "2024-02-01 13:57:25,967 INFO     pid:9731 __main__:011:logMetrics E17 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:25,968 INFO     pid:9731 __main__:068:logMetrics E17 val      0.5095 loss\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:092:logMetrics E17 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:108:logMetrics E17 val_neg  class 0 0.4685 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:125:logMetrics E17 val_pos  class 0 4.7357 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:092:logMetrics E17 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:108:logMetrics E17 val_neg  class 1 1.8750 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:125:logMetrics E17 val_pos  class 1 0.1893 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:25,969 INFO     pid:9731 __main__:092:logMetrics E17 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:25,970 INFO     pid:9731 __main__:108:logMetrics E17 val_neg  class 2 0.2426 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:25,970 INFO     pid:9731 __main__:125:logMetrics E17 val_pos  class 2 1.7226 loss,   0.0% correct (0 of 1652)\n",
      "2024-02-01 13:57:25,972 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E18 Training ----/1155, starting\n",
      "2024-02-01 13:57:26,026 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Training   16/1155, done at 2024-02-01 13:57:27, 0:00:01\n",
      "2024-02-01 13:57:26,087 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Training   64/1155, done at 2024-02-01 13:57:27, 0:00:01\n",
      "2024-02-01 13:57:26,329 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Training  256/1155, done at 2024-02-01 13:57:27, 0:00:01\n",
      "2024-02-01 13:57:27,309 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Training 1024/1155, done at 2024-02-01 13:57:27, 0:00:01\n",
      "2024-02-01 13:57:27,458 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E18 Training 1155/1155, done at 2024-02-01 13:57:27\n",
      "2024-02-01 13:57:27,469 INFO     pid:9731 __main__:011:logMetrics E18 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:27,471 INFO     pid:9731 __main__:068:logMetrics E18 trn      0.4911 loss\n",
      "2024-02-01 13:57:27,471 INFO     pid:9731 __main__:092:logMetrics E18 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:27,472 INFO     pid:9731 __main__:108:logMetrics E18 trn_neg  class 0 0.4563 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:27,472 INFO     pid:9731 __main__:125:logMetrics E18 trn_pos  class 0 4.7094 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:27,472 INFO     pid:9731 __main__:092:logMetrics E18 trn      class 1  81.7% correct, 0.8175 precision, 0.9989 recall, 0.8991 f1 score\n",
      "2024-02-01 13:57:27,472 INFO     pid:9731 __main__:108:logMetrics E18 trn_neg  class 1 1.8022 loss,   0.5% correct (35 of 6765)\n",
      "2024-02-01 13:57:27,472 INFO     pid:9731 __main__:125:logMetrics E18 trn_pos  class 1 0.1972 loss,  99.9% correct (30143 of 30175)\n",
      "2024-02-01 13:57:27,473 INFO     pid:9731 __main__:092:logMetrics E18 trn      class 2  82.5% correct, 0.5517 precision, 0.0050 recall, 0.0098 f1 score\n",
      "2024-02-01 13:57:27,473 INFO     pid:9731 __main__:108:logMetrics E18 trn_neg  class 2 0.2419 loss,  99.9% correct (30451 of 30477)\n",
      "2024-02-01 13:57:27,473 INFO     pid:9731 __main__:125:logMetrics E18 trn_pos  class 2 1.6663 loss,   0.5% correct (32 of 6463)\n",
      "2024-02-01 13:57:27,476 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E18 Validation  ----/287, starting\n",
      "2024-02-01 13:57:27,524 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Validation    16/287, done at 2024-02-01 13:57:27, 0:00:00\n",
      "2024-02-01 13:57:27,597 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Validation    64/287, done at 2024-02-01 13:57:27, 0:00:00\n",
      "2024-02-01 13:57:27,774 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E18 Validation   256/287, done at 2024-02-01 13:57:27, 0:00:00\n",
      "2024-02-01 13:57:27,814 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E18 Validation  287/287, done at 2024-02-01 13:57:27\n",
      "2024-02-01 13:57:27,814 INFO     pid:9731 __main__:011:logMetrics E18 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:27,816 INFO     pid:9731 __main__:068:logMetrics E18 val      0.5112 loss\n",
      "2024-02-01 13:57:27,816 INFO     pid:9731 __main__:092:logMetrics E18 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:27,816 INFO     pid:9731 __main__:108:logMetrics E18 val_neg  class 0 0.4725 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:27,816 INFO     pid:9731 __main__:125:logMetrics E18 val_pos  class 0 4.4915 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:27,816 INFO     pid:9731 __main__:092:logMetrics E18 val      class 1  81.0% correct, 0.8111 precision, 0.9985 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:27,817 INFO     pid:9731 __main__:108:logMetrics E18 val_neg  class 1 1.6871 loss,   0.8% correct (14 of 1740)\n",
      "2024-02-01 13:57:27,817 INFO     pid:9731 __main__:125:logMetrics E18 val_pos  class 1 0.2354 loss,  99.9% correct (7409 of 7420)\n",
      "2024-02-01 13:57:27,817 INFO     pid:9731 __main__:092:logMetrics E18 val      class 2  82.0% correct, 0.5652 precision, 0.0079 recall, 0.0155 f1 score\n",
      "2024-02-01 13:57:27,818 INFO     pid:9731 __main__:108:logMetrics E18 val_neg  class 2 0.2853 loss,  99.9% correct (7498 of 7508)\n",
      "2024-02-01 13:57:27,818 INFO     pid:9731 __main__:125:logMetrics E18 val_pos  class 2 1.5377 loss,   0.8% correct (13 of 1652)\n",
      "2024-02-01 13:57:27,820 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E19 Training ----/1155, starting\n",
      "2024-02-01 13:57:27,889 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Training   16/1155, done at 2024-02-01 13:57:29, 0:00:01\n",
      "2024-02-01 13:57:27,946 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Training   64/1155, done at 2024-02-01 13:57:29, 0:00:01\n",
      "2024-02-01 13:57:28,190 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Training  256/1155, done at 2024-02-01 13:57:29, 0:00:01\n",
      "2024-02-01 13:57:29,175 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Training 1024/1155, done at 2024-02-01 13:57:29, 0:00:01\n",
      "2024-02-01 13:57:29,335 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E19 Training 1155/1155, done at 2024-02-01 13:57:29\n",
      "2024-02-01 13:57:29,348 INFO     pid:9731 __main__:011:logMetrics E19 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:29,350 INFO     pid:9731 __main__:068:logMetrics E19 trn      0.4904 loss\n",
      "2024-02-01 13:57:29,351 INFO     pid:9731 __main__:092:logMetrics E19 trn      class 0  99.2% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:29,351 INFO     pid:9731 __main__:108:logMetrics E19 trn_neg  class 0 0.4560 loss, 100.0% correct (36638 of 36638)\n",
      "2024-02-01 13:57:29,351 INFO     pid:9731 __main__:125:logMetrics E19 trn_pos  class 0 4.6709 loss,   0.0% correct (0 of 302)\n",
      "2024-02-01 13:57:29,352 INFO     pid:9731 __main__:092:logMetrics E19 trn      class 1  81.7% correct, 0.8178 precision, 0.9979 recall, 0.8989 f1 score\n",
      "2024-02-01 13:57:29,352 INFO     pid:9731 __main__:108:logMetrics E19 trn_neg  class 1 1.7934 loss,   0.8% correct (55 of 6765)\n",
      "2024-02-01 13:57:29,352 INFO     pid:9731 __main__:125:logMetrics E19 trn_pos  class 1 0.1983 loss,  99.8% correct (30112 of 30175)\n",
      "2024-02-01 13:57:29,352 INFO     pid:9731 __main__:092:logMetrics E19 trn      class 2  82.5% correct, 0.4414 precision, 0.0076 recall, 0.0149 f1 score\n",
      "2024-02-01 13:57:29,353 INFO     pid:9731 __main__:108:logMetrics E19 trn_neg  class 2 0.2426 loss,  99.8% correct (30415 of 30477)\n",
      "2024-02-01 13:57:29,353 INFO     pid:9731 __main__:125:logMetrics E19 trn_pos  class 2 1.6589 loss,   0.8% correct (49 of 6463)\n",
      "2024-02-01 13:57:29,356 INFO     pid:9731 myutil.util:130:enumerateWithEstimate E19 Validation  ----/287, starting\n",
      "2024-02-01 13:57:29,407 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Validation    16/287, done at 2024-02-01 13:57:29, 0:00:00\n",
      "2024-02-01 13:57:29,487 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Validation    64/287, done at 2024-02-01 13:57:29, 0:00:00\n",
      "2024-02-01 13:57:29,665 INFO     pid:9731 myutil.util:150:enumerateWithEstimate E19 Validation   256/287, done at 2024-02-01 13:57:29, 0:00:00\n",
      "2024-02-01 13:57:29,704 INFO     pid:9731 myutil.util:165:enumerateWithEstimate E19 Validation  287/287, done at 2024-02-01 13:57:29\n",
      "2024-02-01 13:57:29,704 INFO     pid:9731 __main__:011:logMetrics E19 StockPCTLabelPredictLSTM\n",
      "2024-02-01 13:57:29,706 INFO     pid:9731 __main__:068:logMetrics E19 val      0.5120 loss\n",
      "2024-02-01 13:57:29,706 INFO     pid:9731 __main__:092:logMetrics E19 val      class 0  99.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:29,706 INFO     pid:9731 __main__:108:logMetrics E19 val_neg  class 0 0.4694 loss, 100.0% correct (9072 of 9072)\n",
      "2024-02-01 13:57:29,706 INFO     pid:9731 __main__:125:logMetrics E19 val_pos  class 0 4.8971 loss,   0.0% correct (0 of 88)\n",
      "2024-02-01 13:57:29,707 INFO     pid:9731 __main__:092:logMetrics E19 val      class 1  81.0% correct, 0.8100 precision, 1.0000 recall, 0.8951 f1 score\n",
      "2024-02-01 13:57:29,707 INFO     pid:9731 __main__:108:logMetrics E19 val_neg  class 1 1.9409 loss,   0.0% correct (0 of 1740)\n",
      "2024-02-01 13:57:29,707 INFO     pid:9731 __main__:125:logMetrics E19 val_pos  class 1 0.1769 loss, 100.0% correct (7420 of 7420)\n",
      "2024-02-01 13:57:29,708 INFO     pid:9731 __main__:092:logMetrics E19 val      class 2  82.0% correct, 0.0000 precision, 0.0000 recall, 0.0000 f1 score\n",
      "2024-02-01 13:57:29,708 INFO     pid:9731 __main__:108:logMetrics E19 val_neg  class 2 0.2322 loss, 100.0% correct (7508 of 7508)\n",
      "2024-02-01 13:57:29,708 INFO     pid:9731 __main__:125:logMetrics E19 val_pos  class 2 1.7834 loss,   0.0% correct (0 of 1652)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:0:00:38.152595\n"
     ]
    }
   ],
   "source": [
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "config = {\n",
    "    \"return_period\": 5,\n",
    "    \"seq_len\": 5,\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.11646759543664197,\n",
    "    \"optim_type\": 1,  # Adam\n",
    "    \"num_layers\": 4,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_fc_layers\": 1,\n",
    "    \"activation_type\": 2,  # Sigmoid\n",
    "}\n",
    "# epoch_num = 20\n",
    "# os.mkdir(log_dir)\n",
    "print(log_dir)\n",
    "start = datetime.now()\n",
    "train_LSTM(config)\n",
    "print(f\"Elapsed time:{datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_layers', 'hidden_size', 'num_fc_layers', 'activation_type']\n",
      "Total count of configs = 48\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"return_period\": tune.grid_search([5]),  # [2,3,5,10]\n",
    "    \"seq_len\": tune.grid_search([5]),  # 10]),\n",
    "    \"lr\": tune.grid_search([0.01]),  # [0.001, 0.01, 0.1]\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([1]),  # 1: Adam, 2: SGD\n",
    "    \"num_layers\": tune.grid_search([4, 16]),  # [1, 2, 4, 8]\n",
    "    \"hidden_size\": tune.grid_search([32, 64, 128, 256]),  # [8, 16, 32, 64, 128]\n",
    "    \"num_fc_layers\": tune.grid_search([1, 2]),  # 1, 2, 3]),\n",
    "    \"activation_type\": tune.grid_search(\n",
    "        [1, 2, 3]\n",
    "    ),  # 1: ReLU(),  2: Sigmoid(),  3: Tanh()\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if (\n",
    "        type(v).__name__ == \"dict\"\n",
    "        and list(v.keys())[0] == \"grid_search\"\n",
    "        and len(list(v.values())[0]) > 1\n",
    "    ):\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 07:46:06,366\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-31 09:44:07</td></tr>\n",
       "<tr><td>Running for: </td><td>01:58:00.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.3/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0.1/32 CPUs, 0.1/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  activation_type</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  num_fc_layers</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  optim_type</th><th style=\"text-align: right;\">  return_period</th><th style=\"text-align: right;\">  seq_len</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_d8976_00000</td><td>TERMINATED</td><td>192.168.0.125:24015 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.146467</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.760761</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1051.29 </td><td style=\"text-align: right;\">0.279489</td><td style=\"text-align: right;\">        0.877123</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00001</td><td>TERMINATED</td><td>192.168.0.125:24016 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.792941</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.755978</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.39 </td><td style=\"text-align: right;\">0.275412</td><td style=\"text-align: right;\">        0.879232</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00002</td><td>TERMINATED</td><td>192.168.0.125:24017 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.580892</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.775761</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.67 </td><td style=\"text-align: right;\">0.282232</td><td style=\"text-align: right;\">        0.876717</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00003</td><td>TERMINATED</td><td>192.168.0.125:24018 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.666458</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.779348</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.68 </td><td style=\"text-align: right;\">0.247599</td><td style=\"text-align: right;\">        0.892131</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00004</td><td>TERMINATED</td><td>192.168.0.125:24019 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.116468</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.767283</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.02 </td><td style=\"text-align: right;\">0.255386</td><td style=\"text-align: right;\">        0.889995</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00005</td><td>TERMINATED</td><td>192.168.0.125:24020 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.875928</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.748696</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.54 </td><td style=\"text-align: right;\">0.276691</td><td style=\"text-align: right;\">        0.879313</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00006</td><td>TERMINATED</td><td>192.168.0.125:24021 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.765954</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764783</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.53 </td><td style=\"text-align: right;\">0.2478  </td><td style=\"text-align: right;\">        0.894348</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00007</td><td>TERMINATED</td><td>192.168.0.125:24022 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.269871</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.758804</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1051.99 </td><td style=\"text-align: right;\">0.25969 </td><td style=\"text-align: right;\">        0.888345</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00008</td><td>TERMINATED</td><td>192.168.0.125:24023 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.24546 </td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.781739</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050    </td><td style=\"text-align: right;\">0.221508</td><td style=\"text-align: right;\">        0.90484 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00009</td><td>TERMINATED</td><td>192.168.0.125:24024 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.246724</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.743587</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         831.816</td><td style=\"text-align: right;\">0.214047</td><td style=\"text-align: right;\">        0.910032</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00010</td><td>TERMINATED</td><td>192.168.0.125:42405 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.343394</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.746739</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         842.169</td><td style=\"text-align: right;\">0.238096</td><td style=\"text-align: right;\">        0.896376</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00011</td><td>TERMINATED</td><td>192.168.0.125:46919 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.519805</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75587 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         844.187</td><td style=\"text-align: right;\">0.206978</td><td style=\"text-align: right;\">        0.913521</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00012</td><td>TERMINATED</td><td>192.168.0.125:46923 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.445556</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.769022</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1105.5  </td><td style=\"text-align: right;\">0.297823</td><td style=\"text-align: right;\">        0.870714</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00013</td><td>TERMINATED</td><td>192.168.0.125:47101 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.332983</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.746087</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1103.6  </td><td style=\"text-align: right;\">0.291819</td><td style=\"text-align: right;\">        0.872715</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00014</td><td>TERMINATED</td><td>192.168.0.125:47102 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.589482</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.777609</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.96 </td><td style=\"text-align: right;\">0.283477</td><td style=\"text-align: right;\">        0.875014</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00015</td><td>TERMINATED</td><td>192.168.0.125:47103 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.211595</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.761957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.98 </td><td style=\"text-align: right;\">0.263868</td><td style=\"text-align: right;\">        0.88464 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00016</td><td>TERMINATED</td><td>192.168.0.125:47104 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.333716</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75    </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1104.52 </td><td style=\"text-align: right;\">0.270398</td><td style=\"text-align: right;\">        0.882639</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00017</td><td>TERMINATED</td><td>192.168.0.125:47105 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.393089</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.770543</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1105.68 </td><td style=\"text-align: right;\">0.259121</td><td style=\"text-align: right;\">        0.887128</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00018</td><td>TERMINATED</td><td>192.168.0.125:47112 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.464856</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.760978</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.03 </td><td style=\"text-align: right;\">0.258532</td><td style=\"text-align: right;\">        0.888048</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00019</td><td>TERMINATED</td><td>192.168.0.125:47422 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.728141</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.754239</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1103.1  </td><td style=\"text-align: right;\">0.282495</td><td style=\"text-align: right;\">        0.879367</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00020</td><td>TERMINATED</td><td>192.168.0.125:60806 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.259739</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757609</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1297.25 </td><td style=\"text-align: right;\">0.252851</td><td style=\"text-align: right;\">        0.893294</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00021</td><td>TERMINATED</td><td>192.168.0.125:65214 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.511388</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757717</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         992.928</td><td style=\"text-align: right;\">0.232827</td><td style=\"text-align: right;\">        0.902299</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00022</td><td>TERMINATED</td><td>192.168.0.125:71098 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.573932</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.729674</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1097.44 </td><td style=\"text-align: right;\">0.253164</td><td style=\"text-align: right;\">        0.891888</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00023</td><td>TERMINATED</td><td>192.168.0.125:71211 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.13716 </td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.754239</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1096.79 </td><td style=\"text-align: right;\">0.246975</td><td style=\"text-align: right;\">        0.893618</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00024</td><td>TERMINATED</td><td>192.168.0.125:71212 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.586036</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.758152</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2028.17 </td><td style=\"text-align: right;\">0.70876 </td><td style=\"text-align: right;\">        0.763953</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00025</td><td>TERMINATED</td><td>192.168.0.125:71478 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.236419</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766848</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2024.58 </td><td style=\"text-align: right;\">0.712974</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00026</td><td>TERMINATED</td><td>192.168.0.125:71479 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.152041</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2024.66 </td><td style=\"text-align: right;\">0.710159</td><td style=\"text-align: right;\">        0.763034</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00027</td><td>TERMINATED</td><td>192.168.0.125:71480 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.859108</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.767065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1194.47 </td><td style=\"text-align: right;\">0.713082</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00028</td><td>TERMINATED</td><td>192.168.0.125:71481 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.872506</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.763696</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1188.06 </td><td style=\"text-align: right;\">0.711219</td><td style=\"text-align: right;\">        0.762574</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00029</td><td>TERMINATED</td><td>192.168.0.125:71959 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.746718</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.763804</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1185.85 </td><td style=\"text-align: right;\">0.711106</td><td style=\"text-align: right;\">        0.762493</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00030</td><td>TERMINATED</td><td>192.168.0.125:84837 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.343691</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768043</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1665.3  </td><td style=\"text-align: right;\">0.712976</td><td style=\"text-align: right;\">        0.76152 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00031</td><td>TERMINATED</td><td>192.168.0.125:86321 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.178138</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764891</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1701.69 </td><td style=\"text-align: right;\">0.711721</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00032</td><td>TERMINATED</td><td>192.168.0.125:90472 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.647386</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.7675  </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1785.74 </td><td style=\"text-align: right;\">0.712908</td><td style=\"text-align: right;\">        0.761709</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00033</td><td>TERMINATED</td><td>192.168.0.125:90573 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.452122</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762717</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1792.14 </td><td style=\"text-align: right;\">0.710962</td><td style=\"text-align: right;\">        0.76271 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00034</td><td>TERMINATED</td><td>192.168.0.125:92487 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.197631</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1789.91 </td><td style=\"text-align: right;\">0.709124</td><td style=\"text-align: right;\">        0.764224</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00035</td><td>TERMINATED</td><td>192.168.0.125:92603 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.496142</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.759565</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1793.13 </td><td style=\"text-align: right;\">0.713636</td><td style=\"text-align: right;\">        0.763575</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00036</td><td>TERMINATED</td><td>192.168.0.125:92933 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.127511</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768587</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2565.55 </td><td style=\"text-align: right;\">0.713929</td><td style=\"text-align: right;\">        0.761222</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00037</td><td>TERMINATED</td><td>192.168.0.125:101869</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.827456</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2523.46 </td><td style=\"text-align: right;\">0.713243</td><td style=\"text-align: right;\">        0.761817</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00038</td><td>TERMINATED</td><td>192.168.0.125:101870</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.307024</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2537.13 </td><td style=\"text-align: right;\">0.712817</td><td style=\"text-align: right;\">        0.761763</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00039</td><td>TERMINATED</td><td>192.168.0.125:102041</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.630018</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.765326</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1647.38 </td><td style=\"text-align: right;\">0.712451</td><td style=\"text-align: right;\">        0.762196</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00040</td><td>TERMINATED</td><td>192.168.0.125:106248</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.349369</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764891</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1643.49 </td><td style=\"text-align: right;\">0.712109</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00041</td><td>TERMINATED</td><td>192.168.0.125:107749</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.516054</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762174</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1647.46 </td><td style=\"text-align: right;\">0.711074</td><td style=\"text-align: right;\">        0.762926</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00042</td><td>TERMINATED</td><td>192.168.0.125:112004</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.537368</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.756413</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1660.4  </td><td style=\"text-align: right;\">0.708159</td><td style=\"text-align: right;\">        0.764413</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00043</td><td>TERMINATED</td><td>192.168.0.125:112305</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.247884</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75913 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1656.65 </td><td style=\"text-align: right;\">0.709709</td><td style=\"text-align: right;\">        0.763683</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00044</td><td>TERMINATED</td><td>192.168.0.125:113479</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.875668</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768043</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1630.44 </td><td style=\"text-align: right;\">0.714536</td><td style=\"text-align: right;\">        0.761466</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00045</td><td>TERMINATED</td><td>192.168.0.125:113722</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.720106</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764457</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1632.09 </td><td style=\"text-align: right;\">0.71276 </td><td style=\"text-align: right;\">        0.762304</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00046</td><td>TERMINATED</td><td>192.168.0.125:122624</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.851599</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762174</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1212.52 </td><td style=\"text-align: right;\">0.711295</td><td style=\"text-align: right;\">        0.76298 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00047</td><td>TERMINATED</td><td>192.168.0.125:123712</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.815862</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.759348</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1146.56 </td><td style=\"text-align: right;\">0.710236</td><td style=\"text-align: right;\">        0.763548</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  mean_accuracy</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_d8976_00000</td><td style=\"text-align: right;\">0.279489</td><td style=\"text-align: right;\">       0.760761</td><td style=\"text-align: right;\">        0.877123</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00001</td><td style=\"text-align: right;\">0.275412</td><td style=\"text-align: right;\">       0.755978</td><td style=\"text-align: right;\">        0.879232</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00002</td><td style=\"text-align: right;\">0.282232</td><td style=\"text-align: right;\">       0.775761</td><td style=\"text-align: right;\">        0.876717</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00003</td><td style=\"text-align: right;\">0.247599</td><td style=\"text-align: right;\">       0.779348</td><td style=\"text-align: right;\">        0.892131</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00004</td><td style=\"text-align: right;\">0.255386</td><td style=\"text-align: right;\">       0.767283</td><td style=\"text-align: right;\">        0.889995</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00005</td><td style=\"text-align: right;\">0.276691</td><td style=\"text-align: right;\">       0.748696</td><td style=\"text-align: right;\">        0.879313</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00006</td><td style=\"text-align: right;\">0.2478  </td><td style=\"text-align: right;\">       0.764783</td><td style=\"text-align: right;\">        0.894348</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00007</td><td style=\"text-align: right;\">0.25969 </td><td style=\"text-align: right;\">       0.758804</td><td style=\"text-align: right;\">        0.888345</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00008</td><td style=\"text-align: right;\">0.221508</td><td style=\"text-align: right;\">       0.781739</td><td style=\"text-align: right;\">        0.90484 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00009</td><td style=\"text-align: right;\">0.214047</td><td style=\"text-align: right;\">       0.743587</td><td style=\"text-align: right;\">        0.910032</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00010</td><td style=\"text-align: right;\">0.238096</td><td style=\"text-align: right;\">       0.746739</td><td style=\"text-align: right;\">        0.896376</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00011</td><td style=\"text-align: right;\">0.206978</td><td style=\"text-align: right;\">       0.75587 </td><td style=\"text-align: right;\">        0.913521</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00012</td><td style=\"text-align: right;\">0.297823</td><td style=\"text-align: right;\">       0.769022</td><td style=\"text-align: right;\">        0.870714</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00013</td><td style=\"text-align: right;\">0.291819</td><td style=\"text-align: right;\">       0.746087</td><td style=\"text-align: right;\">        0.872715</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00014</td><td style=\"text-align: right;\">0.283477</td><td style=\"text-align: right;\">       0.777609</td><td style=\"text-align: right;\">        0.875014</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00015</td><td style=\"text-align: right;\">0.263868</td><td style=\"text-align: right;\">       0.761957</td><td style=\"text-align: right;\">        0.88464 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00016</td><td style=\"text-align: right;\">0.270398</td><td style=\"text-align: right;\">       0.75    </td><td style=\"text-align: right;\">        0.882639</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00017</td><td style=\"text-align: right;\">0.259121</td><td style=\"text-align: right;\">       0.770543</td><td style=\"text-align: right;\">        0.887128</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00018</td><td style=\"text-align: right;\">0.258532</td><td style=\"text-align: right;\">       0.760978</td><td style=\"text-align: right;\">        0.888048</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00019</td><td style=\"text-align: right;\">0.282495</td><td style=\"text-align: right;\">       0.754239</td><td style=\"text-align: right;\">        0.879367</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00020</td><td style=\"text-align: right;\">0.252851</td><td style=\"text-align: right;\">       0.757609</td><td style=\"text-align: right;\">        0.893294</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00021</td><td style=\"text-align: right;\">0.232827</td><td style=\"text-align: right;\">       0.757717</td><td style=\"text-align: right;\">        0.902299</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00022</td><td style=\"text-align: right;\">0.253164</td><td style=\"text-align: right;\">       0.729674</td><td style=\"text-align: right;\">        0.891888</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00023</td><td style=\"text-align: right;\">0.246975</td><td style=\"text-align: right;\">       0.754239</td><td style=\"text-align: right;\">        0.893618</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00024</td><td style=\"text-align: right;\">0.70876 </td><td style=\"text-align: right;\">       0.758152</td><td style=\"text-align: right;\">        0.763953</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00025</td><td style=\"text-align: right;\">0.712974</td><td style=\"text-align: right;\">       0.766848</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00026</td><td style=\"text-align: right;\">0.710159</td><td style=\"text-align: right;\">       0.762065</td><td style=\"text-align: right;\">        0.763034</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00027</td><td style=\"text-align: right;\">0.713082</td><td style=\"text-align: right;\">       0.767065</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00028</td><td style=\"text-align: right;\">0.711219</td><td style=\"text-align: right;\">       0.763696</td><td style=\"text-align: right;\">        0.762574</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00029</td><td style=\"text-align: right;\">0.711106</td><td style=\"text-align: right;\">       0.763804</td><td style=\"text-align: right;\">        0.762493</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00030</td><td style=\"text-align: right;\">0.712976</td><td style=\"text-align: right;\">       0.768043</td><td style=\"text-align: right;\">        0.76152 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00031</td><td style=\"text-align: right;\">0.711721</td><td style=\"text-align: right;\">       0.764891</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00032</td><td style=\"text-align: right;\">0.712908</td><td style=\"text-align: right;\">       0.7675  </td><td style=\"text-align: right;\">        0.761709</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00033</td><td style=\"text-align: right;\">0.710962</td><td style=\"text-align: right;\">       0.762717</td><td style=\"text-align: right;\">        0.76271 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00034</td><td style=\"text-align: right;\">0.709124</td><td style=\"text-align: right;\">       0.757065</td><td style=\"text-align: right;\">        0.764224</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00035</td><td style=\"text-align: right;\">0.713636</td><td style=\"text-align: right;\">       0.759565</td><td style=\"text-align: right;\">        0.763575</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00036</td><td style=\"text-align: right;\">0.713929</td><td style=\"text-align: right;\">       0.768587</td><td style=\"text-align: right;\">        0.761222</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00037</td><td style=\"text-align: right;\">0.713243</td><td style=\"text-align: right;\">       0.766957</td><td style=\"text-align: right;\">        0.761817</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00038</td><td style=\"text-align: right;\">0.712817</td><td style=\"text-align: right;\">       0.766957</td><td style=\"text-align: right;\">        0.761763</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00039</td><td style=\"text-align: right;\">0.712451</td><td style=\"text-align: right;\">       0.765326</td><td style=\"text-align: right;\">        0.762196</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00040</td><td style=\"text-align: right;\">0.712109</td><td style=\"text-align: right;\">       0.764891</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00041</td><td style=\"text-align: right;\">0.711074</td><td style=\"text-align: right;\">       0.762174</td><td style=\"text-align: right;\">        0.762926</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00042</td><td style=\"text-align: right;\">0.708159</td><td style=\"text-align: right;\">       0.756413</td><td style=\"text-align: right;\">        0.764413</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00043</td><td style=\"text-align: right;\">0.709709</td><td style=\"text-align: right;\">       0.75913 </td><td style=\"text-align: right;\">        0.763683</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00044</td><td style=\"text-align: right;\">0.714536</td><td style=\"text-align: right;\">       0.768043</td><td style=\"text-align: right;\">        0.761466</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00045</td><td style=\"text-align: right;\">0.71276 </td><td style=\"text-align: right;\">       0.764457</td><td style=\"text-align: right;\">        0.762304</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00046</td><td style=\"text-align: right;\">0.711295</td><td style=\"text-align: right;\">       0.762174</td><td style=\"text-align: right;\">        0.76298 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00047</td><td style=\"text-align: right;\">0.710236</td><td style=\"text-align: right;\">       0.759348</td><td style=\"text-align: right;\">        0.763548</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 09:44:07,044\tINFO tune.py:1042 -- Total run time: 7080.68 seconds (7080.65 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "os.mkdir(log_dir)\n",
    "analysis = tune.run(\n",
    "    train_LSTM,\n",
    "    config=search_space,\n",
    "    resources_per_trial={\"cpu\": 0.1, \"gpu\": 0.1},\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_accuracy     trial_id\n",
      "0           0.760  d8976_00000\n",
      "1           0.770  d8976_00001\n",
      "2           0.781  d8976_00002\n",
      "3           0.775  d8976_00003\n",
      "4           0.782  d8976_00004\n",
      "5           0.760  d8976_00005\n",
      "6           0.750  d8976_00006\n",
      "7           0.763  d8976_00007\n",
      "8           0.774  d8976_00008\n",
      "9           0.753  d8976_00009\n",
      "10          0.750  d8976_00010\n",
      "11          0.753  d8976_00011\n",
      "12          0.764  d8976_00012\n",
      "13          0.758  d8976_00013\n",
      "14          0.775  d8976_00014\n",
      "15          0.777  d8976_00015\n",
      "16          0.760  d8976_00016\n",
      "17          0.760  d8976_00017\n",
      "18          0.758  d8976_00018\n",
      "19          0.758  d8976_00019\n",
      "20          0.755  d8976_00020\n",
      "21          0.761  d8976_00021\n",
      "22          0.738  d8976_00022\n",
      "23          0.757  d8976_00023\n",
      "24          0.758  d8976_00024\n",
      "25          0.767  d8976_00025\n",
      "26          0.762  d8976_00026\n",
      "27          0.767  d8976_00027\n",
      "28          0.764  d8976_00028\n",
      "29          0.764  d8976_00029\n",
      "30          0.768  d8976_00030\n",
      "31          0.765  d8976_00031\n",
      "32          0.767  d8976_00032\n",
      "33          0.763  d8976_00033\n",
      "34          0.757  d8976_00034\n",
      "35          0.760  d8976_00035\n",
      "36          0.769  d8976_00036\n",
      "37          0.767  d8976_00037\n",
      "38          0.767  d8976_00038\n",
      "39          0.765  d8976_00039\n",
      "40          0.765  d8976_00040\n",
      "41          0.762  d8976_00041\n",
      "42          0.756  d8976_00042\n",
      "43          0.759  d8976_00043\n",
      "44          0.768  d8976_00044\n",
      "45          0.764  d8976_00045\n",
      "46          0.762  d8976_00046\n",
      "47          0.759  d8976_00047\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "trial_list = list(analysis.trial_dataframes.values())\n",
    "for i, trial in enumerate(trial_list):\n",
    "    if trial.empty == False:\n",
    "        d = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"mean_accuracy\": trial.describe().loc[\"mean\", \"mean_accuracy\"],\n",
    "                \"trial_id\": trial.loc[0:0, \"trial_id\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": [np.NaN], \"trial_id\": [np.NaN]})\n",
    "    accuracy_list.append(d)\n",
    "accuracy_df = pd.concat(accuracy_list)\n",
    "accuracy_df = accuracy_df.reset_index().loc[:, [\"mean_accuracy\", \"trial_id\"]]\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0               5        5 0.010     0.146           1           4           32              1                1\n",
      "1               5        5 0.010     0.793           1           4           32              1                2\n",
      "2               5        5 0.010     0.581           1           4           32              1                3\n",
      "3               5        5 0.010     0.666           1           4           64              1                1\n",
      "4               5        5 0.010     0.116           1           4           64              1                2\n",
      "5               5        5 0.010     0.876           1           4           64              1                3\n",
      "6               5        5 0.010     0.766           1           4          128              1                1\n",
      "7               5        5 0.010     0.270           1           4          128              1                2\n",
      "8               5        5 0.010     0.245           1           4          128              1                3\n",
      "9               5        5 0.010     0.247           1           4          256              1                1\n",
      "10              5        5 0.010     0.343           1           4          256              1                2\n",
      "11              5        5 0.010     0.520           1           4          256              1                3\n",
      "12              5        5 0.010     0.446           1           4           32              2                1\n",
      "13              5        5 0.010     0.333           1           4           32              2                2\n",
      "14              5        5 0.010     0.589           1           4           32              2                3\n",
      "15              5        5 0.010     0.212           1           4           64              2                1\n",
      "16              5        5 0.010     0.334           1           4           64              2                2\n",
      "17              5        5 0.010     0.393           1           4           64              2                3\n",
      "18              5        5 0.010     0.465           1           4          128              2                1\n",
      "19              5        5 0.010     0.728           1           4          128              2                2\n",
      "20              5        5 0.010     0.260           1           4          128              2                3\n",
      "21              5        5 0.010     0.511           1           4          256              2                1\n",
      "22              5        5 0.010     0.574           1           4          256              2                2\n",
      "23              5        5 0.010     0.137           1           4          256              2                3\n",
      "24              5        5 0.010     0.586           1          16           32              1                1\n",
      "25              5        5 0.010     0.236           1          16           32              1                2\n",
      "26              5        5 0.010     0.152           1          16           32              1                3\n",
      "27              5        5 0.010     0.859           1          16           64              1                1\n",
      "28              5        5 0.010     0.873           1          16           64              1                2\n",
      "29              5        5 0.010     0.747           1          16           64              1                3\n",
      "30              5        5 0.010     0.344           1          16          128              1                1\n",
      "31              5        5 0.010     0.178           1          16          128              1                2\n",
      "32              5        5 0.010     0.647           1          16          128              1                3\n",
      "33              5        5 0.010     0.452           1          16          256              1                1\n",
      "34              5        5 0.010     0.198           1          16          256              1                2\n",
      "35              5        5 0.010     0.496           1          16          256              1                3\n",
      "36              5        5 0.010     0.128           1          16           32              2                1\n",
      "37              5        5 0.010     0.827           1          16           32              2                2\n",
      "38              5        5 0.010     0.307           1          16           32              2                3\n",
      "39              5        5 0.010     0.630           1          16           64              2                1\n",
      "40              5        5 0.010     0.349           1          16           64              2                2\n",
      "41              5        5 0.010     0.516           1          16           64              2                3\n",
      "42              5        5 0.010     0.537           1          16          128              2                1\n",
      "43              5        5 0.010     0.248           1          16          128              2                2\n",
      "44              5        5 0.010     0.876           1          16          128              2                3\n",
      "45              5        5 0.010     0.720           1          16          256              2                1\n",
      "46              5        5 0.010     0.852           1          16          256              2                2\n",
      "47              5        5 0.010     0.816           1          16          256              2                3\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0           0.760  d8976_00000              5        5 0.010     0.146           1           4           32              1                1\n",
      "1           0.770  d8976_00001              5        5 0.010     0.793           1           4           32              1                2\n",
      "2           0.781  d8976_00002              5        5 0.010     0.581           1           4           32              1                3\n",
      "3           0.775  d8976_00003              5        5 0.010     0.666           1           4           64              1                1\n",
      "4           0.782  d8976_00004              5        5 0.010     0.116           1           4           64              1                2\n",
      "5           0.760  d8976_00005              5        5 0.010     0.876           1           4           64              1                3\n",
      "6           0.750  d8976_00006              5        5 0.010     0.766           1           4          128              1                1\n",
      "7           0.763  d8976_00007              5        5 0.010     0.270           1           4          128              1                2\n",
      "8           0.774  d8976_00008              5        5 0.010     0.245           1           4          128              1                3\n",
      "9           0.753  d8976_00009              5        5 0.010     0.247           1           4          256              1                1\n",
      "10          0.750  d8976_00010              5        5 0.010     0.343           1           4          256              1                2\n",
      "11          0.753  d8976_00011              5        5 0.010     0.520           1           4          256              1                3\n",
      "12          0.764  d8976_00012              5        5 0.010     0.446           1           4           32              2                1\n",
      "13          0.758  d8976_00013              5        5 0.010     0.333           1           4           32              2                2\n",
      "14          0.775  d8976_00014              5        5 0.010     0.589           1           4           32              2                3\n",
      "15          0.777  d8976_00015              5        5 0.010     0.212           1           4           64              2                1\n",
      "16          0.760  d8976_00016              5        5 0.010     0.334           1           4           64              2                2\n",
      "17          0.760  d8976_00017              5        5 0.010     0.393           1           4           64              2                3\n",
      "18          0.758  d8976_00018              5        5 0.010     0.465           1           4          128              2                1\n",
      "19          0.758  d8976_00019              5        5 0.010     0.728           1           4          128              2                2\n",
      "20          0.755  d8976_00020              5        5 0.010     0.260           1           4          128              2                3\n",
      "21          0.761  d8976_00021              5        5 0.010     0.511           1           4          256              2                1\n",
      "22          0.738  d8976_00022              5        5 0.010     0.574           1           4          256              2                2\n",
      "23          0.757  d8976_00023              5        5 0.010     0.137           1           4          256              2                3\n",
      "24          0.758  d8976_00024              5        5 0.010     0.586           1          16           32              1                1\n",
      "25          0.767  d8976_00025              5        5 0.010     0.236           1          16           32              1                2\n",
      "26          0.762  d8976_00026              5        5 0.010     0.152           1          16           32              1                3\n",
      "27          0.767  d8976_00027              5        5 0.010     0.859           1          16           64              1                1\n",
      "28          0.764  d8976_00028              5        5 0.010     0.873           1          16           64              1                2\n",
      "29          0.764  d8976_00029              5        5 0.010     0.747           1          16           64              1                3\n",
      "30          0.768  d8976_00030              5        5 0.010     0.344           1          16          128              1                1\n",
      "31          0.765  d8976_00031              5        5 0.010     0.178           1          16          128              1                2\n",
      "32          0.767  d8976_00032              5        5 0.010     0.647           1          16          128              1                3\n",
      "33          0.763  d8976_00033              5        5 0.010     0.452           1          16          256              1                1\n",
      "34          0.757  d8976_00034              5        5 0.010     0.198           1          16          256              1                2\n",
      "35          0.760  d8976_00035              5        5 0.010     0.496           1          16          256              1                3\n",
      "36          0.769  d8976_00036              5        5 0.010     0.128           1          16           32              2                1\n",
      "37          0.767  d8976_00037              5        5 0.010     0.827           1          16           32              2                2\n",
      "38          0.767  d8976_00038              5        5 0.010     0.307           1          16           32              2                3\n",
      "39          0.765  d8976_00039              5        5 0.010     0.630           1          16           64              2                1\n",
      "40          0.765  d8976_00040              5        5 0.010     0.349           1          16           64              2                2\n",
      "41          0.762  d8976_00041              5        5 0.010     0.516           1          16           64              2                3\n",
      "42          0.756  d8976_00042              5        5 0.010     0.537           1          16          128              2                1\n",
      "43          0.759  d8976_00043              5        5 0.010     0.248           1          16          128              2                2\n",
      "44          0.768  d8976_00044              5        5 0.010     0.876           1          16          128              2                3\n",
      "45          0.764  d8976_00045              5        5 0.010     0.720           1          16          256              2                1\n",
      "46          0.762  d8976_00046              5        5 0.010     0.852           1          16          256              2                2\n",
      "47          0.759  d8976_00047              5        5 0.010     0.816           1          16          256              2                3\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "4           0.782  d8976_00004              5        5 0.010     0.116           1           4           64              1                2\n",
      "2           0.781  d8976_00002              5        5 0.010     0.581           1           4           32              1                3\n",
      "15          0.777  d8976_00015              5        5 0.010     0.212           1           4           64              2                1\n",
      "3           0.775  d8976_00003              5        5 0.010     0.666           1           4           64              1                1\n",
      "14          0.775  d8976_00014              5        5 0.010     0.589           1           4           32              2                3\n",
      "8           0.774  d8976_00008              5        5 0.010     0.245           1           4          128              1                3\n",
      "1           0.770  d8976_00001              5        5 0.010     0.793           1           4           32              1                2\n",
      "36          0.769  d8976_00036              5        5 0.010     0.128           1          16           32              2                1\n",
      "44          0.768  d8976_00044              5        5 0.010     0.876           1          16          128              2                3\n",
      "30          0.768  d8976_00030              5        5 0.010     0.344           1          16          128              1                1\n",
      "32          0.767  d8976_00032              5        5 0.010     0.647           1          16          128              1                3\n",
      "27          0.767  d8976_00027              5        5 0.010     0.859           1          16           64              1                1\n",
      "37          0.767  d8976_00037              5        5 0.010     0.827           1          16           32              2                2\n",
      "38          0.767  d8976_00038              5        5 0.010     0.307           1          16           32              2                3\n",
      "25          0.767  d8976_00025              5        5 0.010     0.236           1          16           32              1                2\n",
      "39          0.765  d8976_00039              5        5 0.010     0.630           1          16           64              2                1\n",
      "40          0.765  d8976_00040              5        5 0.010     0.349           1          16           64              2                2\n",
      "31          0.765  d8976_00031              5        5 0.010     0.178           1          16          128              1                2\n",
      "45          0.764  d8976_00045              5        5 0.010     0.720           1          16          256              2                1\n",
      "29          0.764  d8976_00029              5        5 0.010     0.747           1          16           64              1                3\n",
      "12          0.764  d8976_00012              5        5 0.010     0.446           1           4           32              2                1\n",
      "28          0.764  d8976_00028              5        5 0.010     0.873           1          16           64              1                2\n",
      "7           0.763  d8976_00007              5        5 0.010     0.270           1           4          128              1                2\n",
      "33          0.763  d8976_00033              5        5 0.010     0.452           1          16          256              1                1\n",
      "46          0.762  d8976_00046              5        5 0.010     0.852           1          16          256              2                2\n",
      "41          0.762  d8976_00041              5        5 0.010     0.516           1          16           64              2                3\n",
      "26          0.762  d8976_00026              5        5 0.010     0.152           1          16           32              1                3\n",
      "21          0.761  d8976_00021              5        5 0.010     0.511           1           4          256              2                1\n",
      "16          0.760  d8976_00016              5        5 0.010     0.334           1           4           64              2                2\n",
      "5           0.760  d8976_00005              5        5 0.010     0.876           1           4           64              1                3\n",
      "17          0.760  d8976_00017              5        5 0.010     0.393           1           4           64              2                3\n",
      "0           0.760  d8976_00000              5        5 0.010     0.146           1           4           32              1                1\n",
      "35          0.760  d8976_00035              5        5 0.010     0.496           1          16          256              1                3\n",
      "47          0.759  d8976_00047              5        5 0.010     0.816           1          16          256              2                3\n",
      "43          0.759  d8976_00043              5        5 0.010     0.248           1          16          128              2                2\n",
      "18          0.758  d8976_00018              5        5 0.010     0.465           1           4          128              2                1\n",
      "13          0.758  d8976_00013              5        5 0.010     0.333           1           4           32              2                2\n",
      "24          0.758  d8976_00024              5        5 0.010     0.586           1          16           32              1                1\n",
      "19          0.758  d8976_00019              5        5 0.010     0.728           1           4          128              2                2\n",
      "34          0.757  d8976_00034              5        5 0.010     0.198           1          16          256              1                2\n",
      "23          0.757  d8976_00023              5        5 0.010     0.137           1           4          256              2                3\n",
      "42          0.756  d8976_00042              5        5 0.010     0.537           1          16          128              2                1\n",
      "20          0.755  d8976_00020              5        5 0.010     0.260           1           4          128              2                3\n",
      "11          0.753  d8976_00011              5        5 0.010     0.520           1           4          256              1                3\n",
      "9           0.753  d8976_00009              5        5 0.010     0.247           1           4          256              1                1\n",
      "6           0.750  d8976_00006              5        5 0.010     0.766           1           4          128              1                1\n",
      "10          0.750  d8976_00010              5        5 0.010     0.343           1           4          256              1                2\n",
      "22          0.738  d8976_00022              5        5 0.010     0.574           1           4          256              2                2\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-31_07.46.06/5_5_0.01_0.11646759543664197_1_4_64_1_2.pt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "config_df = pd.DataFrame(analysis.get_all_configs().values())\n",
    "print(config_df)\n",
    "\n",
    "results = pd.concat([accuracy_df, config_df], axis=1)\n",
    "print(results)\n",
    "\n",
    "sorted_results = results.sort_values(by=\"mean_accuracy\", ascending=False)\n",
    "print(sorted_results.head(100))\n",
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results.to_csv(sorted_results_file)\n",
    "\n",
    "best_config = config_df.iloc[sorted_results.index[0]]\n",
    "id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in best_config.to_list())\n",
    "best_model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/StockPCTLabelPredictLSTM.pt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(best_model_name, f\"{log_dir_base}/{task_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLsAAADTCAYAAABp7hHfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAviUlEQVR4nO3deXBU5ZrH8V9nJ5CFEEIIu2zKLiAxCEQFCcs4LJYCCoRdHBi5BlFQLwhYBi6XbQSNXoEo4qAoA14RRQOIShDZdBBl17AkAVkSEiQs/c4fFj32TYCk051Our+fqlNFv/2e9zzvS+fkqSenz7EYY4wAAAAAAAAAD+Dj7gAAAAAAAAAAZ6HYBQAAAAAAAI9BsQsAAAAAAAAeg2IXAAAAAAAAPAbFLgAAAAAAAHgMil0AAAAAAADwGBS7AAAAAAAA4DEodgEAAAAAAMBjUOwCAAAAAACAx6DYBcBrzZkzR7fddpt8fX3Vpk0bp42bmpoqi8WiX375xWljAgAAOBu5EABPRbELgFfasGGDnnnmGd1zzz1atmyZXn75ZXeHVCFlZmZq8uTJuu+++xQSEiKLxaLNmze7OywAAHAL5ELOkZaWphEjRqhJkyYKDg7WbbfdplGjRikzM9PdoQFezc/dAQCAO2zcuFE+Pj5asmSJAgIC3B1OhbV//37Nnj1bjRs3VsuWLZWenu7ukAAAQDGQCznHs88+q7Nnz+rhhx9W48aNdeTIES1atEgff/yx9uzZo+joaHeHCHglil0AvNKpU6dUqVIlr0zu8vPzVblyZaeM1a5dO505c0YRERH64IMP9PDDDztlXAAA4FrkQs7JhebNm6dOnTrJx+f/vzTVo0cPxcfHa9GiRXrppZecchwAJcPXGAGU2osvviiLxaJDhw5p2LBhCg8PV1hYmIYPH66LFy9Kkn755RdZLBalpqYW2t9isejFF18sNN6BAwc0ePBghYWFqXr16vrrX/8qY4yOHTumPn36KDQ0VNHR0Zo7d26J4rVYLFq2bJny8/NlsVgKxfXOO++oQ4cOCg4OVtWqVdWlSxdt2LDBkaWxWbt2rXr37q2YmBgFBgaqYcOGmjlzpq5du2brM23aNPn7++v06dOF9h8zZozCw8N16dIlW9v69evVuXNnVa5cWSEhIerdu7d+/PFHu/2GDRumKlWq6PDhw+rVq5dCQkL02GOPSZIOHjyohx56SNHR0QoKClLt2rU1cOBA5eTkFHteISEhioiIKOlyAADgUciFbs1Tc6EuXbrYFbqut0VEROinn34q9jgAnItiFwCneeSRR3ThwgUlJyfrkUceUWpqqqZPn+7weAMGDJDVatWsWbMUGxurl156SQsWLNADDzygWrVqafbs2WrUqJGefvppbdmypdjjLl++XJ07d1ZgYKCWL1+u5cuXq0uXLpKk6dOna8iQIfL399eMGTM0ffp01alTRxs3bnR4HtIfN2qtUqWKkpKStHDhQrVr105Tp07V5MmTbX2GDBmiq1ev6r333rPb9/Lly/rggw/00EMPKSgoyDaH3r17q0qVKpo9e7b++te/at++ferUqVOhm8FevXpVCQkJioqK0t///nc99NBDunz5shISErRt2zb953/+pxYvXqwxY8boyJEjOn/+fKnmCgCAtyIXujFvyoXy8vKUl5enyMjIUo0DoBQMAJTStGnTjCQzYsQIu/Z+/fqZatWqGWOMOXr0qJFkli1bVmh/SWbatGmFxhszZoyt7erVq6Z27drGYrGYWbNm2drPnTtnKlWqZBITE0sUc2JioqlcubJd28GDB42Pj4/p16+fuXbtmt17Vqu12GMvW7bMSDJHjx61tV28eLFQv8cff9wEBwebS5cu2dri4uJMbGysXb/Vq1cbSWbTpk3GGGMuXLhgwsPDzejRo+36ZWVlmbCwMLv2xMREI8lMnjzZru/u3buNJLNq1apiz+tWVq1aZRcnAADeglzInrfmQtfNnDnTSDJpaWlOHxtA8XBlFwCnGTt2rN3rzp0768yZM8rNzXVovFGjRtn+7evrq/bt28sYo5EjR9raw8PD1bRpUx05csSxoP9kzZo1slqtmjp1aqHL0S0WS6nGrlSpku3fFy5c0G+//abOnTvr4sWL+vnnn23vDR06VN9++60OHz5sa1uxYoXq1Kmj+Ph4SdLnn3+u8+fPa9CgQfrtt99sm6+vr2JjY7Vp06ZCx3/iiSfsXoeFhUmSPvvsM9vXKwAAQOmQC92Yt+RCW7Zs0fTp0/XII4/o/vvvd9q4AEqGYhcAp6lbt67d66pVq0qSzp0755TxwsLCFBQUVOiS8LCwMIeP8WeHDx+Wj4+PmjVrVuqx/tWPP/6ofv36KSwsTKGhoapevboGDx4sSXb3hRgwYIACAwO1YsUK23sff/yxHnvsMVuSefDgQUnS/fffr+rVq9ttGzZs0KlTp+yO7efnp9q1a9u1NWjQQElJSXrzzTcVGRmphIQELV68uET3qAAAAPbIhW7MG3Khn3/+Wf369VOLFi305ptvOjwOgNLjaYwAnMbX17fIdmPMDf8a+OebkhZnvJsdo7w6f/684uPjFRoaqhkzZqhhw4YKCgrSrl279Oyzz8pqtdr6Vq1aVf/2b/+mFStWaOrUqfrggw9UUFBgSwYl2fovX768yMdZ+/nZn9oDAwML/XVWkubOnathw4Zp7dq12rBhg5588kklJydr27ZthRJCAABwa+RCRfOGXOjYsWPq3r27wsLC9MknnygkJKRE+wNwLopdAMrE9b9s/usNP3/99Vc3RFO0hg0bymq1at++fWrTpo3Txt28ebPOnDmj1atX227+KklHjx4tsv/QoUPVp08ffffdd1qxYoXuvPNONW/e3C5OSYqKilK3bt1KFVvLli3VsmVLvfDCC9q6davuuecepaSk8JhsAACcjFzIc3OhM2fOqHv37iooKFBaWppq1qxZqpgAlB5fYwRQJkJDQxUZGVnoSUGvvvqqmyIqrG/fvvLx8dGMGTPs/sIole6vpdf/AvvnMS5fvnzDuffs2VORkZGaPXu2vvzyS7u/ZEpSQkKCQkND9fLLL+vKlSuF9i/qcd3/Kjc3V1evXrVra9mypXx8fFRQUHDL/QEAQMmQC3lmLpSfn69evXrpxIkT+uSTT9S4ceNi7wvAdbiyC0CZGTVqlGbNmqVRo0apffv22rJliw4cOODusGwaNWqk559/XjNnzlTnzp3Vv39/BQYG6rvvvlNMTIySk5MdGrdjx46qWrWqEhMT9eSTT8pisWj58uU3TBr9/f01cOBALVq0SL6+vho0aJDd+6GhoXrttdc0ZMgQtW3bVgMHDlT16tWVkZGhdevW6Z577tGiRYtuGtPGjRs1fvx4Pfzww2rSpImuXr2q5cuXy9fXVw899FCJ5nf9L58//vijpD++UvD1119Lkl544YUSjQUAgCcjF/K8XOixxx7T9u3bNWLECP3000/66aefbO9VqVJFffv2LfZYAJyHYheAMjN16lSdPn1aH3zwgd5//3317NlT69evV1RUlLtDs5kxY4YaNGigV155Rc8//7yCg4PVqlUrDRkyxOExq1Wrpo8//lgTJ07UCy+8oKpVq2rw4MHq2rWrEhISitxn6NChWrRokbp27VrkpfCPPvqoYmJiNGvWLM2ZM0cFBQWqVauWOnfurOHDh98yptatWyshIUH//Oc/deLECQUHB6t169Zav3697r777hLN769//avd66VLl9r+TbELAID/Ry7kebnQnj17JP2R//w5B5KkevXqUewC3MRiyvOdDAHAS33//fdq06aN3n777VIllwAAABURuRCA0uCeXQBQDv3jH/9QlSpV1L9/f3eHAgAAUObIhQCUBl9jBOBRTp8+fdNHeAcEBCgiIsKhsfPy8pSXl3fTPtWrV7/hI8GL45///Kf27dunN954Q+PHj1flypUdHqs0cnJy9Pvvv9+0T1GP+gYAAO5FLuQc5EJAxcbXGAF4lPr169/0Ed7x8fHavHmzQ2O/+OKLmj59+k37HD16VPXr13dofOmP+LOzs5WQkKDly5crJCTE4bFKY9iwYXrrrbdu2odfHwAAlD/kQs5BLgRUbBS7AHiUb7755qZ/hatataratWvn0NhHjhzRkSNHbtqnU6dOCgoKcmj88mTfvn06efLkTft069atjKIBAADFRS7kHORCQMVGsQsAAAAAAAAegxvUAwAAAAAAwGN43Q3qrVarTp48qZCQEFksFneHAwAAUGzGGF24cEExMTHy8XHsb5bkQgAAoKIqbi7kdcWukydPqk6dOu4OAwAAwGHHjh1T7dq1HdqXXAgAAFR0t8qFvK7Ydf1pHseOHVNoaKibowEAACi+3Nxc1alTp1RPJyMXAgAAFVVxcyGvK3Zdv1w/NDSUBA8AAFRIpfn6IbkQAACo6G6VC7n1BvXJycm66667FBISoqioKPXt21f79++/5X6rVq3S7bffrqCgILVs2VKffPJJGUQLAAAAAACA8s6txa4vv/xS48aN07Zt2/T555/rypUr6t69u/Lz82+4z9atWzVo0CCNHDlSu3fvVt++fdW3b1/t3bu3DCMHAAAAAABAeWQxxhh3B3Hd6dOnFRUVpS+//FJdunQpss+AAQOUn5+vjz/+2NZ29913q02bNkpJSbnlMXJzcxUWFqacnBwu3QcAABWKM/IYciEAAFBRFTePKVf37MrJyZEkRURE3LBPenq6kpKS7NoSEhK0Zs2aIvsXFBSooKDA9jo3N7f0gQIAAFQQ5EIAAMDblJtil9Vq1V/+8hfdc889atGixQ37ZWVlqUaNGnZtNWrUUFZWVpH9k5OTNX36dKfGWlz1J68rVr9fZvV2cSTwVN74GSvunKXyP29PmguA8suduRAA3ExJciFnc3Zu5c683F3HJpdFeebWe3b92bhx47R3716tXLnSqeNOmTJFOTk5tu3YsWNOHR8AAKA8IxcCAADeplxc2TV+/Hh9/PHH2rJli2rXrn3TvtHR0crOzrZry87OVnR0dJH9AwMDFRgY6LRYAQAAKhJyIQAA4G3cemWXMUbjx4/X//zP/2jjxo1q0KDBLfeJi4tTWlqaXdvnn3+uuLg4V4UJAAAAAACACsKtV3aNGzdO7777rtauXauQkBDbfbfCwsJUqVIlSdLQoUNVq1YtJScnS5ImTJig+Ph4zZ07V71799bKlSu1Y8cOvfHGG26bBwAAAAAAAMoHt17Z9dprryknJ0f33nuvatasadvee+89W5+MjAxlZmbaXnfs2FHvvvuu3njjDbVu3VoffPCB1qxZc9Ob2gMAAAAAAMA7uPXKLmPMLfts3ry5UNvDDz+shx9+2AURAQAAAAAAoCIrN09jBAAAAAAAAEqLYhcAAAAAAAA8BsUuAAAAAAAAeAyKXQAAAAAAAPAYFLsAAAAAAADgMSh2AQAAAAAAwGNQ7AIAAAAAAIDHoNgFAAAAAAAAj0GxCwAAAAAAAB6DYhcAAAAAAAA8BsUuAAAAAAAAeAyKXQAAAAAAAPAYFLsAAAAAAADgMSh2AQAAAAAAwGNQ7AIAAAAAAIDHoNgFAAAAAAAAj0GxCwAAAAAAAB6DYhcAAAAAAAA8BsUuAAAAAAAAeAyKXQAAAAAAAPAYFLsAAAAAAADgMSh2AQAAAAAAwGNQ7AIAAAAAAIDHcGuxa8uWLXrwwQcVExMji8WiNWvW3LT/5s2bZbFYCm1ZWVllEzAAAAAAAADKNbcWu/Lz89W6dWstXry4RPvt379fmZmZti0qKspFEQIAAAAAAKAi8XPnwXv27KmePXuWeL+oqCiFh4c7PyAAAAAAAABUaA5d2XXkyBFnx1Eibdq0Uc2aNfXAAw/om2++uWnfgoIC5ebm2m0AAADeglwIAAB4G4eKXY0aNdJ9992nd955R5cuXXJ2TDdUs2ZNpaSk6MMPP9SHH36oOnXq6N5779WuXbtuuE9ycrLCwsJsW506dcosXgAAAHcjFwIAAN7GoWLXrl271KpVKyUlJSk6OlqPP/64tm/f7uzYCmnatKkef/xxtWvXTh07dtTSpUvVsWNHzZ8//4b7TJkyRTk5Obbt2LFjLo8TAACgvCAXAgAA3sahYlebNm20cOFCnTx5UkuXLlVmZqY6deqkFi1aaN68eTp9+rSz47yhDh066NChQzd8PzAwUKGhoXYbAACAtyAXAgAA3qZUT2P08/NT//79tWrVKs2ePVuHDh3S008/rTp16mjo0KHKzMx0Vpw3tGfPHtWsWdPlxwEAAAAAAED5V6qnMe7YsUNLly7VypUrVblyZT399NMaOXKkjh8/runTp6tPnz43/XpjXl6e3VVZR48e1Z49exQREaG6detqypQpOnHihN5++21J0oIFC9SgQQM1b95cly5d0ptvvqmNGzdqw4YNpZkGAAAAAAAAPIRDxa558+Zp2bJl2r9/v3r16qW3335bvXr1ko/PHxeKNWjQQKmpqapfv/5Nx9mxY4fuu+8+2+ukpCRJUmJiolJTU5WZmamMjAzb+5cvX9bEiRN14sQJBQcHq1WrVvriiy/sxgAAAAAAAID3cqjY9dprr2nEiBEaNmzYDb9CGBUVpSVLltx0nHvvvVfGmBu+n5qaavf6mWee0TPPPFPieAEAAAAAAOAdHCp2HTx48JZ9AgIClJiY6MjwAAAAAAAAgEMcukH9smXLtGrVqkLtq1at0ltvvVXqoAAAAAAAAABHOFTsSk5OVmRkZKH2qKgovfzyy6UOCgAAAAAAAHCEQ8WujIwMNWjQoFB7vXr17G4oDwAAAAAAAJQlh4pdUVFR+uGHHwq1f//996pWrVqpgwIAAAAAAAAc4VCxa9CgQXryySe1adMmXbt2TdeuXdPGjRs1YcIEDRw40NkxAgAAAAAAAMXi0NMYZ86cqV9++UVdu3aVn98fQ1itVg0dOpR7dgEAAAAAAMBtHCp2BQQE6L333tPMmTP1/fffq1KlSmrZsqXq1avn7PgAAAAAAACAYnOo2HVdkyZN1KRJE2fFAgAAAAAAAJSKQ8Wua9euKTU1VWlpaTp16pSsVqvd+xs3bnRKcAAAAAAAAEBJOFTsmjBhglJTU9W7d2+1aNFCFovF2XEBAAAAAAAAJeZQsWvlypV6//331atXL2fHAwAAAAAAADjMx5GdAgIC1KhRI2fHAgAAAAAAAJSKQ8WuiRMnauHChTLGODseAAAAAAAAwGEOfY3x66+/1qZNm7R+/Xo1b95c/v7+du+vXr3aKcEBAAAAAAAAJeFQsSs8PFz9+vVzdiwAAAAAAABAqThU7Fq2bJmz4wAAAAAAAABKzaF7dknS1atX9cUXX+j111/XhQsXJEknT55UXl6e04IDAAAAAAAASsKhK7t+/fVX9ejRQxkZGSooKNADDzygkJAQzZ49WwUFBUpJSXF2nAAAAAAAAMAtOXRl14QJE9S+fXudO3dOlSpVsrX369dPaWlpTgsOAAAAAAAAKAmHruz66quvtHXrVgUEBNi1169fXydOnHBKYAAAAAAAAEBJOXRll9Vq1bVr1wq1Hz9+XCEhIaUOCgAAAAAAAHCEQ8Wu7t27a8GCBbbXFotFeXl5mjZtmnr16uWs2AAAAAAAAIAScehrjHPnzlVCQoKaNWumS5cu6dFHH9XBgwcVGRmp//7v/3Z2jAAAAAAAAECxOHRlV+3atfX999/rueee01NPPaU777xTs2bN0u7duxUVFVXscbZs2aIHH3xQMTExslgsWrNmzS332bx5s9q2bavAwEA1atRIqampjkwBAAAAAAAAHsihK7skyc/PT4MHDy7VwfPz89W6dWuNGDFC/fv3v2X/o0ePqnfv3ho7dqxWrFihtLQ0jRo1SjVr1lRCQkKpYgEAAAAAAEDF51Cx6+23377p+0OHDi3WOD179lTPnj2LfdyUlBQ1aNBAc+fOlSTdcccd+vrrrzV//nyKXQAAAAAAAHCs2DVhwgS711euXNHFixcVEBCg4ODgYhe7Sio9PV3dunWza0tISNBf/vKXG+5TUFCggoIC2+vc3FyXxAYAAFAekQsBAABv41Cx69y5c4XaDh48qCeeeEKTJk0qdVA3kpWVpRo1ati11ahRQ7m5ufr9999VqVKlQvskJydr+vTpLoupoqs/eZ1Tx/tlVm+njudOzl4byX3rU9y5lCQ+V6yPu7hifZytvP+sliQ+bzxPFHfOFeGzWBG46/+lJGOWJXflQhV93YDyyp3nOG/krvVxxXHdmWeU91zWk7ji929FzFEdukF9URo3bqxZs2YVuurL3aZMmaKcnBzbduzYMXeHBAAAUGbIhQAAgLdx+Ab1RQ7m56eTJ086c0g70dHRys7OtmvLzs5WaGhokVd1SVJgYKACAwNdFhMAAEB5Ri4EAAC8jUPFro8++sjutTFGmZmZWrRoke655x6nBFaUuLg4ffLJJ3Ztn3/+ueLi4lx2TAAAAAAAAFQcDhW7+vbta/faYrGoevXquv/++21PSiyOvLw8HTp0yPb66NGj2rNnjyIiIlS3bl1NmTJFJ06csD39cezYsVq0aJGeeeYZjRgxQhs3btT777+vdev4njkAAAAAAAAcLHZZrVanHHzHjh267777bK+TkpIkSYmJiUpNTVVmZqYyMjJs7zdo0EDr1q3TU089pYULF6p27dp68803lZCQ4JR4AAAAAAAAULE59Z5dJXXvvffKGHPD91NTU4vcZ/fu3S6MCgAAAAAAABWVQ8Wu61dgFce8efMcOQQAAAAAAABQYg4Vu3bv3q3du3frypUratq0qSTpwIED8vX1Vdu2bW39LBaLc6IEAAAAAAAAisGhYteDDz6okJAQvfXWW6pataok6dy5cxo+fLg6d+6siRMnOjVIAAAAAAAAoDh8HNlp7ty5Sk5OthW6JKlq1ap66aWXSvQ0RgAAAAAAAMCZHCp25ebm6vTp04XaT58+rQsXLpQ6KAAAAAAAAMARDhW7+vXrp+HDh2v16tU6fvy4jh8/rg8//FAjR45U//79nR0jAAAAAAAAUCwO3bMrJSVFTz/9tB599FFduXLlj4H8/DRy5EjNmTPHqQECAAAAAAAAxeVQsSs4OFivvvqq5syZo8OHD0uSGjZsqMqVKzs1OAAAAAAAAKAkHPoa43WZmZnKzMxU48aNVblyZRljnBUXAAAAAAAAUGIOFbvOnDmjrl27qkmTJurVq5cyMzMlSSNHjtTEiROdGiAAAAAAAABQXA4Vu5566in5+/srIyNDwcHBtvYBAwbo008/dVpwAAAAAAAAQEk4dM+uDRs26LPPPlPt2rXt2hs3bqxff/3VKYEBAAAAAAAAJeXQlV35+fl2V3Rdd/bsWQUGBpY6KAAAAAAAAMARDhW7OnfurLffftv22mKxyGq16m9/+5vuu+8+pwUHAAAAAAAAlIRDX2P829/+pq5du2rHjh26fPmynnnmGf344486e/asvvnmG2fHCAAAAAAAABSLQ1d2tWjRQgcOHFCnTp3Up08f5efnq3///tq9e7caNmzo7BgBAAAAAACAYinxlV1XrlxRjx49lJKSoueff94VMQEAAAAAAAAOKfGVXf7+/vrhhx9cEQsAAAAAAABQKg59jXHw4MFasmSJs2MBAAAAAAAASsWhG9RfvXpVS5cu1RdffKF27dqpcuXKdu/PmzfPKcEBAAAAAAAAJVGiYteRI0dUv3597d27V23btpUkHThwwK6PxWJxXnQAAAAAAABACZSo2NW4cWNlZmZq06ZNkqQBAwbov/7rv1SjRg2XBAcAAAAAAACURInu2WWMsXu9fv165efnOzUgAAAAAAAAwFEO3aD+un8tfgEAAAAAAADuVKJil8ViKXRPLmfco2vx4sWqX7++goKCFBsbq+3bt9+wb2pqqi2O61tQUFCpYwAAAAAAAEDFV6J7dhljNGzYMAUGBkqSLl26pLFjxxZ6GuPq1auLPeZ7772npKQkpaSkKDY2VgsWLFBCQoL279+vqKioIvcJDQ3V/v37ba+5KT4AAAAAAACkEha7EhMT7V4PHjy41AHMmzdPo0eP1vDhwyVJKSkpWrdunZYuXarJkycXuY/FYlF0dHSpjw0AAAAAAADPUqJi17Jly5x68MuXL2vnzp2aMmWKrc3Hx0fdunVTenr6DffLy8tTvXr1ZLVa1bZtW7388stq3rx5kX0LCgpUUFBge52bm+u8CQAAAJRz5EIAAMDblOoG9aX122+/6dq1a6pRo4Zde40aNZSVlVXkPk2bNtXSpUu1du1avfPOO7JarerYsaOOHz9eZP/k5GSFhYXZtjp16jh9HgAAAOUVuRAAAPA2bi12OSIuLk5Dhw5VmzZtFB8fr9WrV6t69ep6/fXXi+w/ZcoU5eTk2LZjx46VccQAAADuQy4EAAC8TYm+xuhskZGR8vX1VXZ2tl17dnZ2se/J5e/vrzvvvFOHDh0q8v3AwEDbDfUBAAC8DbkQAADwNm69sisgIEDt2rVTWlqarc1qtSotLU1xcXHFGuPatWv63//9X9WsWdNVYQIAAAAAAKCCcOuVXZKUlJSkxMREtW/fXh06dNCCBQuUn59vezrj0KFDVatWLSUnJ0uSZsyYobvvvluNGjXS+fPnNWfOHP36668aNWqUO6cBAAAAAACAcsDtxa4BAwbo9OnTmjp1qrKystSmTRt9+umntpvWZ2RkyMfn/y9AO3funEaPHq2srCxVrVpV7dq109atW9WsWTN3TQEAAAAAAADlhNuLXZI0fvx4jR8/vsj3Nm/ebPd6/vz5mj9/fhlEBQAAAAAAgIqmwj2NEQAAAAAAALgRil0AAAAAAADwGBS7AAAAAAAA4DEodgEAAAAAAMBjUOwCAAAAAACAx6DYBQAAAAAAAI9BsQsAAAAAAAAeg2IXAAAAAAAAPAbFLgAAAAAAAHgMil0AAAAAAADwGBS7AAAAAAAA4DEodgEAAAAAAMBjUOwCAAAAAACAx6DYBQAAAAAAAI9BsQsAAAAAAAAeg2IXAAAAAAAAPAbFLgAAAAAAAHgMil0AAAAAAADwGBS7AAAAAAAA4DEodgEAAAAAAMBjUOwCAAAAAACAx6DYBQAAAAAAAI9BsQsAAAAAAAAeo1wUuxYvXqz69esrKChIsbGx2r59+037r1q1SrfffruCgoLUsmVLffLJJ2UUKQAAAAAAAMoztxe73nvvPSUlJWnatGnatWuXWrdurYSEBJ06darI/lu3btWgQYM0cuRI7d69W3379lXfvn21d+/eMo4cAAAAAAAA5Y3bi13z5s3T6NGjNXz4cDVr1kwpKSkKDg7W0qVLi+y/cOFC9ejRQ5MmTdIdd9yhmTNnqm3btlq0aFEZRw4AAAAAAIDyxs+dB798+bJ27typKVOm2Np8fHzUrVs3paenF7lPenq6kpKS7NoSEhK0Zs2aIvsXFBSooKDA9jonJ0eSlJubW8rob81acLFY/coilhspbozF5c65OJuz10Zy/vq48//PFevjbMWdj7t+Vt25hu6cizeeJ8r7Z9HTuOv/pSRjOur6+MaYYu/jrlyoPK0b4EnceY7zJO5an4rw/+JJMXojV/z+LU85arFzIeNGJ06cMJLM1q1b7donTZpkOnToUOQ+/v7+5t1337VrW7x4sYmKiiqy/7Rp04wkNjY2NjY2NjaP2Y4dO1bsfItciI2NjY2Njc3TtlvlQm69sqssTJkyxe5KMKvVqrNnz6patWqyWCxlEkNubq7q1KmjY8eOKTQ0tEyO6SlYO8ewbo5h3RzH2jmGdXOMN6+bMUYXLlxQTExMsfcpD7lQWfPmz8h13r4G3j5/iTXw9vlLrIG3z1/yzDUobi7k1mJXZGSkfH19lZ2dbdeenZ2t6OjoIveJjo4uUf/AwEAFBgbatYWHhzsedCmEhoZ6zAesrLF2jmHdHMO6OY61cwzr5hhvXbewsLAS9S9PuVBZ89bPyJ95+xp4+/wl1sDb5y+xBt4+f8nz1qA4uZBbb1AfEBCgdu3aKS0tzdZmtVqVlpamuLi4IveJi4uz6y9Jn3/++Q37AwAAAAAAwHu4/WuMSUlJSkxMVPv27dWhQwctWLBA+fn5Gj58uCRp6NChqlWrlpKTkyVJEyZMUHx8vObOnavevXtr5cqV2rFjh9544w13TgMAAAAAAADlgNuLXQMGDNDp06c1depUZWVlqU2bNvr0009Vo0YNSVJGRoZ8fP7/ArSOHTvq3Xff1QsvvKDnnntOjRs31po1a9SiRQt3TeGWAgMDNW3atEJfIcCtsXaOYd0cw7o5jrVzDOvmGNYNt8JnhDXw9vlLrIG3z19iDbx9/pJ3r4HFmBI8uxoAAAAAAAAox9x6zy4AAAAAAADAmSh2AQAAAAAAwGNQ7AIAAAAAAIDHoNgFAAAAAAAAj0GxCwAAAAAAAB6DYlcxLV68WPXr11dQUJBiY2O1ffv2G/a99957ZbFYCm29e/e29Rk2bFih93v06GE3ztmzZ/XYY48pNDRU4eHhGjlypPLy8lw2R1dwx7rVr1+/UJ9Zs2a5bI6u4Ox1k6SffvpJ//7v/66wsDBVrlxZd911lzIyMmzvX7p0SePGjVO1atVUpUoVPfTQQ8rOznbZHF3BHetW1Dhjx4512RxdxdlrV9T7FotFc+bMsfXhHOfYunnCOU5y/trl5eVp/Pjxql27tipVqqRmzZopJSXFbhxPOM95C1ecz68bO3asLBaLFixYYNde3s5J7liD8nZ+8fb829vzaPJhcluJHJVcsxQMbmnlypUmICDALF261Pz4449m9OjRJjw83GRnZxfZ/8yZMyYzM9O27d271/j6+pply5bZ+iQmJpoePXrY9Tt79qzdOD169DCtW7c227ZtM1999ZVp1KiRGTRokCun6lTuWrd69eqZGTNm2PXJy8tz5VSdyhXrdujQIRMREWEmTZpkdu3aZQ4dOmTWrl1rN+bYsWNNnTp1TFpamtmxY4e5++67TceOHV09Xadx17rFx8eb0aNH242Vk5Pj6uk6lSvW7s/vZ2ZmmqVLlxqLxWIOHz5s68M5zrF1q+jnOGNcs3ajR482DRs2NJs2bTJHjx41r7/+uvH19TVr16619ano5zlv4YrPx3WrV682rVu3NjExMWb+/Pl275Wnc5K71qA8nV+8Pf/29jyafJjc1hhyVHLN0qHYVQwdOnQw48aNs72+du2aiYmJMcnJycXaf/78+SYkJMTuA5KYmGj69Olzw3327dtnJJnvvvvO1rZ+/XpjsVjMiRMnSj4JN3DHuhnzxw/nvyZvFYkr1m3AgAFm8ODBN9zn/Pnzxt/f36xatcrW9tNPPxlJJj093YFZlD13rJsxfyQEEyZMcCjm8sIVa/ev+vTpY+6//37ba85xjq2bMRX/HGeMa9auefPmZsaMGXb92rZta55//nljjGec57yFq362jh8/bmrVqmX27t1b6OeovJ2T3LEGxpSv84u359/enkeTD5PbGkOOSq5ZOnyN8RYuX76snTt3qlu3brY2Hx8fdevWTenp6cUaY8mSJRo4cKAqV65s175582ZFRUWpadOmeuKJJ3TmzBnbe+np6QoPD1f79u1tbd26dZOPj4++/fbbUs7K9dy1btfNmjVL1apV05133qk5c+bo6tWrpZtQGXHFulmtVq1bt05NmjRRQkKCoqKiFBsbqzVr1tj22blzp65cuWJ33Ntvv11169Yt9nHdyV3rdt2KFSsUGRmpFi1aaMqUKbp48aJT5lUWXPmzel12drbWrVunkSNH2to4xzm2btdV1HOc5Lq169ixoz766COdOHFCxhht2rRJBw4cUPfu3SVV/POct3DV58NqtWrIkCGaNGmSmjdvXmif8nROctcaXFcezi/enn97ex5NPkxuK5GjkmuWnp+7AyjvfvvtN127dk01atSwa69Ro4Z+/vnnW+6/fft27d27V0uWLLFr79Gjh/r3768GDRro8OHDeu6559SzZ0+lp6fL19dXWVlZioqKstvHz89PERERysrKKv3EXMxd6yZJTz75pNq2bauIiAht3bpVU6ZMUWZmpubNm+e8CbqIK9bt1KlTysvL06xZs/TSSy9p9uzZ+vTTT9W/f39t2rRJ8fHxysrKUkBAgMLDwwsd11s/b8VZN0l69NFHVa9ePcXExOiHH37Qs88+q/3792v16tXOnaSLuOpn9c/eeusthYSEqH///rY2znGOrZtUsc9xkuvW7pVXXtGYMWNUu3Zt+fn5ycfHR//4xz/UpUsXSarw5zlv4arPx+zZs+Xn56cnn3yyyP3K0znJXWsglZ/zi7fn396eR5MPk9tK5KjkmqVHscvFlixZopYtW6pDhw527QMHDrT9u2XLlmrVqpUaNmyozZs3q2vXrmUdZrlTmnVLSkqy9WnVqpUCAgL0+OOPKzk5WYGBgWUzATcpat2sVqskqU+fPnrqqackSW3atNHWrVuVkpJi+8XmzUqzbmPGjLHt07JlS9WsWVNdu3bV4cOH1bBhwzKchXvc6Gf1z5YuXarHHntMQUFBZRhZ+VaadfPmc5x047V75ZVXtG3bNn300UeqV6+etmzZonHjxikmJsbur6LwbEV9Pnbu3KmFCxdq165dslgsboyubJRmDTzl/OLt+be359Hkw+S2EjkquSZPY7ylyMhI+fr6FnoKR3Z2tqKjo2+6b35+vlauXFnkZYH/6rbbblNkZKQOHTokSYqOjtapU6fs+ly9elVnz5695XHLA3etW1FiY2N19epV/fLLL8WK3Z1csW6RkZHy8/NTs2bN7NrvuOMO25NXoqOjdfnyZZ0/f77Exy0P3LVuRYmNjZWkm34myxNX/6x+9dVX2r9/v0aNGmXXzjnOsXUrSkU6x0muWbvff/9dzz33nObNm6cHH3xQrVq10vjx4zVgwAD9/e9/l1Txz3PewhWfj6+++kqnTp1S3bp15efnJz8/P/3666+aOHGi6tevL6l8nZPctQZFcdf5xdvzb2/Po8mHyW0lclRyzdKj2HULAQEBateundLS0mxtVqtVaWlpiouLu+m+q1atUkFBgQYPHnzL4xw/flxnzpxRzZo1JUlxcXE6f/68du7caeuzceNGWa1W2wmnPHPXuhVlz5498vHxKXQ5annkinULCAjQXXfdpf3799u1HzhwQPXq1ZMktWvXTv7+/nbH3b9/vzIyMm553PLAXetWlD179kjSTT+T5Ymrf1aXLFmidu3aqXXr1nbtnOMcW7eiVKRznOSatbty5YquXLkiHx/7tMbX19f2l+yKfp7zFq74fAwZMkQ//PCD9uzZY9tiYmI0adIkffbZZ5LK1znJXWtQFHedX7w9//b2PJp8mNxWIkcl13QCd98hvyJYuXKlCQwMNKmpqWbfvn1mzJgxJjw83GRlZRljjBkyZIiZPHlyof06depkBgwYUKj9woUL5umnnzbp6enm6NGj5osvvjBt27Y1jRs3NpcuXbL169Gjh7nzzjvNt99+a77++mvTuHFjtz0C2xHuWLetW7ea+fPnmz179pjDhw+bd955x1SvXt0MHTrUtZN1ImevmzF/PGbc39/fvPHGG+bgwYPmlVdeMb6+vuarr76y9Rk7dqypW7eu2bhxo9mxY4eJi4szcXFxrpmkC7hj3Q4dOmRmzJhhduzYYY4ePWrWrl1rbrvtNtOlSxfXTdQFXLF2xhiTk5NjgoODzWuvvVbk+5zjinazdfOEc5wxrlm7+Ph407x5c7Np0yZz5MgRs2zZMhMUFGReffVVW5+Kfp7zFq762fqzop40VZ7OSe5Yg/J2fvH2/Nvb82jyYXJbY8hRyTVLh2JXMb3yyiumbt26JiAgwHTo0MFs27bN9l58fLxJTEy06//zzz8bSWbDhg2Fxrp48aLp3r27qV69uvH39zf16tUzo0ePtn1orztz5owZNGiQqVKligkNDTXDhw83Fy5ccMn8XKWs123nzp0mNjbWhIWFmaCgIHPHHXeYl19+2S6JqQicuW7XLVmyxDRq1MgEBQWZ1q1bmzVr1ti9//vvv5v/+I//MFWrVjXBwcGmX79+JjMz06nzcrWyXreMjAzTpUsXExERYQIDA02jRo3MpEmTTE5OjtPn5mquWLvXX3/dVKpUyZw/f77I9znHFe1m6+Yp5zhjnL92mZmZZtiwYSYmJsYEBQWZpk2bmrlz5xqr1Wrr4wnnOW/hip+tPyuq2FXezkllvQbl8fzi7fm3t+fR5MPktsaQo5JrOs5ijDFuuKAMAAAAAAAAcDru2QUAAAAAAACPQbELAAAAAAAAHoNiFwAAAAAAADwGxS4AAAAAAAB4DIpdAAAAAAAA8BgUuwAAAAAAAOAxKHYBAAAAAADAY1DsAgAAAAAAgMeg2AUAAAAAAACPQbELAAAAAAAAHoNiFwAAAAAAADzG/wHRZ8Jzg1pZvwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_desc = sorted_results[\"mean_accuracy\"].astype(\"float32\").describe()\n",
    "xlimit_range = [\n",
    "    accuracy_desc[\"min\"] - accuracy_desc[\"std\"],\n",
    "    accuracy_desc[\"max\"] + accuracy_desc[\"std\"],\n",
    "]\n",
    "for hperparameter_name in turning_parameters:\n",
    "    parameter_group = sorted_results.groupby(hperparameter_name)\n",
    "    fix, axs = pyplot.subplots(\n",
    "        1,\n",
    "        len(parameter_group),\n",
    "        layout=\"constrained\",\n",
    "        sharex=False,\n",
    "        sharey=True,\n",
    "        figsize=(12, 2),\n",
    "    )\n",
    "    for i, g in enumerate(parameter_group):\n",
    "        g[1][\"mean_accuracy\"].astype(\"float32\").plot(\n",
    "            kind=\"hist\", bins=50, subplots=True, sharex=False, sharey=True, ax=axs[i]\n",
    "        )\n",
    "        axs[i].set_title(f\"{hperparameter_name}_{g[0]}\")\n",
    "\n",
    "pyplot.xlim(xlimit_range)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                           4\n",
      "mean_accuracy       0.7823684782608694\n",
      "trial_id                   d8976_00004\n",
      "return_period                        5\n",
      "seq_len                              5\n",
      "lr                                0.01\n",
      "momentum           0.11646759543664197\n",
      "optim_type                           1\n",
      "num_layers                           4\n",
      "hidden_size                         64\n",
      "num_fc_layers                        1\n",
      "activation_type                      2\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results = pd.read_csv(sorted_results_file, dtype=\"str\")\n",
    "best_config = sorted_results.loc[0]\n",
    "print(best_config)\n",
    "# id_str_of_best = f\"5_5_0.01_{best_config.momentum}_{best_config.optim_type}_{best_config.num_layers}_{best_config.hidden_size}_{best_config.num_fc_layers}_{best_config.activation_type}\"\n",
    "# best_model_name = f\"/mnt/AIWorkSpace/work/fin-ml/runs/{_TARGET_STK}/{time_str}/{id_str_of_best}.pt\"\n",
    "# print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.87\n",
      "Test Accuracy: 0.76500\n",
      "Train F1: 0.00\n",
      "Test F1: 0.00000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "model, config = load_model(f\"{log_dir_base}/{task_name}.pt\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader, features_size = prepare_dataloader(config[\"return_period\"])\n",
    "model.eval()\n",
    "\n",
    "(trainAccuracy, trainF1) = eval_dl_method(model, train_loader, device=device)\n",
    "(testAccuracy, testF1) = eval_dl_method(model, test_loader, device=device)\n",
    "print(f\"Train Accuracy: {trainAccuracy:.2f}\\nTest Accuracy: {testAccuracy:.5f}\")\n",
    "print(f\"Train F1: {trainF1:.2f}\\nTest F1: {testF1:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
