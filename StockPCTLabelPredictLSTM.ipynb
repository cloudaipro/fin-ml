{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockPCTLabelPredictLSTM\n",
      "/mnt/AIWorkSpace/work/fin-ml/data/\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%run 'nb_utils.ipynb'\n",
    "task_name = get_filename_of_ipynb()\n",
    "print(task_name)\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/{task_name}'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')\n",
    "\n",
    "return_period = 5\n",
    "pct_threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 14:52:07,660\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a76182a0a19452e8408190c5e47c8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.8.18</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.9.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.18', ray_version='2.9.1', ray_commit='cfbf98c315cfb2710c56039a3c96477d196de049', protocol_version=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read /mnt/AIWorkSpace/work/fin-ml/data/AAPL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MSFT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMZN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NVDA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOGL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TSLA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/META.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOG.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADBE.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NFLX.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CSCO.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTC.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTU.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CMCSA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TXN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMAT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADSK.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMD.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/QCOM.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MU.csv completely!\n",
      "[              Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  19.846  19.894  19.715  19.755     17.319  234684800\n",
      "2014-01-03  19.745  19.775  19.301  19.321     16.938  392467600\n",
      "2014-01-06  19.195  19.529  19.057  19.426     17.031  412610800\n",
      "2014-01-07  19.440  19.499  19.211  19.287     16.909  317209200\n",
      "2014-01-08  19.243  19.484  19.239  19.409     17.016  258529600\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 195.180 195.410 192.970 193.600    193.600   37122800\n",
      "2023-12-26 193.610 193.890 192.830 193.050    193.050   28919300\n",
      "2023-12-27 192.490 193.500 191.090 193.150    193.150   48087700\n",
      "2023-12-28 194.140 194.660 193.170 193.580    193.580   34049900\n",
      "2023-12-29 193.900 194.400 191.730 192.530    192.530   42628800\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  37.350  37.400  37.100  37.160     31.291  30632200\n",
      "2014-01-03  37.200  37.220  36.600  36.910     31.080  31134800\n",
      "2014-01-06  36.850  36.890  36.110  36.130     30.423  43603700\n",
      "2014-01-07  36.330  36.490  36.210  36.410     30.659  35802800\n",
      "2014-01-08  36.000  36.140  35.580  35.760     30.112  59971700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 373.680 375.180 372.710 374.580    374.580  17091100\n",
      "2023-12-26 375.000 376.940 373.500 374.660    374.660  12673100\n",
      "2023-12-27 373.690 375.060 372.810 374.070    374.070  14905400\n",
      "2023-12-28 375.370 376.460 374.160 375.280    375.280  14327000\n",
      "2023-12-29 376.000 377.160 373.480 376.040    376.040  18723000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  19.940  19.968  19.701  19.899     19.899  42756000\n",
      "2014-01-03  19.914  20.135  19.811  19.822     19.822  44204000\n",
      "2014-01-06  19.792  19.850  19.421  19.681     19.681  63412000\n",
      "2014-01-07  19.752  19.924  19.715  19.902     19.902  38320000\n",
      "2014-01-08  19.924  20.150  19.802  20.096     20.096  46330000\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 153.770 154.350 152.710 153.420    153.420  29480100\n",
      "2023-12-26 153.560 153.980 153.030 153.410    153.410  25067200\n",
      "2023-12-27 153.560 154.780 153.120 153.340    153.340  31434700\n",
      "2023-12-28 153.720 154.080 152.950 153.380    153.380  27057000\n",
      "2023-12-29 153.100 153.890 151.030 151.940    151.940  39789000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.980   3.995   3.930   3.965      3.741  26009200\n",
      "2014-01-03   3.973   3.980   3.905   3.918      3.696  25933200\n",
      "2014-01-06   3.957   4.000   3.920   3.970      3.745  40949200\n",
      "2014-01-07   4.010   4.050   3.983   4.035      3.807  33328800\n",
      "2014-01-08   4.050   4.110   4.035   4.090      3.859  30819200\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 491.950 493.830 484.670 488.300    488.300  25213900\n",
      "2023-12-26 489.680 496.000 489.600 492.790    492.790  24420000\n",
      "2023-12-27 495.110 496.800 490.850 494.170    494.170  23364800\n",
      "2023-12-28 496.430 498.840 494.120 495.220    495.220  24658700\n",
      "2023-12-29 498.130 499.970 487.510 495.220    495.220  38869000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.914  27.972  27.734  27.856     27.856   72783144\n",
      "2014-01-03  27.903  27.951  27.651  27.653     27.653   66601332\n",
      "2014-01-06  27.853  27.999  27.689  27.961     27.961   70701228\n",
      "2014-01-07  28.153  28.521  28.057  28.500     28.500  102001896\n",
      "2014-01-08  28.679  28.712  28.361  28.559     28.559   89610300\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 140.770 141.990 140.710 141.490    141.490   26514600\n",
      "2023-12-26 141.590 142.680 141.190 141.520    141.520   16780300\n",
      "2023-12-27 141.590 142.080 139.890 140.370    140.370   19628600\n",
      "2023-12-28 140.780 141.140 139.750 140.230    140.230   16045700\n",
      "2023-12-29 139.630 140.360 138.780 139.690    139.690   18727200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02   9.987  10.165   9.770  10.007     10.007   92826000\n",
      "2014-01-03  10.000  10.146   9.907   9.971      9.971   70425000\n",
      "2014-01-06  10.000  10.027   9.683   9.800      9.800   80416500\n",
      "2014-01-07   9.841  10.027   9.683   9.957      9.957   75511500\n",
      "2014-01-08   9.923  10.247   9.917  10.085     10.085   92448000\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 256.760 258.220 251.370 252.540    252.540   93249800\n",
      "2023-12-26 254.490 257.970 252.910 256.610    256.610   86892400\n",
      "2023-12-27 258.350 263.340 257.520 261.440    261.440  106494400\n",
      "2023-12-28 263.660 265.130 252.710 253.180    253.180  113619900\n",
      "2023-12-29 255.100 255.190 247.430 248.480    248.480  100615300\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  54.830  55.220  54.190  54.710     54.710  43195500\n",
      "2014-01-03  55.020  55.650  54.530  54.560     54.560  38246200\n",
      "2014-01-06  54.420  57.260  54.050  57.200     57.200  68852600\n",
      "2014-01-07  57.700  58.550  57.220  57.920     57.920  77207400\n",
      "2014-01-08  57.600  58.410  57.230  58.230     58.230  56682400\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 355.580 357.200 351.220 353.390    353.390  11764200\n",
      "2023-12-26 354.990 356.980 353.450 354.830    354.830   9898600\n",
      "2023-12-27 356.070 359.000 355.310 357.830    357.830  13207900\n",
      "2023-12-28 359.700 361.900 357.810 358.320    358.320  11798800\n",
      "2023-12-29 358.990 360.000 351.820 353.960    353.960  14980500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.782  27.839  27.603  27.724     27.724   73129082\n",
      "2014-01-03  27.771  27.819  27.520  27.522     27.522   66917888\n",
      "2014-01-06  27.721  27.867  27.558  27.829     27.829   71037271\n",
      "2014-01-07  28.020  28.386  27.924  28.365     28.365  102486711\n",
      "2014-01-08  28.543  28.576  28.226  28.424     28.424   90036218\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 142.130 143.250 142.055 142.720    142.720   18494700\n",
      "2023-12-26 142.980 143.945 142.500 142.820    142.820   11170100\n",
      "2023-12-27 142.830 143.320 141.051 141.440    141.440   17288400\n",
      "2023-12-28 141.850 142.270 140.828 141.280    141.280   12192500\n",
      "2023-12-29 140.680 141.435 139.900 140.930    140.930   14872700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  59.060  59.530  58.940  59.290     59.290  2745900\n",
      "2014-01-03  59.190  59.690  59.110  59.160     59.160  1589000\n",
      "2014-01-06  58.060  58.770  58.010  58.120     58.120  3753600\n",
      "2014-01-07  58.260  59.050  58.060  58.970     58.970  2963600\n",
      "2014-01-08  59.120  59.280  58.460  58.900     58.900  3456000\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 600.800 601.860 596.000 598.750    598.750  1659800\n",
      "2023-12-26 598.920 601.690 596.500 598.260    598.260  1595100\n",
      "2023-12-27 598.600 599.790 593.710 596.080    596.080  1394900\n",
      "2023-12-28 597.440 599.040 593.630 595.520    595.520  1702600\n",
      "2023-12-29 596.090 600.750 592.940 596.600    596.600  1893900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  52.401  52.511  51.543  51.831     51.831  12325600\n",
      "2014-01-03  52.000  52.496  51.843  51.871     51.871  10817100\n",
      "2014-01-06  51.890  52.044  50.476  51.367     51.367  15501500\n",
      "2014-01-07  49.684  49.699  48.153  48.500     48.500  36167600\n",
      "2014-01-08  48.104  49.426  48.074  48.713     48.713  20001100\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 494.000 496.020 485.450 486.760    486.760   2701100\n",
      "2023-12-26 489.390 491.480 486.380 491.190    491.190   2034500\n",
      "2023-12-27 491.240 494.020 489.250 491.790    491.790   2561300\n",
      "2023-12-28 492.000 492.890 489.070 490.510    490.510   1710500\n",
      "2023-12-29 490.370 492.230 481.940 486.880    486.880   2739500\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 22.170 22.290 21.910 22.000     16.095  44377000\n",
      "2014-01-03 22.090 22.120 21.830 21.980     16.080  36328200\n",
      "2014-01-06 21.960 22.230 21.930 22.010     16.102  34150300\n",
      "2014-01-07 22.260 22.410 22.150 22.310     16.322  37368800\n",
      "2014-01-08 22.290 22.360 22.150 22.290     16.307  38362700\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 49.840 50.390 49.840 50.090     49.703  12900700\n",
      "2023-12-26 50.110 50.400 50.050 50.280     49.892   9721200\n",
      "2023-12-27 50.300 50.560 50.280 50.440     50.051  10414300\n",
      "2023-12-28 50.580 50.630 50.420 50.480     50.090   8549900\n",
      "2023-12-29 50.450 50.590 50.220 50.520     50.130  12491200\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.780 25.820 25.470 25.790     19.436  31833300\n",
      "2014-01-03 25.860 25.900 25.600 25.780     19.428  27796700\n",
      "2014-01-06 25.770 25.790 25.450 25.460     19.187  28682300\n",
      "2014-01-07 25.540 25.730 25.470 25.590     19.285  19665100\n",
      "2014-01-08 25.640 25.710 25.300 25.430     19.164  29680500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 47.250 48.160 47.200 48.000     48.000  30053700\n",
      "2023-12-26 48.920 50.520 48.710 50.500     50.500  60287400\n",
      "2023-12-27 50.630 51.280 50.190 50.760     50.760  52148000\n",
      "2023-12-28 50.810 50.870 50.160 50.390     50.390  27705200\n",
      "2023-12-29 50.300 50.570 49.770 50.250     50.250  29266500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  76.110  76.160  75.400  75.940     69.476  1355700\n",
      "2014-01-03  75.750  76.380  75.530  75.800     69.348   998800\n",
      "2014-01-06  75.800  76.000  75.470  75.740     69.293  1268700\n",
      "2014-01-07  75.880  77.350  75.740  77.060     70.501  1551000\n",
      "2014-01-08  76.890  77.050  76.010  76.440     70.107  2437600\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 622.830 625.150 617.680 624.070    623.131   820800\n",
      "2023-12-26 625.170 628.330 622.730 624.850    623.910   638300\n",
      "2023-12-27 623.990 629.800 622.260 629.120    628.174   734400\n",
      "2023-12-28 630.740 631.070 627.180 628.020    627.075   680700\n",
      "2023-12-29 628.020 630.830 622.460 625.030    624.090   724300\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.900 25.950 25.635 25.725     20.931  19522400\n",
      "2014-01-03 25.815 25.855 25.425 25.535     20.777  13371400\n",
      "2014-01-06 25.565 25.810 25.335 25.510     20.756  17987800\n",
      "2014-01-07 25.670 26.600 25.555 26.415     21.493  37161400\n",
      "2014-01-08 26.345 26.725 26.260 26.375     21.460  29731600\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 44.130 44.620 43.810 44.000     43.709  11893900\n",
      "2023-12-26 44.000 44.070 43.500 43.930     43.639   9624300\n",
      "2023-12-27 43.900 44.170 43.710 43.990     43.699   9253800\n",
      "2023-12-28 43.970 44.410 43.890 44.120     43.828   9023400\n",
      "2023-12-29 44.090 44.140 43.560 43.850     43.560  13694900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  43.440  43.500  42.880  43.100     32.954  6959200\n",
      "2014-01-03  43.120  43.460  42.970  43.290     33.100  4693300\n",
      "2014-01-06  43.250  43.280  42.850  42.930     32.824  4446300\n",
      "2014-01-07  42.980  43.110  42.640  42.700     32.648  5078900\n",
      "2014-01-08  42.960  43.320  42.620  43.290     33.100  6353500\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 167.260 168.920 166.820 168.240    168.240  3492400\n",
      "2023-12-26 168.940 171.530 168.450 170.810    170.810  3202200\n",
      "2023-12-27 171.220 171.620 170.330 171.230    171.230  3264900\n",
      "2023-12-28 172.000 172.310 170.710 171.720    171.720  3023000\n",
      "2023-12-29 171.540 171.700 169.920 170.460    170.460  2920600\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  17.690  17.690  17.370  17.550     15.280  7785900\n",
      "2014-01-03  17.540  17.700  17.470  17.510     15.245  6773000\n",
      "2014-01-06  17.500  17.510  17.220  17.290     15.053  9975500\n",
      "2014-01-07  17.370  17.430  17.260  17.370     15.123  8133200\n",
      "2014-01-08  17.400  17.450  17.180  17.420     15.166  8026100\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 161.600 163.000 160.840 162.050    162.050  2770600\n",
      "2023-12-26 162.300 164.970 162.100 164.280    164.280  2520500\n",
      "2023-12-27 164.540 164.990 163.530 164.210    164.210  3319600\n",
      "2023-12-28 165.000 165.010 162.850 163.120    163.120  2909700\n",
      "2023-12-29 163.110 163.560 160.700 162.070    162.070  2980700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  49.330  49.740  48.880  49.250     49.250  2488000\n",
      "2014-01-03  49.110  49.500  48.780  48.900     48.900  1934200\n",
      "2014-01-06  48.980  49.290  48.290  48.550     48.550  1856500\n",
      "2014-01-07  48.890  50.100  48.630  49.680     49.680  2002500\n",
      "2014-01-08  49.500  50.550  49.050  50.240     50.240  2047400\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 243.740 244.030 240.310 242.760    242.760   719400\n",
      "2023-12-26 242.490 245.360 241.960 245.070    245.070   595000\n",
      "2023-12-27 245.360 245.880 244.380 245.110    245.110   771900\n",
      "2023-12-28 245.630 245.850 244.020 244.910    244.910   537200\n",
      "2023-12-29 243.720 245.400 242.790 243.480    243.480   721400\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.850   3.980   3.840   3.950      3.950  20548400\n",
      "2014-01-03   3.980   4.000   3.880   4.000      4.000  22887200\n",
      "2014-01-06   4.010   4.180   3.990   4.130      4.130  42398300\n",
      "2014-01-07   4.190   4.250   4.110   4.180      4.180  42932100\n",
      "2014-01-08   4.230   4.260   4.140   4.180      4.180  30678700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 140.480 140.700 138.310 139.600    139.600  35370400\n",
      "2023-12-26 140.070 143.850 139.920 143.410    143.410  47157400\n",
      "2023-12-27 144.720 146.250 143.180 146.070    146.070  49033400\n",
      "2023-12-28 146.800 150.410 145.950 148.760    148.760  63800700\n",
      "2023-12-29 149.500 151.050 147.200 147.410    147.410  62028200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  73.610  73.770  73.260  73.320     54.740  10110200\n",
      "2014-01-03  73.330  73.480  72.440  72.890     54.419   7970400\n",
      "2014-01-06  73.080  73.200  72.550  72.700     54.277   7696200\n",
      "2014-01-07  72.800  73.310  72.600  73.240     54.680   5902700\n",
      "2014-01-08  73.150  73.680  72.680  73.680     55.008   8976900\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 143.250 144.400 142.750 143.490    143.490   4658300\n",
      "2023-12-26 144.170 146.050 143.960 145.460    145.460   4381200\n",
      "2023-12-27 145.850 146.220 145.040 145.720    145.720   4470300\n",
      "2023-12-28 146.180 146.890 145.730 145.860    145.860   4928800\n",
      "2023-12-29 145.410 145.620 143.790 144.630    144.630   4838400\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 21.680 21.790 21.270 21.660     21.293  26413500\n",
      "2014-01-03 21.200 21.430 20.900 20.970     20.615  34590200\n",
      "2014-01-06 20.970 20.970 20.640 20.670     20.320  38180500\n",
      "2014-01-07 20.890 21.940 20.890 21.730     21.362  67904500\n",
      "2014-01-08 24.200 24.500 23.560 23.870     23.466  93499500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 86.150 87.490 85.620 86.490     86.374  22519000\n",
      "2023-12-26 86.700 87.870 86.430 87.060     86.944  11203900\n",
      "2023-12-27 87.480 87.490 86.220 86.660     86.544   9186300\n",
      "2023-12-28 86.750 86.750 85.840 86.000     85.885   9606200\n",
      "2023-12-29 85.840 86.140 85.030 85.340     85.340   8546000\n",
      "\n",
      "[2516 rows x 6 columns]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "\n",
    "# Loading the data\n",
    "stk_tickers = [\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"AMZN\",\n",
    "    \"NVDA\",\n",
    "    \"GOOGL\",\n",
    "    \"TSLA\",\n",
    "    \"META\",\n",
    "    \"GOOG\",\n",
    "    \"ADBE\",\n",
    "    \"NFLX\",\n",
    "    \"CSCO\",\n",
    "    \"INTC\",\n",
    "    \"INTU\",\n",
    "    \"CMCSA\",\n",
    "    \"TXN\",\n",
    "    \"AMAT\",\n",
    "    \"ADSK\",\n",
    "    \"AMD\",\n",
    "    \"QCOM\",\n",
    "    \"MU\",\n",
    "]\n",
    "\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for stk_symbol in stk_tickers:\n",
    "    stk_file = f\"{data_dir}{stk_symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index(\"Date\")\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([stk_symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {stk_symbol} from yfin and write to {stk_file} completely!\")\n",
    "    ticks_data.append(_stk_data)\n",
    "\n",
    "print(ticks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name:cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device_name = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_name)\n",
    "seq_len = 3\n",
    "validation_size = 0.2\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "shuffle = True\n",
    "print(f\"device_name:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=10),\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=60),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    buy_signal = sma[\"SMA_10\"] > sma[\"SMA_60\"]\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal[\"Signal\"] = (buy_signal).astype(\"int\")\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "\n",
    "# 0: PCT <= -0.05\n",
    "# 1: 0.05 < PCT < -0.05\n",
    "# 2: PCT >= -0.05\n",
    "def gen_pct_label(stk_data, _return_period):\n",
    "    pct_data = stk_data[\"Adj Close\"].pct_change(_return_period)\n",
    "    pct_label = pct_data.apply(\n",
    "        lambda x: 2 if x >= pct_threshold else 0 if x <= -pct_threshold else 1\n",
    "    ).astype(\"int8\")\n",
    "    pct_label.name = \"label\"\n",
    "    return pct_label\n",
    "\n",
    "\n",
    "def gen_analysis_data(stk_data, _return_period):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.adosc(),\n",
    "            stk_data.ta.kvo(),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=10) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=30) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=200) / 100,\n",
    "            stk_data.ta.stoch(k=10) / 100,\n",
    "            stk_data.ta.stoch(k=30) / 100,\n",
    "            stk_data.ta.stoch(k=200) / 100,\n",
    "            gen_buy_sell_signal(stk_data),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data = pd.concat(\n",
    "        [data.dropna().astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "        axis=1,\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, ticks_data_X, ticks_data_Y, seq_len):\n",
    "        self.ticks_data_X = ticks_data_X\n",
    "        self.ticks_data_Y = ticks_data_Y\n",
    "        self.seq_len = seq_len\n",
    "        len_array = [len(d) - self.seq_len + 1 for d in ticks_data_X]\n",
    "        self.idx_boundary = [len_array[0]]\n",
    "\n",
    "        for i in range(1, len(len_array)):\n",
    "            self.idx_boundary.append(len_array[i] + self.idx_boundary[i - 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(f\"len of dataset:{self.idx_boundary[-1]}\")\n",
    "        return self.idx_boundary[-1]  # len(self.X) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for ticks_data_idx in range(len(self.ticks_data_X)):\n",
    "            if self.idx_boundary[ticks_data_idx] > idx:\n",
    "                break\n",
    "        offset = (\n",
    "            idx if ticks_data_idx == 0 else idx - self.idx_boundary[ticks_data_idx - 1]\n",
    "        )\n",
    "        # print(f\"{ticks_data_idx}, {offset}\")\n",
    "        return (\n",
    "            np.array(self.ticks_data_X[ticks_data_idx][offset : offset + self.seq_len]),\n",
    "            int(self.ticks_data_Y[ticks_data_idx].iloc[offset + self.seq_len - 1, :]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_dataloader(_return_period):\n",
    "    ticks_dataset = [gen_analysis_data(d, _return_period) for d in ticks_data]\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for dataset in ticks_dataset:\n",
    "        test_size = int(dataset.shape[0] * validation_size)\n",
    "        # random.seed(42)\n",
    "        test_data_idx = random.sample(range(0, dataset.shape[0]), test_size)\n",
    "        mask = np.full(len(dataset), False)\n",
    "        mask[test_data_idx] = True\n",
    "        train_data = dataset[~mask]\n",
    "        test_data = dataset[mask]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[3:].values]\n",
    "        )\n",
    "        # print(features)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_train_data, ticks_Y_train_data, seq_len),\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_test_data, ticks_Y_test_data, seq_len),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, ticks_X_train_data[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class StockPCTLabelPredictLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \"\"\"\n",
    "            input_size    : The number of expected features in the input x\n",
    "            hidden_size   : The number of features in the hidden state h\n",
    "            num_layers    : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "            bias          : If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            batch_first   : If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "            dropout       : If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            bidirectional : If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size     : If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        layers = []\n",
    "        in_features = self.hidden_size\n",
    "        for i in range(1, num_fc_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if out_features <= 3:\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(\n",
    "                nn.ReLU() if activation_type == 1 else nn.Sigmoid()\n",
    "            ) if activation_type == 2 else nn.Tanh()\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, 3))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange = 0.5\n",
    "            nn.init.uniform_(m.weight, -initrange, initrange)\n",
    "            nn.init.zeros_(m.bias)\n",
    "            # print(f\"{m.in_features},{m.out_features}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        out, (h_out, _) = self.rnn(x, (h_0, c_0))\n",
    "\n",
    "        fc_input = h_out[-1].view(-1, self.hidden_size)\n",
    "        return self.fc(fc_input)\n",
    "\n",
    "\n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        \"epoch_num\": epoch_num,\n",
    "        \"time\": str(datetime.now),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hyper_parameters\": hyper_parameters,\n",
    "    }\n",
    "    print(f\"save model:{file_path}\")\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict[\"hyper_parameters\"]\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=data_dict[\"input_size\"],\n",
    "        hidden_size=int(hyper_parameters[\"hidden_size\"]),\n",
    "        num_layers=int(hyper_parameters[\"num_layers\"]),\n",
    "        num_fc_layers=int(hyper_parameters[\"num_fc_layers\"]),\n",
    "        activation_type=int(hyper_parameters[\"activation_type\"]),\n",
    "    )\n",
    "    model.load_state_dict(data_dict[\"model_state\"])\n",
    "    return model, hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def eval_dl_method(model, dl, device=device):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    model.eval()\n",
    "    y_gt = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    f1 = 0\n",
    "    for x, y in dl:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += y.shape[0]\n",
    "        correct += int((predicted == y).sum())\n",
    "        # f1 += f1_score(y.cpu().detach(), predicted.cpu().detach())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1 / len(dl)\n",
    "    # print(f\"Accuracy: {accuracy:.3f}, F1 score:{f1:.3f}\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "_best_loss = 999.0\n",
    "_best_accuracy = 0.0\n",
    "\n",
    "\n",
    "def do_train(model, optimizer, train_dl, test_dl, id_str, config, writer=None):\n",
    "    global _best_loss\n",
    "    global _best_accuracy\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    # for epoch in tqdm(range(epoch_num)):\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_size = 0\n",
    "        total_correct_size = 0\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_size += y.shape[0]\n",
    "            total_correct_size += int((predicted == y).sum())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            accuracy, f1 = eval_dl_method(model, test_dl)\n",
    "            if accuracy > _best_accuracy:\n",
    "                _best_accuracy = accuracy\n",
    "\n",
    "            if loss.item() < _best_loss:\n",
    "                _best_loss = loss.item()\n",
    "                save_model(model, config, model_name)\n",
    "\n",
    "        total_loss += running_loss / len(train_dl)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "        train.report(\n",
    "            {\n",
    "                \"loss\": running_loss / len(train_dl),\n",
    "                \"train_accuracy\": total_correct_size / total_size,\n",
    "                \"mean_accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "        if writer != None:\n",
    "            writer.add_scalars(\n",
    "                \"Training vs. Validation Loss\",\n",
    "                {\n",
    "                    \"loss\": running_loss / len(train_dl),\n",
    "                    \"train_accuracy\": total_correct_size / total_size,\n",
    "                    \"mean_accuracy\": accuracy,\n",
    "                },\n",
    "                epoch + 1,\n",
    "            )\n",
    "            writer.flush()\n",
    "\n",
    "    return {\n",
    "        \"Train loss\": total_loss / epoch_num,\n",
    "        \"Validation Accuracy\": total_accuracy / epoch_num,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(config, tensorreport=True):\n",
    "    _return_period = config[\"return_period\"]\n",
    "    _seq_len = config[\"seq_len\"]\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    num_fc_layers = config[\"num_fc_layers\"]\n",
    "    activation_type = config[\"activation_type\"]\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    print(id_str)\n",
    "\n",
    "    if tensorreport:\n",
    "        writer = SummaryWriter(f\"{log_dir}/{id_str}\")\n",
    "    train_loader, test_loader, features_size = prepare_dataloader(_return_period)\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=features_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_fc_layers=num_fc_layers,\n",
    "        activation_type=activation_type,\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    )\n",
    "    metric_dict = do_train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        id_str,\n",
    "        config,\n",
    "        writer if tensorreport else None,\n",
    "    )\n",
    "\n",
    "    if tensorreport:\n",
    "        writer.add_hparams(config, metric_dict)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-30_15.02.22\n",
      "5_10_0.01_0.34936886087152874_2_4_32_3_1\n"
     ]
    }
   ],
   "source": [
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "config = {\n",
    "    \"return_period\": 5,\n",
    "    \"seq_len\": 10,\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.34936886087152874,\n",
    "    \"optim_type\": 2,\n",
    "    \"num_layers\": 4,\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_fc_layers\": 3,\n",
    "    \"activation_type\": 1,\n",
    "}\n",
    "epoch_num = 20\n",
    "os.mkdir(log_dir)\n",
    "print(log_dir)\n",
    "train_LSTM(config, tensorreport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seq_len', 'num_layers', 'hidden_size', 'num_fc_layers']\n",
      "Total count of configs = 24\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"return_period\": tune.grid_search([5]),  # [2,3,5,10]\n",
    "    \"seq_len\": tune.grid_search([5, 10]),\n",
    "    \"lr\": tune.grid_search([0.01]),  # [0.001, 0.01, 0.1]\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([2]),  # [1, 2]\n",
    "    \"num_layers\": tune.grid_search([2, 4]),  # , 8, 16]),  # [1, 2, 4, 8]\n",
    "    \"hidden_size\": tune.grid_search([32, 64]),  # , 128, 256]),  # [8, 16, 32, 64, 128]\n",
    "    \"num_fc_layers\": tune.grid_search([1, 2, 3]),  # 1, 2, 3]),\n",
    "    \"activation_type\": tune.grid_search([1]),  # , 2, 3]),  # , 2, 3])\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if (\n",
    "        type(v).__name__ == \"dict\"\n",
    "        and list(v.keys())[0] == \"grid_search\"\n",
    "        and len(list(v.values())[0]) > 1\n",
    "    ):\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:04:29,417\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-30 15:05:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:00.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.2/31.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  activation_type</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  num_fc_layers</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  optim_type</th><th style=\"text-align: right;\">  return_period</th><th style=\"text-align: right;\">  seq_len</th><th style=\"text-align: right;\">      acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_ec059_00001</td><td>RUNNING   </td><td>192.168.0.125:274137</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.860571</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.0992016</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.23696</td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">        0.101591</td></tr>\n",
       "<tr><td>train_LSTM_ec059_00002</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.685595</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00003</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.578927</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00004</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.224815</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00005</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.224796</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00006</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.146467</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.792941</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.580892</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.666458</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00010</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.116468</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00011</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.875928</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00012</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.765954</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00013</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.269871</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00014</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.24546 </td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00015</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.246724</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00016</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.343394</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00017</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.519805</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00018</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.445556</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00019</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.332983</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00020</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.589482</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00021</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.211595</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00022</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.333716</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00023</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.393089</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_LSTM_ec059_00000</td><td>TERMINATED</td><td>192.168.0.125:273206</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.399632</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.0948104</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        50.3548 </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">        0.102213</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  mean_accuracy</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_ec059_00000</td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">      0.0948104</td><td style=\"text-align: right;\">        0.102213</td></tr>\n",
       "<tr><td>train_LSTM_ec059_00001</td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">      0.0992016</td><td style=\"text-align: right;\">        0.101591</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:05:29,681\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-01-30 15:05:30,874\tINFO tune.py:1042 -- Total run time: 61.46 seconds (60.24 seconds for the tuning loop).\n",
      "2024-01-30 15:05:30,875\tWARNING tune.py:1057 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-01-30 15:05:30,880\tWARNING experiment_analysis.py:193 -- Failed to fetch metrics for 22 trial(s):\n",
      "- train_LSTM_ec059_00002: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00002: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00002_2_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.6856,num_fc_layers=2,num_layers=2,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00003: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00003: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00003_3_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.5789,num_fc_layers=2,num_layers=2,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00004: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00004: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00004_4_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.2248,num_fc_layers=3,num_layers=2,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00005: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00005: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00005_5_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.2248,num_fc_layers=3,num_layers=2,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00006: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00006: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00006_6_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.1465,num_fc_layers=1,num_layers=4,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00007: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00007: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00007_7_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.7929,num_fc_layers=1,num_layers=4,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00008: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00008: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00008_8_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.5809,num_fc_layers=2,num_layers=4,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00009: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00009: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00009_9_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.6665,num_fc_layers=2,num_layers=4,optim_type=2,retu_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00010: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00010: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00010_10_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.1165,num_fc_layers=3,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00011: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00011: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00011_11_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.8759,num_fc_layers=3,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00012: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00012: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00012_12_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.7660,num_fc_layers=1,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00013: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00013: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00013_13_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.2699,num_fc_layers=1,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00014: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00014: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00014_14_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.2455,num_fc_layers=2,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00015: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00015: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00015_15_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.2467,num_fc_layers=2,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00016: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00016: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00016_16_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.3434,num_fc_layers=3,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00017: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00017: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00017_17_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.5198,num_fc_layers=3,num_layers=2,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00018: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00018: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00018_18_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.4456,num_fc_layers=1,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00019: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00019: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00019_19_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.3330,num_fc_layers=1,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00020: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00020: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00020_20_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.5895,num_fc_layers=2,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00021: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00021: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00021_21_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.2116,num_fc_layers=2,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00022: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00022: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00022_22_activation_type=1,hidden_size=32,lr=0.0100,momentum=0.3337,num_fc_layers=3,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n",
      "- train_LSTM_ec059_00023: FileNotFoundError('Could not fetch metrics for train_LSTM_ec059_00023: both result.json and progress.csv were not found at /home/skchen/ray_results/train_LSTM_2024-01-30_15-04-29/train_LSTM_ec059_00023_23_activation_type=1,hidden_size=64,lr=0.0100,momentum=0.3931,num_fc_layers=3,num_layers=4,optim_type=2,ret_2024-01-30_15-04-29')\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "os.mkdir(log_dir)\n",
    "analysis = tune.run(\n",
    "    train_LSTM,\n",
    "    config=search_space,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_accuracy     trial_id\n",
      "0           0.097  aa488_00000\n",
      "1           0.101  aa488_00001\n",
      "2           0.101  aa488_00002\n",
      "3           0.099  aa488_00003\n",
      "4           0.104  aa488_00004\n",
      "5           0.102  aa488_00005\n",
      "6           0.099  aa488_00006\n",
      "7           0.102  aa488_00007\n",
      "8           0.103  aa488_00008\n",
      "9           0.104  aa488_00009\n",
      "10          0.102  aa488_00010\n",
      "11          0.097  aa488_00011\n",
      "12          0.104  aa488_00012\n",
      "13          0.098  aa488_00013\n",
      "14          0.101  aa488_00014\n",
      "15          0.100  aa488_00015\n",
      "16          0.100  aa488_00016\n",
      "17          0.101  aa488_00017\n",
      "18          0.102  aa488_00018\n",
      "19          0.101  aa488_00019\n",
      "20          0.101  aa488_00020\n",
      "21          0.097  aa488_00021\n",
      "22          0.106  aa488_00022\n",
      "23          0.102  aa488_00023\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "trial_list = list(analysis.trial_dataframes.values())\n",
    "for i, trial in enumerate(trial_list):\n",
    "    if trial.empty == False:\n",
    "        d = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"mean_accuracy\": trial.describe().loc[\"mean\", \"mean_accuracy\"],\n",
    "                \"trial_id\": trial.loc[0:0, \"trial_id\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": [np.NaN], \"trial_id\": [np.NaN]})\n",
    "    accuracy_list.append(d)\n",
    "accuracy_df = pd.concat(accuracy_list)\n",
    "accuracy_df = accuracy_df.reset_index().loc[:, [\"mean_accuracy\", \"trial_id\"]]\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0               5        5 0.010     0.465           2           2           32              1                1\n",
      "1               5        5 0.010     0.728           2           2           64              1                1\n",
      "2               5        5 0.010     0.260           2           2           32              2                1\n",
      "3               5        5 0.010     0.511           2           2           64              2                1\n",
      "4               5        5 0.010     0.574           2           2           32              3                1\n",
      "5               5        5 0.010     0.137           2           2           64              3                1\n",
      "6               5        5 0.010     0.586           2           4           32              1                1\n",
      "7               5        5 0.010     0.236           2           4           64              1                1\n",
      "8               5        5 0.010     0.152           2           4           32              2                1\n",
      "9               5        5 0.010     0.859           2           4           64              2                1\n",
      "10              5        5 0.010     0.873           2           4           32              3                1\n",
      "11              5        5 0.010     0.747           2           4           64              3                1\n",
      "12              5       10 0.010     0.344           2           2           32              1                1\n",
      "13              5       10 0.010     0.178           2           2           64              1                1\n",
      "14              5       10 0.010     0.647           2           2           32              2                1\n",
      "15              5       10 0.010     0.452           2           2           64              2                1\n",
      "16              5       10 0.010     0.198           2           2           32              3                1\n",
      "17              5       10 0.010     0.496           2           2           64              3                1\n",
      "18              5       10 0.010     0.128           2           4           32              1                1\n",
      "19              5       10 0.010     0.827           2           4           64              1                1\n",
      "20              5       10 0.010     0.307           2           4           32              2                1\n",
      "21              5       10 0.010     0.630           2           4           64              2                1\n",
      "22              5       10 0.010     0.349           2           4           32              3                1\n",
      "23              5       10 0.010     0.516           2           4           64              3                1\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0           0.097  aa488_00000              5        5 0.010     0.465           2           2           32              1                1\n",
      "1           0.101  aa488_00001              5        5 0.010     0.728           2           2           64              1                1\n",
      "2           0.101  aa488_00002              5        5 0.010     0.260           2           2           32              2                1\n",
      "3           0.099  aa488_00003              5        5 0.010     0.511           2           2           64              2                1\n",
      "4           0.104  aa488_00004              5        5 0.010     0.574           2           2           32              3                1\n",
      "5           0.102  aa488_00005              5        5 0.010     0.137           2           2           64              3                1\n",
      "6           0.099  aa488_00006              5        5 0.010     0.586           2           4           32              1                1\n",
      "7           0.102  aa488_00007              5        5 0.010     0.236           2           4           64              1                1\n",
      "8           0.103  aa488_00008              5        5 0.010     0.152           2           4           32              2                1\n",
      "9           0.104  aa488_00009              5        5 0.010     0.859           2           4           64              2                1\n",
      "10          0.102  aa488_00010              5        5 0.010     0.873           2           4           32              3                1\n",
      "11          0.097  aa488_00011              5        5 0.010     0.747           2           4           64              3                1\n",
      "12          0.104  aa488_00012              5       10 0.010     0.344           2           2           32              1                1\n",
      "13          0.098  aa488_00013              5       10 0.010     0.178           2           2           64              1                1\n",
      "14          0.101  aa488_00014              5       10 0.010     0.647           2           2           32              2                1\n",
      "15          0.100  aa488_00015              5       10 0.010     0.452           2           2           64              2                1\n",
      "16          0.100  aa488_00016              5       10 0.010     0.198           2           2           32              3                1\n",
      "17          0.101  aa488_00017              5       10 0.010     0.496           2           2           64              3                1\n",
      "18          0.102  aa488_00018              5       10 0.010     0.128           2           4           32              1                1\n",
      "19          0.101  aa488_00019              5       10 0.010     0.827           2           4           64              1                1\n",
      "20          0.101  aa488_00020              5       10 0.010     0.307           2           4           32              2                1\n",
      "21          0.097  aa488_00021              5       10 0.010     0.630           2           4           64              2                1\n",
      "22          0.106  aa488_00022              5       10 0.010     0.349           2           4           32              3                1\n",
      "23          0.102  aa488_00023              5       10 0.010     0.516           2           4           64              3                1\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "22          0.106  aa488_00022              5       10 0.010     0.349           2           4           32              3                1\n",
      "9           0.104  aa488_00009              5        5 0.010     0.859           2           4           64              2                1\n",
      "4           0.104  aa488_00004              5        5 0.010     0.574           2           2           32              3                1\n",
      "12          0.104  aa488_00012              5       10 0.010     0.344           2           2           32              1                1\n",
      "8           0.103  aa488_00008              5        5 0.010     0.152           2           4           32              2                1\n",
      "18          0.102  aa488_00018              5       10 0.010     0.128           2           4           32              1                1\n",
      "23          0.102  aa488_00023              5       10 0.010     0.516           2           4           64              3                1\n",
      "7           0.102  aa488_00007              5        5 0.010     0.236           2           4           64              1                1\n",
      "10          0.102  aa488_00010              5        5 0.010     0.873           2           4           32              3                1\n",
      "5           0.102  aa488_00005              5        5 0.010     0.137           2           2           64              3                1\n",
      "20          0.101  aa488_00020              5       10 0.010     0.307           2           4           32              2                1\n",
      "2           0.101  aa488_00002              5        5 0.010     0.260           2           2           32              2                1\n",
      "1           0.101  aa488_00001              5        5 0.010     0.728           2           2           64              1                1\n",
      "14          0.101  aa488_00014              5       10 0.010     0.647           2           2           32              2                1\n",
      "17          0.101  aa488_00017              5       10 0.010     0.496           2           2           64              3                1\n",
      "19          0.101  aa488_00019              5       10 0.010     0.827           2           4           64              1                1\n",
      "15          0.100  aa488_00015              5       10 0.010     0.452           2           2           64              2                1\n",
      "16          0.100  aa488_00016              5       10 0.010     0.198           2           2           32              3                1\n",
      "6           0.099  aa488_00006              5        5 0.010     0.586           2           4           32              1                1\n",
      "3           0.099  aa488_00003              5        5 0.010     0.511           2           2           64              2                1\n",
      "13          0.098  aa488_00013              5       10 0.010     0.178           2           2           64              1                1\n",
      "11          0.097  aa488_00011              5        5 0.010     0.747           2           4           64              3                1\n",
      "0           0.097  aa488_00000              5        5 0.010     0.465           2           2           32              1                1\n",
      "21          0.097  aa488_00021              5       10 0.010     0.630           2           4           64              2                1\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-30_14.12.32/5_10_0.01_0.34936886087152874_2_4_32_3_1.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-30_14.12.32/5_10_0.01_0.34936886087152874_2_4_32_3_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m best_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model_name)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:418\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    417\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 418\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc, \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-30_14.12.32/5_10_0.01_0.34936886087152874_2_4_32_3_1.pt'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "config_df = pd.DataFrame(analysis.get_all_configs().values())\n",
    "print(config_df)\n",
    "\n",
    "results = pd.concat([accuracy_df, config_df], axis=1)\n",
    "print(results)\n",
    "\n",
    "sorted_results = results.sort_values(by=\"mean_accuracy\", ascending=False)\n",
    "print(sorted_results.head(100))\n",
    "sorted_results_file = f\"{task_name}_sorted_results.csv\"\n",
    "sorted_results.to_csv(sorted_results_file)\n",
    "\n",
    "best_config = config_df.iloc[sorted_results.index[0]]\n",
    "id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in best_config.to_list())\n",
    "best_model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "print(best_model_name)\n",
    "shutil.copy(best_model_name, f\"{task_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLsAAADTCAYAAABp7hHfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwFUlEQVR4nO3de5yN5f7/8fcaY47mZJgx4zRy/DaVREQpanIsGbWJcmq+jN2BfsJm77ZDSDpIUmEnQ9+KptodCDntnRBiJJLzEDOYcpgZYpi5fn/0sLSawZo1a82aub2ej8f9x7rWdd/357qs+2Otz9wHmzHGCAAAAAAAALAAH28HAAAAAAAAALgLxS4AAAAAAABYBsUuAAAAAAAAWAbFLgAAAAAAAFgGxS4AAAAAAABYBsUuAAAAAAAAWAbFLgAAAAAAAFgGxS4AAAAAAABYBsUuAAAAAAAAWAbFLlzV2LFjZbPZ9Msvv1yxX1xcnPr163fV7aWkpMhmsyk9Pf2qfZ3dprcVZ0wAioccdHXkIMAzyD9XR/4BSo5cc3XkGhQXxS7AInbv3q2HH35YNWrUUFBQkBo1aqTnnntOZ86c8XZoACxu+/bt+stf/qLrrrtOQUFBqlKliu6880598cUXDv0KCgqUkpKiLl26qGbNmgoODtYNN9ygCRMm6OzZs16KHoAVbN68WV26dFHlypUVFBSkG264QdOmTbts/5MnTyoqKko2m00fffRRKUYKoDzKzc3VmDFj1KFDB1WuXFk2m00pKSmF+hX3u86pU6c0YsQI1a9fX4GBgapdu7aSkpJ08ODBUhiVtfl6OwBYx86dO+Xjc23WT3v37q2HH35Y/v7+Xtn/zz//rObNmyssLExPPvmkKleurHXr1mnMmDHatGmTPvvsM6/EBZQmcpD3ctCBAweUk5Ojvn37KjY2VmfOnNHHH3+sLl26aObMmRo4cKAk6cyZM+rfv79uu+02DRo0SFFRUfZctWLFCq1cuVI2m80rYwBKgvzjvfwjSV999ZXuv/9+NWnSRP/85z9VqVIl7d27V4cOHbrsOqNHj+YPgih3yDXeyzW//PKLnnvuOdWqVUuNGzfWf/7znyL7Fee7TkFBge699179+OOPevzxx9WgQQPt2bNHb775ppYuXaodO3YoJCSkFEdpLRS74Dbe/JLjbRUqVFCFChW8tv93331XJ0+e1DfffKP4+HhJ0sCBA1VQUKB58+bpxIkTioiI8Fp8QGkgB3kvB3Xq1EmdOnVyaHvyySfVtGlTTZkyxV7s8vPz05o1a9SqVSt7vwEDBiguLs7+JTAhIaFUYwfcgfzjvfyTnZ2tPn36qHPnzvroo4+cKgRs27ZNb731lkaPHq3Ro0eXQpSAe5BrvJdrYmJilJmZqWrVqum7777TrbfeWmS/4nzX+fbbb7Vx40ZNnz5dTzzxhL1/w4YN9dhjj2n58uVKTEz07MAs7NosC8MlJ0+eVL9+/RQeHq6wsDD179/f4S9iRV3vvX37dt19990KDAxUjRo1NGHCBBUUFBTatjFGEyZMsF+C17ZtW23fvv2ycTz99NOqWbOm/P39Va9ePU2ePNlhu+np6bLZbHr55Zc1a9Ys1a1bV/7+/rr11lu1cePGYo/99ddfV3x8vIKCghQREaFmzZrp/ffft7//52vIL153X9TyxzkqKCjQ1KlTFR8fr4CAAEVHRys5OVknTpwoVnzZ2dmSpOjoaIf2mJgY+fj4yM/Pr9hjBsoaclDZzUFFqVChgmrWrKmTJ0/a2/z8/By+/F108Yvcjh07SrxfwBPIP2U3/7z//vs6evSoJk6cKB8fH50+fbrIef6jIUOGKDExUa1bty7WvgBPI9eU3Vzj7++vatWqXbVfcb7rXOk3nCQFBgYWK0Y44swuOK179+6qU6eOJk2apM2bN+vtt99WVFSUJk+eXGT/I0eOqG3btrpw4YJGjhyp4OBgzZo1q8iDdvTo0ZowYYL97IDNmzerXbt2ysvLc+h35swZ3XXXXTp8+LCSk5NVq1YtrV27VqNGjVJmZqamTp3q0P/9999XTk6OkpOTZbPZ9OKLL6pbt27at2+fKlas6NS4//Wvf2nw4MF66KGHNGTIEJ09e1Zbt27V+vXr1atXryLX6datm+rVq+fQtmnTJk2dOlVRUVH2tuTkZKWkpKh///4aPHiw9u/fr+nTpystLU1r1qxxOsY2bdpo8uTJSkpK0rhx4xQZGam1a9fqrbfe0uDBgxUcHOzUdoCyjBxUdnPQRadPn9Zvv/2mU6dO6fPPP9fixYvVo0ePq6535MgRSVKVKlWKtT+gtJB/ym7+Wb58uUJDQ3X48GF17dpVu3btUnBwsHr37q1XX31VAQEBDv1TU1O1du1a7dixgxtdo8wh15TdXFNSRX3XadasmYKDg/XPf/5TlStXVsOGDbVnzx6NGDFCt956K2e7l5QBrmLMmDFGknnssccc2hMTE01kZKT9de3atU3fvn3tr59++mkjyaxfv97eduzYMRMWFmYkmf3799vb/Pz8TOfOnU1BQYG979///ncjyWGb48ePN8HBwWbXrl0OsYwcOdJUqFDBHDx40BhjzP79+40kExkZaY4fP27v99lnnxlJ5osvvnB6/A888ICJj4+/Yp85c+Y4jOnPsrKyTK1atcyNN95ocnNzjTHGrF692kgy7733nkPfJUuWFNl+NePHjzeBgYFGkn35xz/+UaxtAGUROah85CBjjElOTrbnHx8fH/PQQw85jP9yEhISTGhoqDlx4kSx9wl4Evmn7Oefm266yQQFBZmgoCDz1FNPmY8//tg89dRTRpJ5+OGHHfqeOXPG1KpVy4waNcoYY8yqVauMJJOamur0/gBPINeU/VzzRxs3bjSSzJw5c5xe53LfdRYuXGhiYmIcfsO1b9/e5OTkuBQbLuEyRjht0KBBDq9bt26tX3/91X765Z99+eWXuu2229S8eXN7W9WqVfXII4849Fu+fLny8vL01FNPOdyY+Omnny60zdTUVLVu3VoRERH65Zdf7EtCQoLy8/P19ddfO/Tv0aOHw72qLp6uvm/fPucGLSk8PFyHDh1y6XRcScrPz1fPnj2Vk5Ojf//73/azrFJTUxUWFqZ7773XYSxNmzZVpUqVtGrVqmLtJy4uTnfeeadmzZqljz/+WI899pief/55TZ8+3aW4gbKGHFS2c5D0+5wtW7ZMc+fOVceOHZWfn1/oL8Z/9vzzz2v58uV64YUXFB4e7soQAY8j/5Td/JObm6szZ86oT58+mjZtmrp166Zp06YpOTlZ8+fP1+7du+19X3jhBZ0/f15///vfXRoP4GnkmrKba0riSt91qlatqiZNmmjixIn69NNPNXbsWK1evVr9+/cvldisjMsY4bRatWo5vL6Y1E6cOKHQ0NBC/Q8cOKAWLVoUam/YsGGhfpJUv359h/aqVasWuqn67t27tXXrVlWtWrXIGI8dO+Z0zM7629/+puXLl6t58+aqV6+e2rVrp169eun22293av1nn31WK1eu1KJFi1S3bl2HsZw6dcrhNNsrjeVK5s+fr4EDB2rXrl2qUaOGpN9P7S0oKNDf/vY39ezZU5GRkU5vDyiLyEFlNwdd1KhRIzVq1EiS1KdPH7Vr107333+/1q9fX+RTFhcsWKBnn31WSUlJ+utf/1rs/QGlhfxTdvPPxcu1evbs6dDeq1cvzZw5U+vWrVP9+vWVnp6ul156SW+88YYqVark9PaB0kSuKbu5xlVX+q6zb98+tW3bVvPmzdODDz4oSXrggQfs92ZbvHixOnbs6PEYrYpiF5x2uadfGGNKLYaLj2cdMWJEke83aNDA4bU7Yv6f//kf7dy5UwsXLtSSJUv08ccf680339To0aM1bty4K6776aefavLkyRo/frw6dOhQaCxRUVF67733ilz3cv/BFOXNN99UkyZN7IWui7p06aKUlBSlpaVxzTfKPXJQ2c1Bl/PQQw8pOTlZu3btKvTFe9myZfYnqM2YMaPE+wI8ifxTdvNPbGystm/fXugGzxd/3F78wT169GhVr15dbdq0sd+r6+I9dLKyspSenq5atWo59TRHwFPINWU317jiat91UlJSdPbsWd13330O7V26dJEkrVmzhmJXCVDsgsfUrl3b4dTxi3bu3Fmon/R75f26666zt2dlZRX6i0DdunWVm5tb6oWb4OBg9ejRQz169FBeXp66deumiRMnatSoUYVufHrRrl271LdvX3Xt2rXI0+Xr1q2r5cuX6/bbby/xkzaOHj1a6K8yknT+/HlJ0oULF0q0faA8IgeVXg66nN9++02SdOrUKYf29evXKzExUc2aNdOHH34oX1++jsBayD+ll3+aNm2qZcuW6fDhww5F9YyMDEmXfswePHhQe/bscZjnix5//HFJvxfGuJwa5Qm5xvvfdS7Hme86R48elTFG+fn5Du38hnMP/nQBj+nUqZO+/fZbbdiwwd6WlZVVqLKekJCgihUr6vXXX3f4C8Cfn/Qh/f6EknXr1mnp0qWF3jt58qRHEsKvv/7q8NrPz0/XX3+9jDH2RPRnubm5SkxMVPXq1TV37twiL9/p3r278vPzNX78+ELvXbhwQSdPnnQ6xgYNGigtLU27du1yaP/ggw/k4+Ojm266yeltAVZBDiq9HFTUZQDnz5/XvHnzFBgYqOuvv97evmPHDnXu3FlxcXFauHAhj9WGJZF/Si//dO/eXZI0e/Zsh/a3335bvr6+atOmjSRpwoQJ+ve//+2wXNz/iBEjHO7zA5QX5JrSyzXF4ex3nQYNGsgYow8//NCh/YMPPpAkNWnSxCPxXSv4Uyo8ZsSIEXr33XfVoUMHDRkyxP4o3Nq1a2vr1q32flWrVtWwYcM0adIk3XffferUqZPS0tK0ePHiQo+hHz58uD7//HPdd9996tevn5o2barTp0/rhx9+0EcffaT09HS3P7q+Xbt2qlatmm6//XZFR0drx44dmj59ujp37qyQkJAi1xk3bpx+/PFHPfvss/rss88c3qtbt65atmypu+66S8nJyZo0aZK2bNmidu3aqWLFitq9e7dSU1P12muv6aGHHnIqxuHDh2vx4sVq3bq1nnzySUVGRmrhwoVavHix/vd//1exsbElngegvCEHlV4OSk5OVnZ2tu68805Vr15dR44c0XvvvaeffvpJr7zyiv3+ODk5OWrfvr1OnDih4cOHa9GiRUXGBpR35J/Syz9NmjTRY489pnfeeUcXLlzQXXfdpf/85z9KTU3VqFGj7N+B7rjjjkLrXjyL69Zbb1XXrl2dnxigjCDXlF6ukaTp06fr5MmT9jNHv/jiCx06dEiS9NRTTyksLKxY33X69eunl19+WcnJyUpLS1N8fLw2b96st99+W/Hx8UpMTHQ6NhShtB//iPLn4qNws7KyHNr//PjXPz8K1xhjtm7dau666y4TEBBgqlevbsaPH29mz55d6LGx+fn5Zty4cSYmJsYEBgaaNm3amG3bthW5zZycHDNq1ChTr1494+fnZ6pUqWJatWplXn75ZZOXl2eMufQo3JdeeqnQeCSZMWPGOD3+mTNnmjvvvNNERkYaf39/U7duXTN8+HBz6tSpy85F3759HR4f+8flz+OZNWuWadq0qQkMDDQhISHmxhtvNCNGjDAZGRlOx2iMMevXrzcdO3Y01apVMxUrVjQNGjQwEydONOfPny/WdoCyhhxU9nPQBx98YBISEkx0dLTx9fU1ERERJiEhwXz22WcO/S7Oi7OxAd5G/in7+ccYY/Ly8szYsWNN7dq1TcWKFU29evXMq6++etX1Vq1aZSSZ1NTUYu0PcDdyTfnINbVr177sPi/GVdzvOocOHTKPPfaYqVOnjvHz8zMxMTFmwIABhT4LKD6bMaV4tzsAAAAAAADAg7hnFwAAAAAAACyDe3bhmpWXl6fjx49fsU9YWJhXb56cm5ur3NzcK/apWrXqZR/5C6DsIgcB8BbyD4DSQK6BN1HswjVr7dq1atu27RX7zJkzR/369SudgIrw8ssva9y4cVfss3//fsXFxZVOQADchhwEwFvIPwBKA7kG3sQ9u3DNOnHihDZt2nTFPvHx8YqJiSmliArbt2+f9u3bd8U+d9xxhwICAkopIgDuQg4C4C3kHwClgVwDb6LYBQAAAAAAAMvgBvUAAAAAAACwDMvfs6ugoEAZGRkKCQmRzWbzdjgAyiljjHJychQbGysfn5L9nYC8BMAdXMlL5B8A7kD+AeAtzuYfyxe7MjIyVLNmTW+HAcAifv75Z9WoUaNE2yAvAXCn4uQl8g8AdyL/APCWq+Ufyxe7QkJCJP0+EaGhoV6OBkB5lZ2drZo1a9pzSkmQlwC4gyt5ifwDwB3IPwC8xdn8Y/li18VTZENDQ0mqAErMHafdk5cAuFNx8hL5B4A7kX8AeMvV8g83qAcAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlWP4G9aUtbuSiQm3pL3T2QiQAAAAAAADXHs7sAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBlUOwCAAAAAACAZVDsAgAAAAAAgGVQ7AIAAAAAAIBluFTs2rdvn7vjAAAAAAAAAErMpWJXvXr11LZtW/3f//2fzp496+6YAAAAAAAAAJe4VOzavHmzbrrpJg0dOlTVqlVTcnKyNmzYUOztfP3117r//vsVGxsrm82mTz/91OF9Y4xGjx6tmJgYBQYGKiEhQbt373YlZAAAAAAAAFwDXCp23XzzzXrttdeUkZGhd955R5mZmbrjjjt0ww03aMqUKcrKynJqO6dPn1bjxo31xhtvFPn+iy++qGnTpmnGjBlav369goOD1b59e84mAwAAAAAAQJFKdIN6X19fdevWTampqZo8ebL27NmjYcOGqWbNmurTp48yMzOvuH7Hjh01YcIEJSYmFnrPGKOpU6fq2Wef1QMPPKCbbrpJ8+bNU0ZGRqEzwAAAAAAAAACphMWu7777To8//rhiYmI0ZcoUDRs2THv37tWyZcuUkZGhBx54wOVt79+/X0eOHFFCQoK9LSwsTC1atNC6desuu965c+eUnZ3tsACAN5GXAHgL+QeAt5B/AHiTS8WuKVOm6MYbb1SrVq2UkZGhefPm6cCBA5owYYLq1Kmj1q1bKyUlRZs3b3Y5sCNHjkiSoqOjHdqjo6Pt7xVl0qRJCgsLsy81a9Z0OQYAcAfyEgBvIf8A8BbyDwBvcqnY9dZbb6lXr146cOCAPv30U913333y8XHcVFRUlGbPnu2WIItj1KhROnXqlH35+eefSz0GAPgj8hIAbyH/APAW8g8Ab/J1ZSVnnojo5+envn37urJ5SVK1atUkSUePHlVMTIy9/ejRo7r55psvu56/v7/8/f1d3i8AuBt5CYC3kH8AeAv5B4A3uXRm15w5c5SamlqoPTU1VXPnzi1xUJJUp04dVatWTStWrLC3ZWdna/369WrZsqVb9gEAAAAAAABrcanYNWnSJFWpUqVQe1RUlJ5//nmnt5Obm6stW7Zoy5Ytkn6/Kf2WLVt08OBB2Ww2Pf3005owYYI+//xz/fDDD+rTp49iY2PVtWtXV8IGAAAAAACAxbl0GePBgwdVp06dQu21a9fWwYMHnd7Od999p7Zt29pfDx06VJLUt29fpaSkaMSIETp9+rQGDhyokydP6o477tCSJUsUEBDgStgAAAAAAACwOJeKXVFRUdq6davi4uIc2r///ntFRkY6vZ02bdrIGHPZ9202m5577jk999xzroQJAAAAAACAa4xLlzH27NlTgwcP1qpVq5Sfn6/8/HytXLlSQ4YM0cMPP+zuGAEAAAAAAACnuHRm1/jx45Wenq577rlHvr6/b6KgoEB9+vQp1j27AAAAAAAAAHdyqdjl5+enBQsWaPz48fr+++8VGBioG2+8UbVr13Z3fAAAAAAAAIDTXCp2XdSgQQM1aNDAXbEAAK4gbuSiQm3pL3T2QiQArjXkHwAAUJ64VOzKz89XSkqKVqxYoWPHjqmgoMDh/ZUrV7olOAAAAAAAAKA4XCp2DRkyRCkpKercubNuuOEG2Ww2d8cFAAAAAAAAFJtLxa758+frww8/VKdOndwdDwAAAAAAAOAyH1dW8vPzU7169dwdCwAAAAAAAFAiLhW7nnnmGb322msyxrg7HgAAAAAAAMBlLl3G+M0332jVqlVavHix4uPjVbFiRYf3P/nkE7cEBwAAAAAAABSHS8Wu8PBwJSYmujsWAAAAAAAAoERcKnbNmTPH3XEAAAAAAAAAJebSPbsk6cKFC1q+fLlmzpypnJwcSVJGRoZyc3PdFhwAAAAAAABQHC6d2XXgwAF16NBBBw8e1Llz53TvvfcqJCREkydP1rlz5zRjxgx3xwkAAAAAAABclUtndg0ZMkTNmjXTiRMnFBgYaG9PTEzUihUr3BYcAAAAAAAAUBwundm1evVqrV27Vn5+fg7tcXFxOnz4sFsCAwAAAAAAAIrLpTO7CgoKlJ+fX6j90KFDCgkJKXFQAAAAAAAAgCtcKna1a9dOU6dOtb+22WzKzc3VmDFj1KlTJ3fFBgAAAAAAABSLS5cxvvLKK2rfvr2uv/56nT17Vr169dLu3btVpUoVffDBB+6OEQAAAAAAAHCKS8WuGjVq6Pvvv9f8+fO1detW5ebmKikpSY888ojDDesBAAAAAACA0uRSsUuSfH199eijj7ozFgAAAAAAAKBEXCp2zZs374rv9+nTx6VgAAAAAAAAgJJwqdg1ZMgQh9fnz5/XmTNn5Ofnp6CgIIpdAAAAAAAA8AqXil0nTpwo1LZ792799a9/1fDhw0scFADAOXEjFxVqS3+hsxciAQAAAICywcddG6pfv75eeOGFQmd9AQAAAAAAAKXFbcUu6feb1mdkZLhzkwAAAAAAAIDTXLqM8fPPP3d4bYxRZmampk+frttvv90tgQEAAAAAAADF5VKxq2vXrg6vbTabqlatqrvvvluvvPKKO+ICAAAAAAAAis2lYldBQYG74wAAAAAAAABKzK337AIAAAAAAAC8yaUzu4YOHep03ylTpriyCwAAAAAAAKDYXCp2paWlKS0tTefPn1fDhg0lSbt27VKFChV0yy232PvZbDb3RAkAAAAAAAA4waVi1/3336+QkBDNnTtXERERkqQTJ06of//+at26tZ555hm3BgkAAAAAAAA4w6Vi1yuvvKKvvvrKXuiSpIiICE2YMEHt2rWj2AUAXhQ3clGhtvQXOnshEgAAAAAofS7doD47O1tZWVmF2rOyspSTk1PioAAAAAAAAABXuFTsSkxMVP/+/fXJJ5/o0KFDOnTokD7++GMlJSWpW7du7o4RAAAAAAAAcIpLlzHOmDFDw4YNU69evXT+/PnfN+Trq6SkJL300ktuDRAAAAAAAABwlkvFrqCgIL355pt66aWXtHfvXklS3bp1FRwc7NbgAAAAAAAAgOJw6TLGizIzM5WZman69esrODhYxhh3xQUAAAAAAAAUm0vFrl9//VX33HOPGjRooE6dOikzM1OSlJSU5NYnMY4dO1Y2m81hadSokdu2DwAAAAAAAGtxqdj1//7f/1PFihV18OBBBQUF2dt79OihJUuWuC04SYqPj7efQZaZmalvvvnGrdsHAAAAAACAdbh0z66vvvpKS5cuVY0aNRza69evrwMHDrglsIt8fX1VrVo1t24TAAAAAAAA1uRSsev06dMOZ3RddPz4cfn7+5c4qD/avXu3YmNjFRAQoJYtW2rSpEmqVavWZfufO3dO586ds7/Ozs52azwAUFzkJQDeQv4B4C3kHwDe5NJljK1bt9a8efPsr202mwoKCvTiiy+qbdu2bguuRYsWSklJ0ZIlS/TWW29p//79at26tXJyci67zqRJkxQWFmZfatas6bZ4/ixu5KJCCwD8WWnmJQD4I/IPAG8h/wDwJptx4RGK27Zt0z333KNbbrlFK1euVJcuXbR9+3YdP35ca9asUd26dT0Rq06ePKnatWtrypQpSkpKKrJPUX9BqFmzpk6dOqXQ0FC3xuNscSv9hc5u3S+A0pedna2wsDCXcom78lJJCurkIcB6nMlLnsw/5BXg2lWa+QcA/sjZ32UuXcZ4ww03aNeuXZo+fbpCQkKUm5urbt266YknnlBMTIzLQV9NeHi4GjRooD179ly2j7+/v9svpQSAkiAvAfAW8g8AbyH/APCmYhe7zp8/rw4dOmjGjBn6xz/+4YmYLis3N1d79+5V7969S3W/AAAAAAAAKB+Kfc+uihUrauvWrZ6IpZBhw4bpv//9r9LT07V27VolJiaqQoUK6tmzZ6nsHwAAAAAAAOWLSzeof/TRRzV79mx3x1LIoUOH1LNnTzVs2FDdu3dXZGSkvv32W1WtWtXj+wYAAAAAAED549I9uy5cuKB33nlHy5cvV9OmTRUcHOzw/pQpU9wS3Pz5892yHQAAAAAAAFwbilXs2rdvn+Li4rRt2zbdcsstkqRdu3Y59LHZbO6LDgAAAAAAACiGYhW76tevr8zMTK1atUqS1KNHD02bNk3R0dEeCQ4AAAAAAAAojmLds8sY4/B68eLFOn36tFsDAgAAAAAAAFzl0j27Lvpz8QsAAADXhriRiwq1pb/Q2eV+APBnJckf5B7g2lasM7tsNluhe3Jxjy4AAAAAAACUFcU6s8sYo379+snf31+SdPbsWQ0aNKjQ0xg/+eQT90UIAAAAAAAAOKlYxa6+ffs6vH700UfdGgwAAAAAAABQEsUqds2ZM8dTcQAAAAAAAAAlVqx7dgEAAAAAAABlGcUuAAAAAAAAWAbFLgAAAAAAAFgGxS4AAAAAAABYBsUuAAAAAAAAWEaxnsYIAAAAXE7cyEXeDgEAAIAzuwAAAAAAAGAdFLsAAAAAAABgGRS7AAAAAAAAYBkUuwAAAAAAAGAZFLsAAAAAAABgGRS7AAAAAAAAYBkUuwAAAAAAAGAZFLsAAAAAAABgGb7eDuBaEDdyUaG29Bc6eyESAAAAACi/ivptVZJ1+V1WtvBvBHfhzC4AAAAAAABYBsUuAAAAAAAAWAbFLgAAAAAAAFgGxS4AAAAAAABYBsUuAAAAAAAAWAbFLgAAAAAAAFgGxS4AAAAAAABYBsUuAAAAAAAAWAbFLgAAAAAAAFiGr7cDKIviRi4q1Jb+Qmev7KM0YrGK8jhX5THm8oh5LnoOilIe58Xd/758XlAarvXPmbdyEvnCOXxPtTb+3RyV5PNeFG/llNLIb87y1ueJHF/2leaccmYXAAAAAAAALINiFwAAAAAAACyDYhcAAAAAAAAsg2IXAAAAAAAALINiFwAAAAAAACyjXBS73njjDcXFxSkgIEAtWrTQhg0bvB0SAAAAAAAAyqAyX+xasGCBhg4dqjFjxmjz5s1q3Lix2rdvr2PHjnk7NAAAAAAAAJQxZb7YNWXKFA0YMED9+/fX9ddfrxkzZigoKEjvvPOOt0MDAAAAAABAGePr7QCuJC8vT5s2bdKoUaPsbT4+PkpISNC6deuKXOfcuXM6d+6c/fWpU6ckSdnZ2U7vt+DcmUJtRa1fVL+ScHYfxRnLtaQ8zlV5jLk8csc8X+xvjCn2/t2RlyT355yilMfPn7uPI45LlIbSykvlKf8Uxd3HHvnCOSX5LmyF8Vtdeco/ZSn3OBuLt3JKaeQ3Z1l5DshxJVOqv8tMGXb48GEjyaxdu9ahffjw4aZ58+ZFrjNmzBgjiYWFhcUjy88//1zsXEZeYmFh8eRypbxE/mFhYfHkQv5hYWHx1nK132U2Y1w4TaGUZGRkqHr16lq7dq1atmxpbx8xYoT++9//av369YXW+fNfEAoKCnT8+HFFRkbKZrNdcX/Z2dmqWbOmfv75Z4WGhrpvILBjjj2POfYMY4xycnIUGxsrH5/iXQHuSl7i3/ES5sIR83HJtT4XzuSlknwvKm+u9c+DpzCvnlHe59VK+ae8/1v8EWMpmxiLezn7u6xMX8ZYpUoVVahQQUePHnVoP3r0qKpVq1bkOv7+/vL393doCw8PL9Z+Q0NDy/2HsKxjjj2POXa/sLAwl9YrSV7i3/ES5sIR83HJtTwXV8tL7vheVN5cy58HT2JePaM8z6vV8k95/rf4M8ZSNjEW93Hmd1mZvkG9n5+fmjZtqhUrVtjbCgoKtGLFCoczvQAAAAAAAACpjJ/ZJUlDhw5V37591axZMzVv3lxTp07V6dOn1b9/f2+HBgAAAAAAgDKmzBe7evTooaysLI0ePVpHjhzRzTffrCVLlig6Otrt+/L399eYMWMKnW4L92GOPY85tgb+HS9hLhwxH5cwF/gjPg+ewbx6BvNadljp34KxlE2MxTvK9A3qAQAAAAAAgOIo0/fsAgAAAAAAAIqDYhcAAAAAAAAsg2IXAAAAAAAALINiFwAAAAAAACyDYhcAAAAAAAAsw1LFrjfeeENxcXEKCAhQixYttGHDhiv2T01NVaNGjRQQEKAbb7xRX375pcP7Y8eOVaNGjRQcHKyIiAglJCRo/fr1Dn02b96se++9V+Hh4YqMjNTAgQOVm5vr9rGVFe6e4z8aNGiQbDabpk6d6tB+/PhxPfLIIwoNDVV4eLiSkpKY4z9wxxxPnDhRrVq1UlBQkMLDw90wCpCPLiFvOOIYv6S05yI9PV1JSUmqU6eOAgMDVbduXY0ZM0Z5eXnuGhJKgFzhGRxnnuONz+xF586d08033yybzaYtW7aUYBTln5Vyh5WOVysdH94ay6JFi9SiRQsFBgYqIiJCXbt2LeFIvDOWXbt26YEHHlCVKlUUGhqqO+64Q6tWrSrxWK7KWMT8+fONn5+feeedd8z27dvNgAEDTHh4uDl69GiR/desWWMqVKhgXnzxRfPjjz+aZ5991lSsWNH88MMP9j7vvfeeWbZsmdm7d6/Ztm2bSUpKMqGhoebYsWPGGGMOHz5sIiIizKBBg8xPP/1kNmzYYFq1amUefPDBUhlzafPEHF/0ySefmMaNG5vY2Fjz6quvOrzXoUMH07hxY/Ptt9+a1atXm3r16pmePXt6Yohe5605Hj16tJkyZYoZOnSoCQsL88DIri3ko0vIG444xi/xxlwsXrzY9OvXzyxdutTs3bvXfPbZZyYqKso888wznhomnESu8AyOM8/x1mf2osGDB5uOHTsaSSYtLc2NIytfrJQ7rHS8Wun48NZYPvroIxMREWHeeusts3PnTrN9+3azYMGCcjmW+vXrm06dOpnvv//e7Nq1yzz++OMmKCjIZGZmlmg8V2OZYlfz5s3NE088YX+dn59vYmNjzaRJk4rs3717d9O5c2eHthYtWpjk5OTL7uPUqVNGklm+fLkxxpiZM2eaqKgok5+fb++zdetWI8ns3r27JMMpkzw1x4cOHTLVq1c327ZtM7Vr13Y4OH788UcjyWzcuNHetnjxYmOz2czhw4fdMKqyxRtz/Edz5swpMz+EyzPy0SXkDUcc45d4ey4uevHFF02dOnVcGwTchlzhGRxnnuPNuf3yyy9No0aNzPbt26/5YpeVcoeVjlcrHR/eGMv58+dN9erVzdtvv12i2P/MG2PJysoykszXX39tb8vOzjaSzLJly9wwqsuzxGWMeXl52rRpkxISEuxtPj4+SkhI0Lp164pcZ926dQ79Jal9+/aX7Z+Xl6dZs2YpLCxMjRs3lvT76ZF+fn7y8bk0jYGBgZKkb775pkRjKms8NccFBQXq3bu3hg8frvj4+CK3ER4ermbNmtnbEhIS5OPjU+gSrvLOW3MM9yIfOcZJ3riEY/ySsjQXp06dUuXKlV0YBdyFXOEZHGee4825PXr0qAYMGKB3331XQUFBbhhN+WWl3GGl49VKx4e3xrJ582YdPnxYPj4+atKkiWJiYtSxY0dt27at3I0lMjJSDRs21Lx583T69GlduHBBM2fOVFRUlJo2beryeJxhiWLXL7/8ovz8fEVHRzu0R0dH68iRI0Wuc+TIEaf6L1y4UJUqVVJAQIBeffVVLVu2TFWqVJEk3X333Tpy5Iheeukl5eXl6cSJExo5cqQkKTMz013DKxM8NceTJ0+Wr6+vBg8efNltREVFObT5+vqqcuXKl91veeWtOYZ7kY8uIW844hi/pKzMxZ49e/T6668rOTm5mCOAO5ErPIPjzHO8NbfGGPXr10+DBg1yKLRcq6yUO6x0vFrp+PDWWPbt2yfp93v2Pvvss1q4cKEiIiLUpk0bHT9+vFyNxWazafny5UpLS1NISIgCAgI0ZcoULVmyRBERES6NxVm+Ht26BbRt21ZbtmzRL7/8on/961/q3r271q9fr6ioKMXHx2vu3LkaOnSoRo0apQoVKmjw4MGKjo52OLsCRdu0aZNee+01bd68WTabzdvhWBJzbC3kIz7Tf8Z8XFLcuTh8+LA6dOigv/zlLxowYEApRIjSxLHhGRxnnuPM3L7++uvKycnRqFGjSjm6a4eVcoeVjlcrHR/OjKWgoECS9I9//EMPPvigJGnOnDmqUaOGUlNTy8wfD5wZizFGTzzxhKKiorR69WoFBgbq7bff1v3336+NGzcqJibGY/FZ4hdQlSpVVKFCBR09etSh/ejRo6pWrVqR61SrVs2p/sHBwapXr55uu+02zZ49W76+vpo9e7b9/V69eunIkSM6fPiwfv31V40dO1ZZWVm67rrr3DS6ssETc7x69WodO3ZMtWrVkq+vr3x9fXXgwAE988wziouLs2/j2LFjDtu4cOGCjh8/ftn9llfemmO4F/noEvKGI47xS7w9FxkZGWrbtq1atWqlWbNmuW9gcAm5wjM4zjzHW3O7cuVKrVu3Tv7+/vL19VW9evUkSc2aNVPfvn3dPMqyz0q5w0rHq5WOD2+N5WIB6Prrr7dvw9/fX9ddd50OHjxYrsaycuVKLVy4UPPnz9ftt9+uW265RW+++aYCAwM1d+5cl8biLEsUu/z8/NS0aVOtWLHC3lZQUKAVK1aoZcuWRa7TsmVLh/6StGzZssv2/+N2z507V6g9OjpalSpV0oIFCxQQEKB7773XhZGUXZ6Y4969e2vr1q3asmWLfYmNjdXw4cO1dOlS+zZOnjypTZs22bexcuVKFRQUqEWLFu4epld5a47hXuSjS8gbjjjGL/HmXBw+fFht2rRR06ZNNWfOHEud+VhekSs8g+PMc7w1t9OmTdP3339vf//LL7+UJC1YsEATJ070xFDLNCvlDisdr1Y6Prw1lqZNm8rf3187d+60b+P8+fNKT09X7dq1y9VYzpw5I0mFPlc+Pj72M9g8xqO3vy9F8+fPN/7+/iYlJcX8+OOPZuDAgSY8PNwcOXLEGGNM7969zciRI+3916xZY3x9fc3LL79sduzYYcaMGePwGM3c3FwzatQos27dOpOenm6+++47079/f+Pv72+2bdtm387rr79uNm3aZHbu3GmmT59uAgMDzWuvvVa6gy8l7p7johT1VI0OHTqYJk2amPXr15tvvvnG1K9f31KPBf8jb83xgQMHTFpamhk3bpypVKmSSUtLM2lpaSYnJ8cj47Q68tEl5A1HHOOXeGMuDh06ZOrVq2fuuecec+jQIZOZmWlf4F3kCs/gOPMcb31m/2j//v3X/NMYrZQ7rHS8Wun48NZYhgwZYqpXr26WLl1qfvrpJ5OUlGSioqLM8ePHy9VYsrKyTGRkpOnWrZvZsmWL2blzpxk2bJipWLGi2bJli8tjcYZlil3G/P5Dr1atWsbPz880b97cfPvtt/b37rrrLtO3b1+H/h9++KFp0KCB8fPzM/Hx8WbRokX293777TeTmJhoYmNjjZ+fn4mJiTFdunQxGzZscNhG7969TeXKlY2fn5+56aabzLx58zw6Rm9z5xwXpagD/ddffzU9e/Y0lSpVMqGhoaZ///6WLsJ4Y4779u1rJBVaVq1a5aZRXXvIR5eQNxxxjF9S2nMxZ86cIufBQn/7K9fIFZ7BceY53vjM/hHFrt9ZKXdY6Xi10vHhjbHk5eWZZ555xkRFRZmQkBCTkJDg8Efu8jSWjRs3mnbt2pnKlSubkJAQc9ttt5kvv/yyxGO5Gpsxxnj23DEAAAAAAACgdFjrAnoAAAAAAABc0yh2AQAAAAAAwDIodgEAAAAAAMAyKHYBAAAAAADAMih2AQAAAAAAwDIodgEAAAAAAMAyKHYBAAAAAADAMih2AQAAAAAAwDIodgEAAAAAAMAyKHYBAAAAAADAMih2AQAAAAAAwDL+Pw+VwQpTGWSIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLwAAADTCAYAAACLMgqmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyEklEQVR4nO3dd3hUdb7H8c8kIQ3SIAldQncVBIQlgiAqYCjXglwL0kGBq7t4L6DCVWkWAiyIgooFgiC7IK5XXYpKU5SqQECKgEBoCUVaCCUh5Hf/8GGWMQlkJieZycn79TzneTKn/Ob7PWfy+535zpkzDmOMEQAAAAAAAGATft4OAAAAAAAAALASBS8AAAAAAADYCgUvAAAAAAAA2AoFLwAAAAAAANgKBS8AAAAAAADYCgUvAAAAAAAA2AoFLwAAAAAAANgKBS8AAAAAAADYCgUvAAAAAAAA2AoFL6CITZw4UbVq1ZK/v78aN25sWbuzZs2Sw+FQSkqKZW0CAIoP4wMAlB70+UDxo+AFFKFvvvlGzz//vO68804lJSXp9ddf93ZIJVJaWpqGDx+ue+65R2FhYXI4HPr222+9HRYAeIzxwRrLly9Xv379VK9ePYWGhqpWrVp68sknlZaW5u3QAMCJPt8aq1at0gMPPKDq1asrODhYlSpVUocOHbR69WpvhwYfFeDtAAA7W7Fihfz8/DRjxgwFBgZ6O5wSa9euXRo/frzq1q2rhg0bau3atd4OCQAKhfHBGi+88IJOnTqlRx55RHXr1tW+ffs0bdo0LVy4UMnJyapUqZK3QwQA+nyL7N69W35+fho0aJAqVaqk06dP6+OPP9Zdd92lRYsWqUOHDt4OET6GghdQhI4fP66QkJBSObCdP39eZcuWtaStpk2b6uTJkypfvrw+/fRTPfLII5a0CwDewvhgzfgwefJktWrVSn5+//7SQocOHdSmTRtNmzZNr776qiXPAwCFQZ9vTZ//5JNP6sknn3SZ9/TTT6tWrVqaMmUKBS/kwlcaUWKMHj1aDodDv/76q/r06aPIyEhFRESob9++unDhgiQpJSVFDodDs2bNyrW9w+HQ6NGjc7W3e/du9ejRQxEREYqJidHLL78sY4wOHTqkBx98UOHh4apUqZImTZrkVrwOh0NJSUk6f/68HA5Hrrg+/vhjNW/eXKGhoYqKitJdd92lb775xpNd4/TFF1+oc+fOqlKlioKCglS7dm298sorunLlinOdUaNGqUyZMjpx4kSu7QcMGKDIyEhdunTJOW/JkiVq3bq1ypYtq7CwMHXu3Fnbt2932a5Pnz4qV66c9u7dq06dOiksLEzdu3eXJO3Zs0ddu3ZVpUqVFBwcrGrVqunxxx/X2bNnC5xXWFiYypcv7+7uAFBKMD7cmF3Hh7vuusul2HV1Xvny5bVz584CtwOg5KDPvzG79vl5CQ0NVUxMjM6cOVOodmBPFLxQ4jz66KM6d+6cxo0bp0cffVSzZs3SmDFjPG7vscceU05OjhITExUfH69XX31VU6ZMUfv27VW1alWNHz9ederU0bBhw7Rq1aoCtztnzhy1bt1aQUFBmjNnjubMmaO77rpLkjRmzBj17NlTZcqU0dixYzVmzBhVr15dK1as8DgP6febVpYrV05DhgzRm2++qaZNm2rkyJEaPny4c52ePXsqOztb8+fPd9k2KytLn376qbp27arg4GBnDp07d1a5cuU0fvx4vfzyy9qxY4datWqV68aY2dnZSkhIUGxsrP72t7+pa9euysrKUkJCgtatW6e//vWvevvttzVgwADt27ePQQmA5Rgf8leaxoeMjAxlZGQoOjq6UO0A8G30+fmze5+fnp6u3377Tb/88ov+93//V9u2bVPbtm3dbgelgAFKiFGjRhlJpl+/fi7zu3TpYipUqGCMMWb//v1GkklKSsq1vSQzatSoXO0NGDDAOS87O9tUq1bNOBwOk5iY6Jx/+vRpExISYnr37u1WzL179zZly5Z1mbdnzx7j5+dnunTpYq5cueKyLCcnp8BtJyUlGUlm//79znkXLlzItd7AgQNNaGiouXTpknNeixYtTHx8vMt6n332mZFkVq5caYwx5ty5cyYyMtI89dRTLusdPXrUREREuMzv3bu3kWSGDx/usu7mzZuNJLNgwYIC53UjCxYscIkTABgfXJXW8eGqV155xUgyy5cvt7xtAN5Hn++qNPb5CQkJRpKRZAIDA83AgQPNxYsXLWkb9sIVXihxBg0a5PK4devWOnnypNLT0z1q79rvgfv7+6tZs2Yyxqh///7O+ZGRkapfv7727dvnWdDX+Pzzz5WTk6ORI0fm+hqGw+EoVNshISHOv8+dO6fffvtNrVu31oULF/TLL784l/Xq1Uvr16/X3r17nfPmzp2r6tWrq02bNpKkpUuX6syZM+rWrZt+++035+Tv76/4+HitXLky1/P/13/9l8vjiIgISdLXX3/tvMQcAIoK40P+Ssv4sGrVKo0ZM0aPPvqo7r33XsvaBeB76PPzZ/c+PzExUd98841mzJihO+64Q1lZWcrOzi50u7AfCl4ocW666SaXx1FRUZKk06dPW9JeRESEgoODc30VIiIiwuPnuNbevXvl5+enW265pdBt/dH27dvVpUsXRUREKDw8XDExMerRo4ckuXw//rHHHlNQUJDmzp3rXLZw4UJ1797dOcDu2bNHknTvvfcqJibGZfrmm290/Phxl+cOCAhQtWrVXObVrFlTQ4YM0Ycffqjo6GglJCTo7bffLvR39QEgL4wP+SsN48Mvv/yiLl26qEGDBvrwww89bgdAyUCfnz+79/mNGzdW+/bt1a9fPy1dulQbNmxQnz59PGoL9savNKLE8ff3z3O+MSbfT0OuvUFjQdq73nP4qjNnzqhNmzYKDw/X2LFjVbt2bQUHB2vTpk164YUXlJOT41w3KipK//Ef/6G5c+dq5MiR+vTTT5WZmekcCCU5158zZ06eP+seEODafQQFBeX6dEqSJk2apD59+uiLL77QN998o8GDB2vcuHFat25drsEQAAqD8SFvpWF8OHTokO677z5FRERo8eLFCgsLc2t7ACUPfX7eSkOff63AwEA98MADSkxM1MWLF12ubgMoeMFWrn6y88ebHx44cMAL0eStdu3aysnJ0Y4dO9S4cWPL2v3222918uRJffbZZ84bYUrS/v3781y/V69eevDBB/Xjjz9q7ty5atKkiW699VaXOCUpNjZW7dq1K1RsDRs2VMOGDfXSSy9pzZo1uvPOOzV9+nR+Lh5AsWF8sO/4cPLkSd13333KzMzU8uXLVbly5ULFBKDko8+3b5+fl4sXL8oYo3PnzlHwggu+0ghbCQ8PV3R0dK5fTnnnnXe8FFFuDz30kPz8/DR27FiXT1ikwn1adPUTqGvbyMrKyjf3jh07Kjo6WuPHj9d3333n8kmOJCUkJCg8PFyvv/66Ll++nGv7vH7C+I/S09NzfZ++YcOG8vPzU2Zm5g23BwCrMD7Yc3w4f/68OnXqpCNHjmjx4sWqW7dugbcFYF/0+fbs8//49Unp96LmP//5T1WvXl2xsbEFbgulA1d4wXaefPJJJSYm6sknn1SzZs20atUq7d6929thOdWpU0cvvviiXnnlFbVu3VoPP/ywgoKC9OOPP6pKlSoaN26cR+22bNlSUVFR6t27twYPHiyHw6E5c+bkO2CWKVNGjz/+uKZNmyZ/f39169bNZXl4eLjeffdd9ezZU7fffrsef/xxxcTE6ODBg1q0aJHuvPNOTZs27boxrVixQn/5y1/0yCOPqF69esrOztacOXPk7++vrl27upXf1U9+tm/fLun3y6p/+OEHSdJLL73kVlsASifGB/uND927d9eGDRvUr18/7dy5Uzt37nQuK1eunB566KECtwXAXujz7dfnd+zYUdWqVVN8fLxiY2N18OBBJSUlKTU1VfPnzy9wOyg9KHjBdkaOHKkTJ07o008/1SeffKKOHTtqyZIlPlXxHzt2rGrWrKmpU6fqxRdfVGhoqG677Tb17NnT4zYrVKighQsXaujQoXrppZcUFRWlHj16qG3btkpISMhzm169emnatGlq27Ztnl8BeeKJJ1SlShUlJiZq4sSJyszMVNWqVdW6dWv17dv3hjE1atRICQkJ+te//qUjR44oNDRUjRo10pIlS3THHXe4ld/LL7/s8njmzJnOvyl4ASgIxgf7jQ/JycmSfh8Trh0XJKlGjRoUvIBSjD7ffn1+v379NG/ePL3xxhs6c+aMoqKidMcdd+jvf/+7WrduXeB2UHo4jC/fcQ9AkdqyZYsaN26s2bNnF2pgBQDYC+MDAJQe9PmwK+7hBZRiH3zwgcqVK6eHH37Y26EAAHwI4wMAlB70+bArvtIIeODEiRPX/VnjwMBAlS9f3qO2MzIylJGRcd11YmJi8v2Z5IL417/+pR07duj999/XX/7yF5UtW9bjtgrj7Nmzunjx4nXXyevnjwHAVzE+WIPxAUBJQJ9vDfp8FBW+0gh4IC4u7ro/a9ymTRt9++23HrU9evRojRkz5rrr7N+/X3FxcR61L/0e/7Fjx5SQkKA5c+YoLCzM47YKo0+fPvroo4+uuw5dFICShPHBGowPAEoC+nxr0OejqFDwAjywevXq634KERUVpaZNm3rU9r59+7Rv377rrtOqVSsFBwd71L4v2bFjh1JTU6+7Trt27YopGgAoPMYHazA+ACgJ6POtQZ+PokLBCwAAAAAAALbCTesBAAAAAABgK7a/aX1OTo5SU1MVFhYmh8Ph7XAAAPkwxujcuXOqUqWK/PyK9vMYxgYAKBmsGhvo9wGgZLDyPYHtC16pqamqXr26t8MAABTQoUOHVK1atSJ9DsYGAChZCjs20O8DQMlixXsC2xe8rv7SxKFDhxQeHu7laAAA+UlPT1f16tWL5ReCGBsAoGSwamyg3weAksHK9wS2L3hdvWQ5PDycwQ0ASoDi+KoJYwMAlCyFHRvo9wGgZLHiPQE3rQcAAAAAAICtUPACAAAAAACArVDwAgAAAAAAgK1Q8AIAAAAAAICt2P6m9QAAFJW44YvynJ+S2LmYIwEAAABwLa7wAgAAAAAAgK1Q8AIAAAAAAICtUPACAAAAAACArVDwAgAAAAAAgK1Q8AIAAAAAAICtUPACAAAAAACArVDwAgAAAAAAgK1Q8AIAAAAAAICtUPACAAAAAACArQR4O4CSLm74onyXpSR2LsZIAAAAAAAAIHGFFwAAAAAAAGyGghcAAAAAAABshYIXAAAAAAAAbIWCFwAAAAAAAGyFghcAAAAAAABshYIXAAAAAAAAbIWCFwAAAAAAAGyFghcAAAAAAABshYIXAAAAAAAAbIWCFwAAAAAAAGyFghcAAAAAAABshYIXAAAAAAAAbIWCFwAAAAAAAGyFghcAAAAAAABshYIXAAAAAAAAbIWCFwAAAAAAAGyFghcAAAAAAABsxasFr1WrVun+++9XlSpV5HA49Pnnn7ssN8Zo5MiRqly5skJCQtSuXTvt2bPHO8ECAAAAAACgRPBqwev8+fNq1KiR3n777TyXT5gwQW+99ZamT5+u9evXq2zZskpISNClS5eKOVIAAAAAAACUFAHefPKOHTuqY8eOeS4zxmjKlCl66aWX9OCDD0qSZs+erYoVK+rzzz/X448/XpyhAgAAAAAAoITwasHrevbv36+jR4+qXbt2znkRERGKj4/X2rVr8y14ZWZmKjMz0/k4PT29yGMFAPg2xgYAKF3o9wEAPnvT+qNHj0qSKlas6DK/YsWKzmV5GTdunCIiIpxT9erVizROAIDvY2wAgNKFfh/ILW74IucElAYeFbz27dtndRyWGTFihM6ePeucDh065O2QAABextgAAKUL/T4AwKOvNNapU0dt2rRR//799Z//+Z8KDg62Oi5VqlRJknTs2DFVrlzZOf/YsWNq3LhxvtsFBQUpKCjI8ngAACUXYwMAlC70+wAAj67w2rRpk2677TYNGTJElSpV0sCBA7VhwwZLA6tZs6YqVaqk5cuXO+elp6dr/fr1atGihaXPBQAAAAAAAPvwqODVuHFjvfnmm0pNTdXMmTOVlpamVq1aqUGDBpo8ebJOnDhRoHYyMjKUnJys5ORkSb/fqD45OVkHDx6Uw+HQf//3f+vVV1/Vl19+qZ9//lm9evVSlSpV9NBDD3kSNgAAAAAAAEqBQt20PiAgQA8//LAWLFig8ePH69dff9WwYcNUvXp19erVS2lpadfd/qefflKTJk3UpEkTSdKQIUPUpEkTjRw5UpL0/PPP669//asGDBigP//5z8rIyNBXX31VJF+hBAAAAAAAgD0UquD1008/6emnn1blypU1efJkDRs2THv37tXSpUuVmpqqBx988Lrb33333TLG5JpmzZolSXI4HBo7dqyOHj2qS5cuadmyZapXr15hQgYAAAAAAIDNeXTT+smTJyspKUm7du1Sp06dNHv2bHXq1El+fr/Xz2rWrKlZs2YpLi7OylgBAAAAAACAG/Ko4PXuu++qX79+6tOnj8svKF4rNjZWM2bMKFRwAAAAAAAAgLs8Knjt2bPnhusEBgaqd+/enjQPAAAAAAAAeMyjgldSUpLKlSunRx55xGX+ggULdOHCBQpdAIBSLW74onyXpSR2LsZIAAAAgNLJo5vWjxs3TtHR0bnmx8bG6vXXXy90UAAAAAAAAICnPCp4HTx4UDVr1sw1v0aNGjp48GChgwIAAAAAAAA85VHBKzY2Vlu3bs01f8uWLapQoUKhgwIAAAAAAAA85VHBq1u3bho8eLBWrlypK1eu6MqVK1qxYoWeffZZPf7441bHCAAAAAAAABSYRzetf+WVV5SSkqK2bdsqIOD3JnJyctSrVy/u4QUAAAAAAACv8qjgFRgYqPnz5+uVV17Rli1bFBISooYNG6pGjRpWxwcAAAAAAAC4xaOC11X16tVTvXr1rIoFAAAAAAAAKDSPCl5XrlzRrFmztHz5ch0/flw5OTkuy1esWGFJcAAAAAAAAIC7PCp4Pfvss5o1a5Y6d+6sBg0ayOFwWB0XAAAAAAAA4BGPCl7z5s3TJ598ok6dOlkdDwAAAAAAAFAofp5sFBgYqDp16lgdCwAAAAAAAFBoHhW8hg4dqjfffFPGGKvjAQAAAAAAAArFo680/vDDD1q5cqWWLFmiW2+9VWXKlHFZ/tlnn1kSHAAAAAAAAOAujwpekZGR6tKli9WxAAAAAAAAAIXmUcErKSnJ6jgAAAAAAAAAS3h0Dy9Jys7O1rJly/Tee+/p3LlzkqTU1FRlZGRYFhwAAAAAAADgLo+u8Dpw4IA6dOiggwcPKjMzU+3bt1dYWJjGjx+vzMxMTZ8+3eo4AQAAAAAAgALx6AqvZ599Vs2aNdPp06cVEhLinN+lSxctX77csuAAAAAAAAAAd3l0hdf333+vNWvWKDAw0GV+XFycjhw5YklgAAAAAAAAgCc8usIrJydHV65cyTX/8OHDCgsLK3RQAAAAAAAAgKc8Knjdd999mjJlivOxw+FQRkaGRo0apU6dOlkVGwAAAAAAAOA2j77SOGnSJCUkJOiWW27RpUuX9MQTT2jPnj2Kjo7WP/7xD6tjBAAAAAAAAArMo4JXtWrVtGXLFs2bN09bt25VRkaG+vfvr+7du7vcxB4AAAAAAAAobh4VvCQpICBAPXr0sDIWAAAAAAAAoNA8KnjNnj37ust79erlUTAAAAAAAABAYXlU8Hr22WddHl++fFkXLlxQYGCgQkNDKXgBAAAAAADAazz6lcbTp0+7TBkZGdq1a5datWrFTesBAAAAAADgVR4VvPJSt25dJSYm5rr6CwAAAAAAAChOlhW8pN9vZJ+ammplkwAAAAAAAIBbPLqH15dffuny2BijtLQ0TZs2TXfeeaclgQEAAAAAAACe8Kjg9dBDD7k8djgciomJ0b333qtJkyZZERcAAAAAAADgEY8KXjk5OVbHAQAAAAAAAFjC0nt4AQAAAAAAAN7m0RVeQ4YMKfC6kydP9uQpAAAAAAAAAI94VPDavHmzNm/erMuXL6t+/fqSpN27d8vf31+33367cz2Hw2FNlAAAAAAAAEABeVTwuv/++xUWFqaPPvpIUVFRkqTTp0+rb9++at26tYYOHWppkAAAAAAAAEBBeXQPr0mTJmncuHHOYpckRUVF6dVXX+VXGgEAAAAAAOBVHhW80tPTdeLEiVzzT5w4oXPnzhU6KAAAAAAAAMBTHhW8unTpor59++qzzz7T4cOHdfjwYf3zn/9U//799fDDD1sW3OjRo+VwOFymm2++2bL2AQAAAAAAYD8e3cNr+vTpGjZsmJ544gldvnz594YCAtS/f39NnDjR0gBvvfVWLVu2zPk4IMCjkAEAAAAAAFBKeFQ9Cg0N1TvvvKOJEydq7969kqTatWurbNmylgYn/V7gqlSpkuXtAgAAAAAAwJ4KdblUWlqa0tLSdNdddykkJETGGDkcDqtikyTt2bNHVapUUXBwsFq0aKFx48bppptuynf9zMxMZWZmOh+np6dbGg8AoORhbACA0oV+HwDgUcHr5MmTevTRR7Vy5Uo5HA7t2bNHtWrVUv/+/RUVFWXZLzXGx8dr1qxZql+/vtLS0jRmzBi1bt1a27ZtU1hYWJ7bjBs3TmPGjLHk+a+KG77I0u1SEjsXJhwAgJuKYmwAAPgu+n0AgEc3rf+f//kflSlTRgcPHlRoaKhz/mOPPaavvvrKsuA6duyoRx55RLfddpsSEhK0ePFinTlzRp988km+24wYMUJnz551TocOHbIsHgBAycTYAAClC/0+AMCjK7y++eYbff3116pWrZrL/Lp16+rAgQOWBJaXyMhI1atXT7/++mu+6wQFBSkoKKjIYgAAlDyMDQBQutDvAwA8usLr/PnzLld2XXXq1KkiHVgyMjK0d+9eVa5cucieAwAAAAAAACWbRwWv1q1ba/bs2c7HDodDOTk5mjBhgu655x7Lghs2bJi+++47paSkaM2aNerSpYv8/f3VrVs3y54DAAAAAAAA9uLRVxonTJigtm3b6qefflJWVpaef/55bd++XadOndLq1astC+7w4cPq1q2bTp48qZiYGLVq1Urr1q1TTEyMZc8BAAAAAAAAe/Go4NWgQQPt3r1b06ZNU1hYmDIyMvTwww/rmWeesfTrhvPmzbOsLQAAAAAAAJQObhe8Ll++rA4dOmj69Ol68cUXiyImAAAAAAAAwGNu38OrTJky2rp1a1HEAgAAAAAAABSaRzet79Gjh2bMmGF1LAAAAAAAAECheXQPr+zsbM2cOVPLli1T06ZNVbZsWZflkydPtiQ4AAAAAAAAwF1uFbz27dunuLg4bdu2Tbfffrskaffu3S7rOBwO66IDAAAAAACWihu+yOVxSmLnAi0DShK3Cl5169ZVWlqaVq5cKUl67LHH9NZbb6lixYpFEhwAAAAAAADgLrfu4WWMcXm8ZMkSnT9/3tKAAAAAAAAAgMLw6Kb1V/2xAAYAAAAAAAB4m1sFL4fDkeseXdyzCwAAAAAAAL7ErXt4GWPUp08fBQUFSZIuXbqkQYMG5fqVxs8++8y6CAEAAAAAAAA3uFXw6t27t8vjHj16WBoMAAAAAAAAUFhuFbySkpKKKg4AAAAAAADAEoW6aT0AAAAAAADgayh4AQAAAAAAwFbc+kojrBE3fFG+y1ISOxdjJAAAAAAAAPbDFV4AAAAAAACwFQpeAAAAAAAAsBUKXgAAAAAAALAVCl4AAAAAAACwFQpeAAAAAAAAsBUKXgAAAAAAALAVCl4AAAAAAACwFQpeAAAAAAAAsBUKXgAAAAAAALAVCl4AAAAAAACwlQBvBwCgdIkbvijfZSmJnYsxEsD35Pf/wf8GAAAoStc7Ry/s+ckft/fWec21cVgdg6/kCFdc4QUAAAAAAABboeAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABboeAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABboeAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABbCfB2AChZ4oYvynN+SmJnt7e50XZWxlBSXW/fXU9++8LqY4Gi4QvHyerXHv7N031bXO3lx+pjW1xjg6ftecKTY+Hr+7U4labx3Y5K8msPQPEqbH//x+2Lciz1Vv9V1Dn6guLIkSu8AAAAAAAAYCsUvAAAAAAAAGArFLwAAAAAAABgKxS8AAAAAAAAYCsUvAAAAAAAAGArJaLg9fbbbysuLk7BwcGKj4/Xhg0bvB0SAAAAAAAAfJTPF7zmz5+vIUOGaNSoUdq0aZMaNWqkhIQEHT9+3NuhAQAAAAAAwAf5fMFr8uTJeuqpp9S3b1/dcsstmj59ukJDQzVz5kxvhwYAAAAAAAAfFODtAK4nKytLGzdu1IgRI5zz/Pz81K5dO61duzbPbTIzM5WZmel8fPbsWUlSenq6x3HkZF7weFt3FSbO4pDfvrhe3Nfbf57k60kMJZWnr7389oXVx8ITvhCDr/OFfWT1a8+dbY0xHreRn6IYG6TiGx887WOLKwZPFNfY4Gl7nvDkWPj6fi1OpWl8t6Oieu15OjYUVb8PlGRWnzMU9P+poM/raXuebFfQc6uijqm42vNF+eVo6XsC48OOHDliJJk1a9a4zH/uuedM8+bN89xm1KhRRhITExMTUwmdDh06ZPl4wtjAxMTEVLInd8cG+n0mJiamkj1Z8Z7AYUwRfJRukdTUVFWtWlVr1qxRixYtnPOff/55fffdd1q/fn2ubf74aU5OTo5OnTqlChUqyOFwuB1Denq6qlevrkOHDik8PNyzRHwcOdoDOdpDac7RGKNz586pSpUq8vOz9hv3Vo8NvqI0vF7cwf74N/aFK/aHq5K0PzwdG6zq90vSvvIE+ZVcds5Nsnd+ds5NKnx+Vr4n8OmvNEZHR8vf31/Hjh1zmX/s2DFVqlQpz22CgoIUFBTkMi8yMrLQsYSHh9vyxXgtcrQHcrSH0ppjREREkTxXUY0NvqI0vF7cwf74N/aFK/aHq5KyPzwZG6zu90vKvvIU+ZVcds5Nsnd+ds5NKlx+Vr0n8Omb1gcGBqpp06Zavny5c15OTo6WL1/ucsUXAAAAAAAAcJVPX+ElSUOGDFHv3r3VrFkzNW/eXFOmTNH58+fVt29fb4cGAAAAAAAAH+TzBa/HHntMJ06c0MiRI3X06FE1btxYX331lSpWrFgszx8UFKRRo0bluiTaTsjRHsjRHsgR7mBfumJ//Bv7whX7wxX7o+Dsvq/Ir+Syc26SvfOzc26Sb+Xn0zetBwAAAAAAANzl0/fwAgAAAAAAANxFwQsAAAAAAAC2QsELAAAAAAAAtkLBCwAAAAAAALZCwQsAAAAAAAC2YvuC19tvv624uDgFBwcrPj5eGzZsuO76CxYs0M0336zg4GA1bNhQixcvdlk+evRo3XzzzSpbtqyioqLUrl07rV+/3mWdTZs2qX379oqMjFSFChU0YMAAZWRkWJ7bVVbneK1BgwbJ4XBoypQpLvNPnTql7t27Kzw8XJGRkerfv7/tcnzttdfUsmVLhYaGKjIy0oIsrq+4c0xJSVH//v1Vs2ZNhYSEqHbt2ho1apSysrKsSikXbxzHBx54QDfddJOCg4NVuXJl9ezZU6mpqVakky9v5HlVZmamGjduLIfDoeTk5EJkcX3eyDEuLk4Oh8NlSkxMtCIdrykN/bc7SkM/6I7S0JcUlLf2xaJFixQfH6+QkBBFRUXpoYceKmQm1vDG/ti9e7cefPBBRUdHKzw8XK1atdLKlSutSMdSxX3u/+233+Yam65OP/74o6Tf+5q8lq9bt87n85MKduwPHjyozp07KzQ0VLGxsXruueeUnZ1ti9zyOnbz5s1zKzdv5VeQ96VWHDtfzs+K4+er52tbt25V69atFRwcrOrVq2vChAlu5eXL+VnWbxobmzdvngkMDDQzZ84027dvN0899ZSJjIw0x44dy3P91atXG39/fzNhwgSzY8cO89JLL5kyZcqYn3/+2bnO3LlzzdKlS83evXvNtm3bTP/+/U14eLg5fvy4McaYI0eOmKioKDNo0CDzyy+/mA0bNpiWLVuarl27lpgcr/rss89Mo0aNTJUqVcwbb7zhsqxDhw6mUaNGZt26deb77783derUMd26dSuKFL2W48iRI83kyZPNkCFDTERERBFk9m/eyHHJkiWmT58+5uuvvzZ79+41X3zxhYmNjTVDhw61TY7GGDN58mSzdu1ak5KSYlavXm1atGhhWrRoURQpGmO8l+dVgwcPNh07djSSzObNmy3M7N+8lWONGjXM2LFjTVpamnPKyMgoihSLRWnov91RGvpBd5SGvqSgvLUvPv30UxMVFWXeffdds2vXLrN9+3Yzf/78okjRLd7aH3Xr1jWdOnUyW7ZsMbt37zZPP/20CQ0NNWlpaUWRpke8ce6fmZnpMi6lpaWZJ5980tSsWdPk5OQYY4zZv3+/kWSWLVvmsl5WVpbP52fMjY99dna2adCggWnXrp3ZvHmzWbx4sYmOjjYjRowo8bkZY4wkk5SU5HLsLl68WODcvJVfQd6XWnHsfDk/Ywp//Hz1fO3s2bOmYsWKpnv37mbbtm3mH//4hwkJCTHvvfdegXPz5fys6jdtXfBq3ry5eeaZZ5yPr1y5YqpUqWLGjRuX5/qPPvqo6dy5s8u8+Ph4M3DgwHyf4+zZs84DYYwx7733nomNjTVXrlxxrrN161YjyezZs6cw6eSpqHI8fPiwqVq1qtm2bZupUaOGywt0x44dRpL58ccfnfOWLFliHA6HOXLkiAVZufJGjtdKSkoq8oKXt3O8asKECaZmzZqeJXEDvpLjF198YRwOh9udZUF5M8/Fixebm2++2Wzfvr1I36R6K8eCHN+SpDT03+7wlT6iKPtBd5SGvqSgvLEvLl++bKpWrWo+/PBD6xKxiDf2x4kTJ4wks2rVKue89PR0I8ksXbrUgqys4Y1z/z/KysoyMTExZuzYsc55V9+4FfZ/yRv5FeTYL1682Pj5+ZmjR48613n33XdNeHi4yczMLNG5GfN7weT//u//CpRHfnz1fakVx86X8zOm8MfPV8/X3nnnHRMVFeVynF544QVTv359W+RnVb9p2680ZmVlaePGjWrXrp1znp+fn9q1a6e1a9fmuc3atWtd1pekhISEfNfPysrS+++/r4iICDVq1EjS718BCAwMlJ/fv3dtSEiIJOmHH34oVE55PX9R5JiTk6OePXvqueee06233ppnG5GRkWrWrJlzXrt27eTn55frMtPC8laOxcmXcjx79qzKly/vQRbX5ys5njp1SnPnzlXLli1VpkwZD7PJnzfzPHbsmJ566inNmTNHoaGhFmSTN28fy8TERFWoUEFNmjTRxIkTPbrk3heUhv7bHd5+XV2rqPpBd5SGvqSgvLUvNm3apCNHjsjPz09NmjRR5cqV1bFjR23bts2izDzjrf1RoUIF1a9fX7Nnz9b58+eVnZ2t9957T7GxsWratKlF2RWOt879/+jLL7/UyZMn1bdv31zLHnjgAcXGxqpVq1b68ssvC5qa87m9kV9Bjv3atWvVsGFDVaxY0eV50tPTtX379hKd21XPPPOMoqOj1bx5c82cOVPGmBvm5e38CvK+tLDHztfzu8rT4+fL52tr167VXXfdpcDAQJfn2bVrl06fPl3i87uqMP2mZON7eP3222+6cuWKyz+vJFWsWFFHjx7Nc5ujR48WaP2FCxeqXLlyCg4O1htvvKGlS5cqOjpaknTvvffq6NGjmjhxorKysnT69GkNHz5ckpSWlmZVepKKLsfx48crICBAgwcPzreN2NhYl3kBAQEqX758vs/rKW/lWJx8Jcdff/1VU6dO1cCBA93M4Ma8neMLL7ygsmXLqkKFCjp48KC++OILDzO5Pm/laYxRnz59NGjQIJeBoyh481gOHjxY8+bN08qVKzVw4EC9/vrrev755wuRjfeUhv7bHd7uI64qyn7QHaWhLykob+2Lffv2Sfr9HjIvvfSSFi5cqKioKN199906depUYVIqFG/tD4fDoWXLlmnz5s0KCwtTcHCwJk+erK+++kpRUVGFzMoa3jr3/6MZM2YoISFB1apVc84rV66cJk2apAULFmjRokVq1aqVHnroIbfevHkrv4Ic+/ye5+qykpybJI0dO1affPKJli5dqq5du+rpp5/W1KlTb5iXt/MryPvSwh47X89PKtzx8+XzNV8+dlbkZ0W/KUkBbq0NSdI999yj5ORk/fbbb/rggw/06KOPav369YqNjdWtt96qjz76SEOGDNGIESPk7++vwYMHq2LFii7VZ1+1ceNGvfnmm9q0aZMcDoe3wykS5JjbkSNH1KFDBz3yyCN66qmniiHCwnMnx+eee079+/fXgQMHNGbMGPXq1UsLFy4sEce/IHlOnTpV586d04gRI4o5OmsU9FgOGTLE+fdtt92mwMBADRw4UOPGjVNQUFBxhOrTSkPf5o7S0A+6ozT0JQVVkH2Rk5MjSXrxxRfVtWtXSVJSUpKqVaumBQsWeL0oaqWC7A9jjJ555hnFxsbq+++/V0hIiD788EPdf//9+vHHH1W5cuVijrp4Xe/c/1qHDx/W119/rU8++cRlfnR0tMsY9uc//1mpqamaOHGiHnjggWLJ4Xqul19JP/ZW5Pbyyy8722vSpInOnz+viRMn+sQH53Z+XypZk5+vHT+7n69ZlZ9V/WbJeKV7IDo6Wv7+/jp27JjL/GPHjqlSpUp5blOpUqUCrV+2bFnVqVNHd9xxh2bMmKGAgADNmDHDufyJJ57Q0aNHdeTIEZ08eVKjR4/WiRMnVKtWLYuy+11R5Pj999/r+PHjuummmxQQEKCAgAAdOHBAQ4cOVVxcnLON48ePu7SRnZ2tU6dO5fu8nvJWjsXJ2zmmpqbqnnvuUcuWLfX+++9bl9g1vJ1jdHS06tWrp/bt22vevHlavHixR7+MdCPeynPFihVau3atgoKCFBAQoDp16kiSmjVrpt69e9six7zEx8crOztbKSkphcrJG0pD/+0Ob7+uiqMfdEdp6EsKylv74uob3VtuucXZRlBQkGrVqqWDBw9alZ7bvPnaWLhwoebNm6c777xTt99+u9555x2FhIToo48+sj5RD3jz3P+qpKQkVahQoUBvxuLj4/Xrr7/ecL2rvJVfQY59fs9zdVlJzi0v8fHxOnz4sDIzM2+Ymzfzk278vrSwx87X88uLO8fPl8/XfPXYFeX5qLv9pmTjgldgYKCaNm2q5cuXO+fl5ORo+fLlatGiRZ7btGjRwmV9SVq6dGm+61/bbl7/MBUrVlS5cuU0f/58BQcHq3379h5kkr+iyLFnz57aunWrkpOTnVOVKlX03HPP6euvv3a2cebMGW3cuNHZxooVK5STk6P4+Hhb5FicvJnjkSNHdPfdd6tp06ZKSkoqsk97fOk4Xv3UvqAnKe7wVp5vvfWWtmzZ4lx+9aeB58+fr9dee80WOeYlOTlZfn5+uT5hLwlKQ//tjtLQD7qjNPQlBeWtfdG0aVMFBQVp165dzjYuX76slJQU1ahRw+o0C8xb++PChQuSlOv/w8/Pzzmuepu3z/2NMUpKSlKvXr0KdJ/Q5ORkt66O8lZ+BTn2LVq00M8//+zyBnbp0qUKDw93KRqXxNzykpycrKioqAJfXe7t16aU//vSwh47X88vL+4cP18+X2vRooVWrVqly5cvuzxP/fr1C/xVc1/OLy/u9puSZOtfaZw3b54JCgoys2bNMjt27DADBgwwkZGRzl+h6Nmzpxk+fLhz/dWrV5uAgADzt7/9zezcudOMGjXK5Sc2MzIyzIgRI8zatWtNSkqK+emnn0zfvn1NUFCQ2bZtm7OdqVOnmo0bN5pdu3aZadOmmZCQEPPmm2+WiBzzktevOHXo0ME0adLErF+/3vzwww+mbt26Rfaz9t7K8cCBA2bz5s1mzJgxply5cmbz5s1m8+bN5ty5c7bI8fDhw6ZOnTqmbdu25vDhwy4/91oUvJHjunXrzNSpU83mzZtNSkqKWb58uWnZsqWpXbu2uXTpkm3y/COrftUkP97Icc2aNeaNN94wycnJZu/evebjjz82MTExplevXkWSY3EoDf23O0pDP+iO0tCXFJS39sWzzz5rqlatar7++mvzyy+/mP79+5vY2Fhz6tSpIsmzoLyxP06cOGEqVKhgHn74YZOcnGx27dplhg0bZsqUKWOSk5OLLFd3eevc3xhjli1bZiSZnTt35opr1qxZ5u9//7vZuXOn2blzp3nttdeMn5+fmTlzps/nV5Bjn52dbRo0aGDuu+8+k5ycbL766isTExNjRowYUeJz+/LLL80HH3xgfv75Z7Nnzx7zzjvvmNDQUDNy5EifP3bG3Ph9qRXHzpfzs+L4+er52pkzZ0zFihVNz549zbZt28y8efNMaGioee+99wqcmy/nZ1W/aeuClzG//xPcdNNNJjAw0DRv3tysW7fOuaxNmzamd+/eLut/8sknpl69eiYwMNDceuutZtGiRc5lFy9eNF26dDFVqlQxgYGBpnLlyuaBBx4wGzZscGmjZ8+epnz58iYwMNDcdtttZvbs2SUmx7zk9QI9efKk6datmylXrpwJDw83ffv2LZJC0FXeyLF3795GUq5p5cqVFmXlqrhzTEpKyjO/oqyDF3eOW7duNffcc48pX768CQoKMnFxcWbQoEHm8OHDVqaVizder9cqjjepxZ3jxo0bTXx8vImIiDDBwcHmT3/6k3n99deLrHBZXEpD/+2O0tAPuqM09CUF5Y19kZWVZYYOHWpiY2NNWFiYadeuXa4ih7d4Y3/8+OOP5r777jPly5c3YWFh5o477jCLFy+2KiXLeOPc3xhjunXrZlq2bJlnTLNmzTJ/+tOfTGhoqAkPDzfNmzc3CxYsKDH5FeTYp6SkmI4dO5qQkBATHR1thg4dai5fvlzic1uyZIlp3LixKVeunClbtqxp1KiRmT59urly5YpbuXkrv4K8L7Xi2PlqflYdP189X9uyZYtp1aqVCQoKMlWrVjWJiYlu5eXL+VnVbzqMceM3VQEAAAAAAAAf5/0bVQAAAAAAAAAWouAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABboeAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABboeAFAAAAAAAAW6HgBQAAAAAAAFuh4AUAAAAAAABboeAFAAAAAAAAW/l/3A4VjyVWnyYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_desc = sorted_results[\"mean_accuracy\"].astype(\"float32\").describe()\n",
    "xlimit_range = [\n",
    "    accuracy_desc[\"min\"] - accuracy_desc[\"std\"],\n",
    "    accuracy_desc[\"max\"] + accuracy_desc[\"std\"],\n",
    "]\n",
    "for hperparameter_name in turning_parameters:\n",
    "    parameter_group = sorted_results.groupby(hperparameter_name)\n",
    "    fix, axs = pyplot.subplots(\n",
    "        1,\n",
    "        len(parameter_group),\n",
    "        layout=\"constrained\",\n",
    "        sharex=False,\n",
    "        sharey=True,\n",
    "        figsize=(12, 2),\n",
    "    )\n",
    "    for i, g in enumerate(parameter_group):\n",
    "        g[1][\"mean_accuracy\"].astype(\"float32\").plot(\n",
    "            kind=\"hist\", bins=50, subplots=True, sharex=False, sharey=True, ax=axs[i]\n",
    "        )\n",
    "        axs[i].set_title(f\"{hperparameter_name}_{g[0]}\")\n",
    "\n",
    "pyplot.xlim(xlimit_range)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                          50\n",
      "mse_score          0.03178376515414065\n",
      "trial_id                   bf758_00050\n",
      "return_period                        3\n",
      "seq_len                              3\n",
      "lr                                0.01\n",
      "momentum           0.41067475256907493\n",
      "optim_type                           2\n",
      "num_layers                           4\n",
      "hidden_size                         32\n",
      "num_fc_layers                        2\n",
      "activation_type                      3\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sorted_results_file = f\"{task_name}_sorted_results.csv\"\n",
    "sorted_results = pd.read_csv(sorted_results_file, dtype=\"str\")\n",
    "best_config = sorted_results.loc[0]\n",
    "print(best_config)\n",
    "# id_str_of_best = f\"5_5_0.01_{best_config.momentum}_{best_config.optim_type}_{best_config.num_layers}_{best_config.hidden_size}_{best_config.num_fc_layers}_{best_config.activation_type}\"\n",
    "# best_model_name = f\"/mnt/AIWorkSpace/work/fin-ml/runs/{_TARGET_STK}/{time_str}/{id_str_of_best}.pt\"\n",
    "# print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'StockPCTLabelPredictLSTM.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.precision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mget_filename_of_ipynb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m train_loader, test_loader, features_size \u001b[38;5;241m=\u001b[39m prepare_dataloader(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_period\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(file_path):\n\u001b[0;32m---> 73\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     hyper_parameters \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyper_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     75\u001b[0m     model \u001b[38;5;241m=\u001b[39m StockPCTLabelPredictLSTM(\n\u001b[1;32m     76\u001b[0m         input_size\u001b[38;5;241m=\u001b[39mdata_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     77\u001b[0m         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(hyper_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         activation_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(hyper_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     81\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/ml4t/lib/python3.8/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/venv/ml4t/lib/python3.8/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/venv/ml4t/lib/python3.8/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'StockPCTLabelPredictLSTM.pt'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "model, config = load_model(f\"{task_name}.pt\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader, features_size = prepare_dataloader(config[\"return_period\"])\n",
    "model.eval()\n",
    "\n",
    "(trainAccuracy, trainF1) = eval_dl_method(model, train_loader, device=device)\n",
    "(testAccuracy, testF1) = eval_dl_method(model, test_loader, device=device)\n",
    "print(f\"Train Accuracy: {trainAccuracy:.2f}\\nTest Accuracy: {testAccuracy:.5f}\")\n",
    "print(f\"Train F1: {trainF1:.2f}\\nTest F1: {testF1:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
