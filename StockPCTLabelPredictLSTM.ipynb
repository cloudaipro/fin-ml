{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockPCTLabelPredictLSTM\n",
      "/mnt/AIWorkSpace/work/fin-ml/data/\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#logging\n",
    "from util.logconf import logging\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "log.setLevel(logging.INFO)\n",
    "# log.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%run 'nb_utils.ipynb'\n",
    "task_name = get_filename_of_ipynb()\n",
    "print(task_name)\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/{task_name}'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')\n",
    "\n",
    "return_period = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read /mnt/AIWorkSpace/work/fin-ml/data/AAPL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MSFT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMZN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NVDA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOGL.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TSLA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/META.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/GOOG.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADBE.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/NFLX.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CSCO.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTC.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/INTU.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/CMCSA.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/TXN.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMAT.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/ADSK.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/AMD.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/QCOM.csv completely!\n",
      "read /mnt/AIWorkSpace/work/fin-ml/data/MU.csv completely!\n",
      "[              Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  19.846  19.894  19.715  19.755     17.319  234684800\n",
      "2014-01-03  19.745  19.775  19.301  19.321     16.938  392467600\n",
      "2014-01-06  19.195  19.529  19.057  19.426     17.031  412610800\n",
      "2014-01-07  19.440  19.499  19.211  19.287     16.909  317209200\n",
      "2014-01-08  19.243  19.484  19.239  19.409     17.016  258529600\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 195.180 195.410 192.970 193.600    193.600   37122800\n",
      "2023-12-26 193.610 193.890 192.830 193.050    193.050   28919300\n",
      "2023-12-27 192.490 193.500 191.090 193.150    193.150   48087700\n",
      "2023-12-28 194.140 194.660 193.170 193.580    193.580   34049900\n",
      "2023-12-29 193.900 194.400 191.730 192.530    192.530   42628800\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  37.350  37.400  37.100  37.160     31.291  30632200\n",
      "2014-01-03  37.200  37.220  36.600  36.910     31.080  31134800\n",
      "2014-01-06  36.850  36.890  36.110  36.130     30.423  43603700\n",
      "2014-01-07  36.330  36.490  36.210  36.410     30.659  35802800\n",
      "2014-01-08  36.000  36.140  35.580  35.760     30.112  59971700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 373.680 375.180 372.710 374.580    374.580  17091100\n",
      "2023-12-26 375.000 376.940 373.500 374.660    374.660  12673100\n",
      "2023-12-27 373.690 375.060 372.810 374.070    374.070  14905400\n",
      "2023-12-28 375.370 376.460 374.160 375.280    375.280  14327000\n",
      "2023-12-29 376.000 377.160 373.480 376.040    376.040  18723000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  19.940  19.968  19.701  19.899     19.899  42756000\n",
      "2014-01-03  19.914  20.135  19.811  19.822     19.822  44204000\n",
      "2014-01-06  19.792  19.850  19.421  19.681     19.681  63412000\n",
      "2014-01-07  19.752  19.924  19.715  19.902     19.902  38320000\n",
      "2014-01-08  19.924  20.150  19.802  20.096     20.096  46330000\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 153.770 154.350 152.710 153.420    153.420  29480100\n",
      "2023-12-26 153.560 153.980 153.030 153.410    153.410  25067200\n",
      "2023-12-27 153.560 154.780 153.120 153.340    153.340  31434700\n",
      "2023-12-28 153.720 154.080 152.950 153.380    153.380  27057000\n",
      "2023-12-29 153.100 153.890 151.030 151.940    151.940  39789000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.980   3.995   3.930   3.965      3.741  26009200\n",
      "2014-01-03   3.973   3.980   3.905   3.918      3.696  25933200\n",
      "2014-01-06   3.957   4.000   3.920   3.970      3.745  40949200\n",
      "2014-01-07   4.010   4.050   3.983   4.035      3.807  33328800\n",
      "2014-01-08   4.050   4.110   4.035   4.090      3.859  30819200\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 491.950 493.830 484.670 488.300    488.300  25213900\n",
      "2023-12-26 489.680 496.000 489.600 492.790    492.790  24420000\n",
      "2023-12-27 495.110 496.800 490.850 494.170    494.170  23364800\n",
      "2023-12-28 496.430 498.840 494.120 495.220    495.220  24658700\n",
      "2023-12-29 498.130 499.970 487.510 495.220    495.220  38869000\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.914  27.972  27.734  27.856     27.856   72783144\n",
      "2014-01-03  27.903  27.951  27.651  27.653     27.653   66601332\n",
      "2014-01-06  27.853  27.999  27.689  27.961     27.961   70701228\n",
      "2014-01-07  28.153  28.521  28.057  28.500     28.500  102001896\n",
      "2014-01-08  28.679  28.712  28.361  28.559     28.559   89610300\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 140.770 141.990 140.710 141.490    141.490   26514600\n",
      "2023-12-26 141.590 142.680 141.190 141.520    141.520   16780300\n",
      "2023-12-27 141.590 142.080 139.890 140.370    140.370   19628600\n",
      "2023-12-28 140.780 141.140 139.750 140.230    140.230   16045700\n",
      "2023-12-29 139.630 140.360 138.780 139.690    139.690   18727200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02   9.987  10.165   9.770  10.007     10.007   92826000\n",
      "2014-01-03  10.000  10.146   9.907   9.971      9.971   70425000\n",
      "2014-01-06  10.000  10.027   9.683   9.800      9.800   80416500\n",
      "2014-01-07   9.841  10.027   9.683   9.957      9.957   75511500\n",
      "2014-01-08   9.923  10.247   9.917  10.085     10.085   92448000\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 256.760 258.220 251.370 252.540    252.540   93249800\n",
      "2023-12-26 254.490 257.970 252.910 256.610    256.610   86892400\n",
      "2023-12-27 258.350 263.340 257.520 261.440    261.440  106494400\n",
      "2023-12-28 263.660 265.130 252.710 253.180    253.180  113619900\n",
      "2023-12-29 255.100 255.190 247.430 248.480    248.480  100615300\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  54.830  55.220  54.190  54.710     54.710  43195500\n",
      "2014-01-03  55.020  55.650  54.530  54.560     54.560  38246200\n",
      "2014-01-06  54.420  57.260  54.050  57.200     57.200  68852600\n",
      "2014-01-07  57.700  58.550  57.220  57.920     57.920  77207400\n",
      "2014-01-08  57.600  58.410  57.230  58.230     58.230  56682400\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 355.580 357.200 351.220 353.390    353.390  11764200\n",
      "2023-12-26 354.990 356.980 353.450 354.830    354.830   9898600\n",
      "2023-12-27 356.070 359.000 355.310 357.830    357.830  13207900\n",
      "2023-12-28 359.700 361.900 357.810 358.320    358.320  11798800\n",
      "2023-12-29 358.990 360.000 351.820 353.960    353.960  14980500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  27.782  27.839  27.603  27.724     27.724   73129082\n",
      "2014-01-03  27.771  27.819  27.520  27.522     27.522   66917888\n",
      "2014-01-06  27.721  27.867  27.558  27.829     27.829   71037271\n",
      "2014-01-07  28.020  28.386  27.924  28.365     28.365  102486711\n",
      "2014-01-08  28.543  28.576  28.226  28.424     28.424   90036218\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 142.130 143.250 142.055 142.720    142.720   18494700\n",
      "2023-12-26 142.980 143.945 142.500 142.820    142.820   11170100\n",
      "2023-12-27 142.830 143.320 141.051 141.440    141.440   17288400\n",
      "2023-12-28 141.850 142.270 140.828 141.280    141.280   12192500\n",
      "2023-12-29 140.680 141.435 139.900 140.930    140.930   14872700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  59.060  59.530  58.940  59.290     59.290  2745900\n",
      "2014-01-03  59.190  59.690  59.110  59.160     59.160  1589000\n",
      "2014-01-06  58.060  58.770  58.010  58.120     58.120  3753600\n",
      "2014-01-07  58.260  59.050  58.060  58.970     58.970  2963600\n",
      "2014-01-08  59.120  59.280  58.460  58.900     58.900  3456000\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 600.800 601.860 596.000 598.750    598.750  1659800\n",
      "2023-12-26 598.920 601.690 596.500 598.260    598.260  1595100\n",
      "2023-12-27 598.600 599.790 593.710 596.080    596.080  1394900\n",
      "2023-12-28 597.440 599.040 593.630 595.520    595.520  1702600\n",
      "2023-12-29 596.090 600.750 592.940 596.600    596.600  1893900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  52.401  52.511  51.543  51.831     51.831  12325600\n",
      "2014-01-03  52.000  52.496  51.843  51.871     51.871  10817100\n",
      "2014-01-06  51.890  52.044  50.476  51.367     51.367  15501500\n",
      "2014-01-07  49.684  49.699  48.153  48.500     48.500  36167600\n",
      "2014-01-08  48.104  49.426  48.074  48.713     48.713  20001100\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 494.000 496.020 485.450 486.760    486.760   2701100\n",
      "2023-12-26 489.390 491.480 486.380 491.190    491.190   2034500\n",
      "2023-12-27 491.240 494.020 489.250 491.790    491.790   2561300\n",
      "2023-12-28 492.000 492.890 489.070 490.510    490.510   1710500\n",
      "2023-12-29 490.370 492.230 481.940 486.880    486.880   2739500\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 22.170 22.290 21.910 22.000     16.095  44377000\n",
      "2014-01-03 22.090 22.120 21.830 21.980     16.080  36328200\n",
      "2014-01-06 21.960 22.230 21.930 22.010     16.102  34150300\n",
      "2014-01-07 22.260 22.410 22.150 22.310     16.322  37368800\n",
      "2014-01-08 22.290 22.360 22.150 22.290     16.307  38362700\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 49.840 50.390 49.840 50.090     49.703  12900700\n",
      "2023-12-26 50.110 50.400 50.050 50.280     49.892   9721200\n",
      "2023-12-27 50.300 50.560 50.280 50.440     50.051  10414300\n",
      "2023-12-28 50.580 50.630 50.420 50.480     50.090   8549900\n",
      "2023-12-29 50.450 50.590 50.220 50.520     50.130  12491200\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.780 25.820 25.470 25.790     19.436  31833300\n",
      "2014-01-03 25.860 25.900 25.600 25.780     19.428  27796700\n",
      "2014-01-06 25.770 25.790 25.450 25.460     19.187  28682300\n",
      "2014-01-07 25.540 25.730 25.470 25.590     19.285  19665100\n",
      "2014-01-08 25.640 25.710 25.300 25.430     19.164  29680500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 47.250 48.160 47.200 48.000     48.000  30053700\n",
      "2023-12-26 48.920 50.520 48.710 50.500     50.500  60287400\n",
      "2023-12-27 50.630 51.280 50.190 50.760     50.760  52148000\n",
      "2023-12-28 50.810 50.870 50.160 50.390     50.390  27705200\n",
      "2023-12-29 50.300 50.570 49.770 50.250     50.250  29266500\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  76.110  76.160  75.400  75.940     69.476  1355700\n",
      "2014-01-03  75.750  76.380  75.530  75.800     69.348   998800\n",
      "2014-01-06  75.800  76.000  75.470  75.740     69.293  1268700\n",
      "2014-01-07  75.880  77.350  75.740  77.060     70.501  1551000\n",
      "2014-01-08  76.890  77.050  76.010  76.440     70.107  2437600\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 622.830 625.150 617.680 624.070    623.131   820800\n",
      "2023-12-26 625.170 628.330 622.730 624.850    623.910   638300\n",
      "2023-12-27 623.990 629.800 622.260 629.120    628.174   734400\n",
      "2023-12-28 630.740 631.070 627.180 628.020    627.075   680700\n",
      "2023-12-29 628.020 630.830 622.460 625.030    624.090   724300\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 25.900 25.950 25.635 25.725     20.931  19522400\n",
      "2014-01-03 25.815 25.855 25.425 25.535     20.777  13371400\n",
      "2014-01-06 25.565 25.810 25.335 25.510     20.756  17987800\n",
      "2014-01-07 25.670 26.600 25.555 26.415     21.493  37161400\n",
      "2014-01-08 26.345 26.725 26.260 26.375     21.460  29731600\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 44.130 44.620 43.810 44.000     43.709  11893900\n",
      "2023-12-26 44.000 44.070 43.500 43.930     43.639   9624300\n",
      "2023-12-27 43.900 44.170 43.710 43.990     43.699   9253800\n",
      "2023-12-28 43.970 44.410 43.890 44.120     43.828   9023400\n",
      "2023-12-29 44.090 44.140 43.560 43.850     43.560  13694900\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  43.440  43.500  42.880  43.100     32.954  6959200\n",
      "2014-01-03  43.120  43.460  42.970  43.290     33.100  4693300\n",
      "2014-01-06  43.250  43.280  42.850  42.930     32.824  4446300\n",
      "2014-01-07  42.980  43.110  42.640  42.700     32.648  5078900\n",
      "2014-01-08  42.960  43.320  42.620  43.290     33.100  6353500\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 167.260 168.920 166.820 168.240    168.240  3492400\n",
      "2023-12-26 168.940 171.530 168.450 170.810    170.810  3202200\n",
      "2023-12-27 171.220 171.620 170.330 171.230    171.230  3264900\n",
      "2023-12-28 172.000 172.310 170.710 171.720    171.720  3023000\n",
      "2023-12-29 171.540 171.700 169.920 170.460    170.460  2920600\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  17.690  17.690  17.370  17.550     15.280  7785900\n",
      "2014-01-03  17.540  17.700  17.470  17.510     15.245  6773000\n",
      "2014-01-06  17.500  17.510  17.220  17.290     15.053  9975500\n",
      "2014-01-07  17.370  17.430  17.260  17.370     15.123  8133200\n",
      "2014-01-08  17.400  17.450  17.180  17.420     15.166  8026100\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 161.600 163.000 160.840 162.050    162.050  2770600\n",
      "2023-12-26 162.300 164.970 162.100 164.280    164.280  2520500\n",
      "2023-12-27 164.540 164.990 163.530 164.210    164.210  3319600\n",
      "2023-12-28 165.000 165.010 162.850 163.120    163.120  2909700\n",
      "2023-12-29 163.110 163.560 160.700 162.070    162.070  2980700\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close   Volume\n",
      "Date                                                          \n",
      "2014-01-02  49.330  49.740  48.880  49.250     49.250  2488000\n",
      "2014-01-03  49.110  49.500  48.780  48.900     48.900  1934200\n",
      "2014-01-06  48.980  49.290  48.290  48.550     48.550  1856500\n",
      "2014-01-07  48.890  50.100  48.630  49.680     49.680  2002500\n",
      "2014-01-08  49.500  50.550  49.050  50.240     50.240  2047400\n",
      "...            ...     ...     ...     ...        ...      ...\n",
      "2023-12-22 243.740 244.030 240.310 242.760    242.760   719400\n",
      "2023-12-26 242.490 245.360 241.960 245.070    245.070   595000\n",
      "2023-12-27 245.360 245.880 244.380 245.110    245.110   771900\n",
      "2023-12-28 245.630 245.850 244.020 244.910    244.910   537200\n",
      "2023-12-29 243.720 245.400 242.790 243.480    243.480   721400\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02   3.850   3.980   3.840   3.950      3.950  20548400\n",
      "2014-01-03   3.980   4.000   3.880   4.000      4.000  22887200\n",
      "2014-01-06   4.010   4.180   3.990   4.130      4.130  42398300\n",
      "2014-01-07   4.190   4.250   4.110   4.180      4.180  42932100\n",
      "2014-01-08   4.230   4.260   4.140   4.180      4.180  30678700\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 140.480 140.700 138.310 139.600    139.600  35370400\n",
      "2023-12-26 140.070 143.850 139.920 143.410    143.410  47157400\n",
      "2023-12-27 144.720 146.250 143.180 146.070    146.070  49033400\n",
      "2023-12-28 146.800 150.410 145.950 148.760    148.760  63800700\n",
      "2023-12-29 149.500 151.050 147.200 147.410    147.410  62028200\n",
      "\n",
      "[2516 rows x 6 columns],               Open    High     Low   Close  Adj Close    Volume\n",
      "Date                                                           \n",
      "2014-01-02  73.610  73.770  73.260  73.320     54.740  10110200\n",
      "2014-01-03  73.330  73.480  72.440  72.890     54.419   7970400\n",
      "2014-01-06  73.080  73.200  72.550  72.700     54.277   7696200\n",
      "2014-01-07  72.800  73.310  72.600  73.240     54.680   5902700\n",
      "2014-01-08  73.150  73.680  72.680  73.680     55.008   8976900\n",
      "...            ...     ...     ...     ...        ...       ...\n",
      "2023-12-22 143.250 144.400 142.750 143.490    143.490   4658300\n",
      "2023-12-26 144.170 146.050 143.960 145.460    145.460   4381200\n",
      "2023-12-27 145.850 146.220 145.040 145.720    145.720   4470300\n",
      "2023-12-28 146.180 146.890 145.730 145.860    145.860   4928800\n",
      "2023-12-29 145.410 145.620 143.790 144.630    144.630   4838400\n",
      "\n",
      "[2516 rows x 6 columns],              Open   High    Low  Close  Adj Close    Volume\n",
      "Date                                                       \n",
      "2014-01-02 21.680 21.790 21.270 21.660     21.293  26413500\n",
      "2014-01-03 21.200 21.430 20.900 20.970     20.615  34590200\n",
      "2014-01-06 20.970 20.970 20.640 20.670     20.320  38180500\n",
      "2014-01-07 20.890 21.940 20.890 21.730     21.362  67904500\n",
      "2014-01-08 24.200 24.500 23.560 23.870     23.466  93499500\n",
      "...           ...    ...    ...    ...        ...       ...\n",
      "2023-12-22 86.150 87.490 85.620 86.490     86.374  22519000\n",
      "2023-12-26 86.700 87.870 86.430 87.060     86.944  11203900\n",
      "2023-12-27 87.480 87.490 86.220 86.660     86.544   9186300\n",
      "2023-12-28 86.750 86.750 85.840 86.000     85.885   9606200\n",
      "2023-12-29 85.840 86.140 85.030 85.340     85.340   8546000\n",
      "\n",
      "[2516 rows x 6 columns]]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "\n",
    "# Loading the data\n",
    "stk_tickers = [\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"AMZN\",\n",
    "    \"NVDA\",\n",
    "    \"GOOGL\",\n",
    "    \"TSLA\",\n",
    "    \"META\",\n",
    "    \"GOOG\",\n",
    "    \"ADBE\",\n",
    "    \"NFLX\",\n",
    "    \"CSCO\",\n",
    "    \"INTC\",\n",
    "    \"INTU\",\n",
    "    \"CMCSA\",\n",
    "    \"TXN\",\n",
    "    \"AMAT\",\n",
    "    \"ADSK\",\n",
    "    \"AMD\",\n",
    "    \"QCOM\",\n",
    "    \"MU\",\n",
    "]\n",
    "\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for stk_symbol in stk_tickers:\n",
    "    stk_file = f\"{data_dir}{stk_symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index(\"Date\")\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([stk_symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {stk_symbol} from yfin and write to {stk_file} completely!\")\n",
    "    ticks_data.append(_stk_data)\n",
    "\n",
    "print(ticks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name:cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device_name = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_name)\n",
    "seq_len = 5\n",
    "validation_size = 0.2\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "pin_memory = False  # True\n",
    "shuffle = False  # True\n",
    "print(f\"device_name:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open    High     Low   Close  Adj Close     Volume\n",
      "Date                                                            \n",
      "2014-01-02  19.846  19.894  19.715  19.755     17.319  234684800\n",
      "2014-01-03  19.745  19.775  19.301  19.321     16.938  392467600\n",
      "2014-01-06  19.195  19.529  19.057  19.426     17.031  412610800\n",
      "2014-01-07  19.440  19.499  19.211  19.287     16.909  317209200\n",
      "2014-01-08  19.243  19.484  19.239  19.409     17.016  258529600\n",
      "...            ...     ...     ...     ...        ...        ...\n",
      "2023-12-22 195.180 195.410 192.970 193.600    193.600   37122800\n",
      "2023-12-26 193.610 193.890 192.830 193.050    193.050   28919300\n",
      "2023-12-27 192.490 193.500 191.090 193.150    193.150   48087700\n",
      "2023-12-28 194.140 194.660 193.170 193.580    193.580   34049900\n",
      "2023-12-29 193.900 194.400 191.730 192.530    192.530   42628800\n",
      "\n",
      "[2516 rows x 6 columns]\n",
      "            max_5\n",
      "Date             \n",
      "2014-01-02 17.031\n",
      "2014-01-03 17.031\n",
      "2014-01-06 17.016\n",
      "2014-01-07 17.108\n",
      "2014-01-08 17.451\n",
      "...           ...\n",
      "2023-12-22    NaN\n",
      "2023-12-26    NaN\n",
      "2023-12-27    NaN\n",
      "2023-12-28    NaN\n",
      "2023-12-29    NaN\n",
      "\n",
      "[2516 rows x 1 columns]\n",
      "            max_5  max_pct_5\n",
      "Date                        \n",
      "2014-01-02 17.031     -0.017\n",
      "2014-01-03 17.031      0.005\n",
      "2014-01-06 17.016     -0.001\n",
      "2014-01-07 17.108      0.012\n",
      "2014-01-08 17.451      0.026\n",
      "2014-01-09 17.451      0.039\n",
      "2014-01-10 17.451      0.046\n",
      "2014-01-13 17.451      0.040\n",
      "2014-01-14 17.451      0.020\n",
      "2014-01-15 17.414     -0.002\n",
      "Date\n",
      "2014-01-02   -0.017\n",
      "2014-01-03    0.005\n",
      "2014-01-06   -0.001\n",
      "2014-01-07    0.012\n",
      "2014-01-08    0.026\n",
      "              ...  \n",
      "2023-12-22      NaN\n",
      "2023-12-26      NaN\n",
      "2023-12-27      NaN\n",
      "2023-12-28      NaN\n",
      "2023-12-29      NaN\n",
      "Name: Adj Close, Length: 2516, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "stk_data = ticks_data[0].copy()\n",
    "print(stk_data)\n",
    "xx = pd.DataFrame(index=stk_data.index)\n",
    "xx[\"max_5\"] = stk_data[\"Adj Close\"].rolling(return_period).max().shift(-return_period)\n",
    "print(xx)\n",
    "xx[\"max_pct_5\"] = (xx[\"max_5\"] - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "# xx[\"test\"] = stk_data[\"Adj Close\"] * (1 + xx[\"max_pct_5\"])\n",
    "print(xx.head(10))\n",
    "\n",
    "a = stk_data[\"Adj Close\"].rolling(return_period).max().shift(-return_period)\n",
    "b = (a - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes = 3\n",
    "# 0: PCT <= -0.05\n",
    "# 1: 0.05 < PCT < -0.05\n",
    "# 2: PCT >= -0.05\n",
    "num_classes = 3\n",
    "pct_threshold = 0.05\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "\n",
    "def gen_pct_label(stk_data, _return_period):\n",
    "    max_price_period = (\n",
    "        stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "    )\n",
    "    max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "    pct_label = max_pct_period.apply(\n",
    "        lambda x: 2 if x >= pct_threshold else 0 if x <= -pct_threshold else 1\n",
    "    ).astype(\"int8\")\n",
    "    pct_label.name = \"label\"\n",
    "    return pct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=10),\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=60),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    buy_signal = sma[\"SMA_10\"] > sma[\"SMA_60\"]\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal[\"Signal\"] = (buy_signal).astype(\"int\")\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "\n",
    "def gen_analysis_data(stk_data, _return_period):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.adosc(),\n",
    "            stk_data.ta.kvo(),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=10) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=30) / 100,\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=200) / 100,\n",
    "            stk_data.ta.stoch(k=10) / 100,\n",
    "            stk_data.ta.stoch(k=30) / 100,\n",
    "            stk_data.ta.stoch(k=200) / 100,\n",
    "            gen_buy_sell_signal(stk_data),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data = pd.concat(\n",
    "        [data.astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AAPL  MSFT  AMZN  NVDA  GOOGL  TSLA  META  GOOG  ADBE  NFLX  CSCO  INTC  INTU  CMCSA   TXN  AMAT  ADSK   AMD  QCOM    MU\n",
      "0     6     7    14    32      9    55    17     8    13    26     7    17    10      8     8    25    17    50    20    41\n",
      "1  2022  2092  1929  1574   2069  1475  1915  2058  1957  1735  2141  1975  1981   2162  2085  1779  1873  1374  1933  1590\n",
      "2   285   214   370   707    235   783   381   247   343   552   165   321   322    143   220   509   423   889   360   682\n",
      "class 0: 0.008%,   390\n",
      "class 1: 0.815%, 37719\n",
      "class 2: 0.176%,  8151\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_df = pd.DataFrame()\n",
    "for i, stk_data in enumerate(ticks_data):\n",
    "    label_stat = gen_analysis_data(stk_data, return_period).groupby(\"label\").size()\n",
    "    label_stat.name = stk_tickers[i]\n",
    "    classes_df = pd.concat([classes_df, label_stat], axis=1)\n",
    "print(classes_df)\n",
    "classes_count = [classes_df.iloc[i].sum() for i in range(num_classes)]\n",
    "total_recs = sum(classes_count)\n",
    "for i, v in enumerate(classes_count):\n",
    "    print(f\"class {i}: {v/total_recs:.3f}%, {v:>5d}\")\n",
    "pyplot.bar([\"<= -0.5%\", \" between \", \">= 0.05%\"], classes_count)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, ticks_data_X, ticks_data_Y, seq_len):\n",
    "        self.ticks_data_X = ticks_data_X\n",
    "        self.ticks_data_Y = ticks_data_Y\n",
    "        self.seq_len = seq_len\n",
    "        len_array = [len(d) - self.seq_len + 1 for d in ticks_data_X]\n",
    "        self.idx_boundary = [len_array[0]]\n",
    "\n",
    "        for i in range(1, len(len_array)):\n",
    "            self.idx_boundary.append(len_array[i] + self.idx_boundary[i - 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(f\"len of dataset:{self.idx_boundary[-1]}\")\n",
    "        return self.idx_boundary[-1]  # len(self.X) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for ticks_data_idx in range(len(self.ticks_data_X)):\n",
    "            if self.idx_boundary[ticks_data_idx] > idx:\n",
    "                break\n",
    "        offset = (\n",
    "            idx if ticks_data_idx == 0 else idx - self.idx_boundary[ticks_data_idx - 1]\n",
    "        )\n",
    "        # print(f\"{ticks_data_idx}, {offset}\")\n",
    "        return (\n",
    "            np.array(self.ticks_data_X[ticks_data_idx][offset : offset + self.seq_len]),\n",
    "            int(self.ticks_data_Y[ticks_data_idx].iloc[offset + self.seq_len - 1, :]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_dataloader(_return_period, _seq_len):\n",
    "    ticks_dataset = [gen_analysis_data(d, _return_period) for d in ticks_data]\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for dataset in ticks_dataset:\n",
    "        test_size = int(dataset.shape[0] * validation_size)\n",
    "        # random.seed(42)\n",
    "        test_data_idx = random.sample(range(0, dataset.shape[0]), test_size)\n",
    "        mask = np.full(len(dataset), False)\n",
    "        mask[test_data_idx] = True\n",
    "        train_data = dataset[~mask]\n",
    "        test_data = dataset[mask]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:3].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[3:].values]\n",
    "        )\n",
    "        # print(features)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_train_data, ticks_Y_train_data, _seq_len),\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        LSTMDataSet(ticks_X_test_data, ticks_Y_test_data, _seq_len),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, ticks_X_train_data[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class StockPCTLabelPredictLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_fc_layers,\n",
    "            activation_type,\n",
    "        )\n",
    "\n",
    "    def __init__(self, input_size, config):\n",
    "        self.setup_model(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_fc_layers=config[\"num_fc_layers\"],\n",
    "            activation_type=config[\"activation_type\"],\n",
    "        )\n",
    "\n",
    "    def setup_model(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \"\"\"\n",
    "            input_size    : The number of expected features in the input x\n",
    "            hidden_size   : The number of features in the hidden state h\n",
    "            num_layers    : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "            bias          : If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            batch_first   : If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "            dropout       : If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            bidirectional : If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size     : If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        layers = []\n",
    "        in_features = self.hidden_size\n",
    "        for i in range(1, num_fc_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if out_features <= num_classes:\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(\n",
    "                nn.ReLU() if activation_type == 1 else nn.Sigmoid()\n",
    "            ) if activation_type == 2 else nn.Tanh()\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange = 0.5\n",
    "            nn.init.uniform_(m.weight, -initrange, initrange)\n",
    "            nn.init.zeros_(m.bias)\n",
    "            # print(f\"{m.in_features},{m.out_features}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        out, (h_out, _) = self.rnn(x, (h_0, c_0))\n",
    "\n",
    "        fc_input = h_out[-1].view(-1, self.hidden_size)\n",
    "        return self.fc(fc_input)\n",
    "\n",
    "\n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        \"epoch_num\": epoch_num,\n",
    "        \"time\": str(datetime.now),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hyper_parameters\": hyper_parameters,\n",
    "    }\n",
    "    # print(f\"save model:{file_path}\")\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict[\"hyper_parameters\"]\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=data_dict[\"input_size\"],\n",
    "        hidden_size=int(hyper_parameters[\"hidden_size\"]),\n",
    "        num_layers=int(hyper_parameters[\"num_layers\"]),\n",
    "        num_fc_layers=int(hyper_parameters[\"num_fc_layers\"]),\n",
    "        activation_type=int(hyper_parameters[\"activation_type\"]),\n",
    "    )\n",
    "    model.load_state_dict(data_dict[\"model_state\"])\n",
    "    return model, hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "def gen_f1_score(ground_truth, predict_results):\n",
    "    score_metric = []\n",
    "    for target_label in range(predict_results.shape[1]):\n",
    "        posLabel_mask = ground_truth == target_label\n",
    "        pos_count = posLabel_mask.sum()\n",
    "        negLabel_mask = ground_truth != target_label\n",
    "        neg_count = negLabel_mask.sum()\n",
    "\n",
    "        predict_value, predict_label = torch.max(predict_results, dim=1)\n",
    "        posPred_mask = predict_label == target_label\n",
    "        posPred_threshold_mask = predict_value > classificationThreshold\n",
    "        # TP, truePos_count\n",
    "        TP = pos_correct = int(\n",
    "            (posLabel_mask & posPred_mask & posPred_threshold_mask).sum()\n",
    "        )\n",
    "\n",
    "        negPred_mask = predict_label != target_label\n",
    "        # TN, trueNeg_count\n",
    "        TN = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "\n",
    "        # FP, falsePos_count\n",
    "        FP = neg_count - neg_correct\n",
    "        # FN, falseNeg_count\n",
    "        FN = pos_count - pos_correct\n",
    "\n",
    "        # precision = TP / (TP + FP)\n",
    "        precision = TP / (TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        recall = TP / (TP + FN)\n",
    "        # F1 = 2 * precision * recall / (precision + recall)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        score_metric.append({\"precision\": precision, \"recall\": recall, \"F1\": f1})\n",
    "\n",
    "    return score_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "METRICS_LABEL_NDX = 0  # ground_truth\n",
    "METRICS_PBTY_NDX = 1  # Probability of predicition\n",
    "METRICS_PRED_NDX = 2  # class(label) of predicition\n",
    "METRICS_LOSS_NDX = 3\n",
    "METRICS_SIZE = 4\n",
    "softmax = nn.Softmax(dim=1)\n",
    "trn_writer = None\n",
    "val_writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writers = {}\n",
    "\n",
    "\n",
    "def initTensorboardWriters():\n",
    "    if trn_writer is None:\n",
    "        writers[\"trn\"] = SummaryWriter(log_dir=log_dir + \"-trn_cls\")\n",
    "        writers[\"val\"] = SummaryWriter(log_dir=log_dir + \"-val_cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2704229955.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 58\u001b[0;36m\u001b[0m\n\u001b[0;31m    metrics_dict[\"loss/neg\"] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "totalTrainingSamples_count = 0\n",
    "\n",
    "\n",
    "def logMetrics(\n",
    "    self,\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5,\n",
    "):\n",
    "    initTensorboardWriters()\n",
    "    log.info(\n",
    "        \"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        )\n",
    "    )\n",
    "    F1_rec = namedtuple(\n",
    "        \"f1_rec\",\n",
    "        \"pos_correct neg_correct pos_count neg_count pos_loss neg_loss precision recall F1\",\n",
    "    )\n",
    "    F1_metrics = []\n",
    "    for target_class in range(num_classes):\n",
    "        posLabel_mask = metrics_t[METRICS_LABEL_NDX] == target_class\n",
    "        pos_count = posLabel_mask.sum()\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] != target_class\n",
    "        neg_count = negLabel_mask.sum()\n",
    "\n",
    "        posPred_mask = metrics_t[METRICS_PRED_NDX] == target_class\n",
    "        posPred_threshold_mask = metrics_t[METRICS_PBTY_NDX] > classificationThreshold\n",
    "        # TP, truePos_count\n",
    "        TP = pos_correct = int(\n",
    "            (posLabel_mask & posPred_mask & posPred_threshold_mask).sum()\n",
    "        )\n",
    "\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] != target_class\n",
    "        # TN, trueNeg_count\n",
    "        TN = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "\n",
    "        # FP, falsePos_count\n",
    "        FP = neg_count - neg_correct\n",
    "        # FN, falseNeg_count\n",
    "        FN = pos_count - pos_correct\n",
    "\n",
    "        # precision = TP / (TP + FP)\n",
    "        precision = 0.0 if (TP + FP) == 0 else TP / np.float32(TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        recall = 0.0 if (TP + FN) == 0 else TP / np.float32(TP + FN)\n",
    "        # F1 = 2 * precision * recall / (precision + recall)\n",
    "        F1 = (\n",
    "            0.0\n",
    "            if (precision + recall) == 0.0\n",
    "            else (2 * precision * recall) / np.float32(precision + recall)\n",
    "        )\n",
    "        F1_metrics.append(\n",
    "            F1_rec(\n",
    "                pos_correct,\n",
    "                neg_correct,\n",
    "                pos_count,\n",
    "                neg_count,\n",
    "                metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean(),\n",
    "                metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean(),\n",
    "                precision,\n",
    "                recall,\n",
    "                F1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"loss/all\"] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "    log.info(\n",
    "        (\"E{} {:8} {loss/all:.4f} loss\").format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            **metrics_dict,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for target_class, rec in enumerate(F1_metrics):\n",
    "        metrics_dict[f\"{target_class} loss/pos\"] = rec.pos_loss\n",
    "        metrics_dict[f\"{target_class} loss/neg\"] = rec.neg_loss\n",
    "        metrics_dict[f\"{target_class} correct/all\"] = (\n",
    "            (rec.pos_correct + rec.neg_correct) / metrics_t.shape[1] * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} correct/neg\"] = (\n",
    "            (rec.neg_correct) / rec.neg_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} correct/pos\"] = (\n",
    "            (rec.pos_correct) / rec.pos_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class} pr/precision\"] = rec.precision\n",
    "        metrics_dict[f\"{target_class} pr/recall\"] = rec.recall\n",
    "        metrics_dict[f\"{target_class} pr/f1_score\"] = rec.F1\n",
    "\n",
    "        log.info(\n",
    "            (\n",
    "                '{f\"{target_class}\" correct/all:-5.1f}% correct, '\n",
    "                + '{f\"{target_class}\" pr/precision:.4f} precision, '\n",
    "                + '{f\"{target_class}\" pr/recall:.4f} recall, '\n",
    "                + '{f\"{target_class}\" pr/f1_score:.4f} f1 score'\n",
    "            ).format(**metrics_dict)\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                'E{} {:8} {f\"{target_class}\" loss/neg:.4f} loss, '\n",
    "                + '{f\"{target_class}\" correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})'\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_neg\",\n",
    "                neg_correct=neg_correct,\n",
    "                neg_count=neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                'E{} {:8} {f\"{target_class}\" loss/pos:.4f} loss, '\n",
    "                + '{f\"{target_class}\" correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})'\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_pos\",\n",
    "                pos_correct=pos_correct,\n",
    "                pos_count=pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    writer = writers[mode_str]\n",
    "    for key, value in metrics_dict.items():\n",
    "        writer.add_scalar(key, value, totalTrainingSamples_count)\n",
    "\n",
    "    writer.add_pr_curve(\n",
    "        \"pr\",\n",
    "        metrics_t[METRICS_LABEL_NDX],\n",
    "        metrics_t[METRICS_PBTY_NDX],\n",
    "        metrics_t[METRICS_PRED_NDX],\n",
    "        totalTrainingSamples_count,\n",
    "    )\n",
    "\n",
    "    # bins = [x / 50.0 for x in range(51)]\n",
    "\n",
    "    # negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
    "    # posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
    "\n",
    "    # if negHist_mask.any():\n",
    "    #     writer.add_histogram(\n",
    "    #         \"is_neg\",\n",
    "    #         metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "    #         self.totalTrainingSamples_count,\n",
    "    #         bins=bins,\n",
    "    #     )\n",
    "    # if posHist_mask.any():\n",
    "    #     writer.add_histogram(\n",
    "    #         \"is_pos\",\n",
    "    #         metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
    "    #         self.totalTrainingSamples_count,\n",
    "    #         bins=bins,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dl_method_v0(model, dl, device=device):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    model.eval()\n",
    "    y_gt = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    f1 = 0\n",
    "    for x, y in dl:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        values, predicted = torch.max(outputs, dim=1)\n",
    "        total += y.shape[0]\n",
    "        correct += int((predicted == y).sum())\n",
    "        # f1 += f1_score(y.cpu().detach(), predicted.cpu().detach())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1 / len(dl)\n",
    "    # print(f\"Accuracy: {accuracy:.3f}, F1 score:{f1:.3f}\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "_best_loss = 999.0\n",
    "_best_accuracy = 0.0\n",
    "\n",
    "\n",
    "def do_train_v0(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    id_str,\n",
    "    config,\n",
    "    writer=None,\n",
    "    report_to_Ray=False,\n",
    "):\n",
    "    global _best_loss\n",
    "    global _best_accuracy\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    # for epoch in tqdm(range(epoch_num)):\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_size = 0\n",
    "        total_correct_size = 0\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            # if torch.isnan(outputs).any():\n",
    "            #     print(f\"NaN: epoch={epoch}, i={i}\")\n",
    "            #     with pd.option_context(\"display.max_rows\", None):\n",
    "            #         print(x)\n",
    "            #     break\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_size += y.shape[0]\n",
    "            total_correct_size += int((predicted == y).sum())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            accuracy, f1 = eval_dl_method(model, test_dl)\n",
    "            if accuracy > _best_accuracy:\n",
    "                _best_accuracy = accuracy\n",
    "\n",
    "            if loss.item() < _best_loss:\n",
    "                _best_loss = loss.item()\n",
    "                save_model(model, config, model_name)\n",
    "\n",
    "        total_loss += running_loss / len(train_dl)\n",
    "        total_accuracy += accuracy\n",
    "        if report_to_Ray:\n",
    "            train.report(\n",
    "                {\n",
    "                    \"loss\": running_loss / len(train_dl),\n",
    "                    \"train_accuracy\": total_correct_size / total_size,\n",
    "                    \"mean_accuracy\": accuracy,\n",
    "                }\n",
    "            )\n",
    "        if writer != None:\n",
    "            writer.add_scalars(\n",
    "                \"Training vs. Validation Loss\",\n",
    "                {\n",
    "                    \"loss\": running_loss / len(train_dl),\n",
    "                    \"train_accuracy\": total_correct_size / total_size,\n",
    "                    \"mean_accuracy\": accuracy,\n",
    "                },\n",
    "                epoch + 1,\n",
    "            )\n",
    "            writer.flush()\n",
    "\n",
    "    return {\n",
    "        \"Train loss\": total_loss / epoch_num,\n",
    "        \"Validation Accuracy\": total_accuracy / epoch_num,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBatchLoss(model, loss_fn, x, y, metrics_g, batch_idx):\n",
    "    x_g = x.to(device)\n",
    "    y_g = y.to(device)\n",
    "    outputs = model(x_g)\n",
    "    loss_g = loss_fn(outputs, y_g)\n",
    "    probability_g, predition_g = torch.max(softmax(outputs), dim=1)\n",
    "\n",
    "    start_ndx = batch_idx * batch_size\n",
    "    end_ndx = start_ndx + y.size(0)\n",
    "\n",
    "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = y_g[:, 1]\n",
    "    metrics_g[METRICS_PBTY_NDX, start_ndx:end_ndx] = probability_g[:, 1]\n",
    "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predition_g[:, 1]\n",
    "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
    "\n",
    "    return loss_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import enumerateWithEstimate\n",
    "import torch\n",
    "\n",
    "\n",
    "def doTraining(model, optimizer, loss_fn, epoch_ndx, train_dl):\n",
    "    model.train()\n",
    "    trnMetrics_g = torch.zeros(\n",
    "        METRICS_SIZE,\n",
    "        len(train_dl.dataset),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, (x, y) in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = computeBatchLoss(\n",
    "            model,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            trnMetrics_g,\n",
    "            batch_ndx,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    totalTrainingSamples_count += len(train_dl.dataset)\n",
    "    return trnMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(model, loss_fn, epoch_ndx, val_dl):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, (x, y) in batch_iter:\n",
    "            computeBatchLoss(model, loss_fn, x, y, valMetrics_g, batch_ndx)\n",
    "\n",
    "    return valMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(config, tensorreport=False):\n",
    "    _return_period = config[\"return_period\"]\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    print(id_str)\n",
    "\n",
    "    if tensorreport:\n",
    "        writer = SummaryWriter(f\"{log_dir}/{id_str}\")\n",
    "    train_loader, test_loader, features_size = prepare_dataloader(_return_period)\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        logMetrics(epoch_ndx, \"trn\", trnMetrics_t)\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        logMetrics(epoch_ndx, \"val\", valMetrics_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "config = {\n",
    "    \"return_period\": 5,\n",
    "    \"seq_len\": 5,\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.11646759543664197,\n",
    "    \"optim_type\": 1,  # Adam\n",
    "    \"num_layers\": 4,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_fc_layers\": 1,\n",
    "    \"activation_type\": 2,  # Sigmoid\n",
    "}\n",
    "epoch_num = 20\n",
    "# os.mkdir(log_dir)\n",
    "print(log_dir)\n",
    "train_LSTM(config, tensorreport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_layers', 'hidden_size', 'num_fc_layers', 'activation_type']\n",
      "Total count of configs = 48\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"return_period\": tune.grid_search([5]),  # [2,3,5,10]\n",
    "    \"seq_len\": tune.grid_search([5]),  # 10]),\n",
    "    \"lr\": tune.grid_search([0.01]),  # [0.001, 0.01, 0.1]\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([1]),  # 1: Adam, 2: SGD\n",
    "    \"num_layers\": tune.grid_search([4, 16]),  # [1, 2, 4, 8]\n",
    "    \"hidden_size\": tune.grid_search([32, 64, 128, 256]),  # [8, 16, 32, 64, 128]\n",
    "    \"num_fc_layers\": tune.grid_search([1, 2]),  # 1, 2, 3]),\n",
    "    \"activation_type\": tune.grid_search(\n",
    "        [1, 2, 3]\n",
    "    ),  # 1: ReLU(),  2: Sigmoid(),  3: Tanh()\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if (\n",
    "        type(v).__name__ == \"dict\"\n",
    "        and list(v.keys())[0] == \"grid_search\"\n",
    "        and len(list(v.values())[0]) > 1\n",
    "    ):\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 07:46:06,366\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-31 09:44:07</td></tr>\n",
       "<tr><td>Running for: </td><td>01:58:00.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.3/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0.1/32 CPUs, 0.1/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  activation_type</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  num_fc_layers</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  optim_type</th><th style=\"text-align: right;\">  return_period</th><th style=\"text-align: right;\">  seq_len</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_d8976_00000</td><td>TERMINATED</td><td>192.168.0.125:24015 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.146467</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.760761</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1051.29 </td><td style=\"text-align: right;\">0.279489</td><td style=\"text-align: right;\">        0.877123</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00001</td><td>TERMINATED</td><td>192.168.0.125:24016 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.792941</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.755978</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.39 </td><td style=\"text-align: right;\">0.275412</td><td style=\"text-align: right;\">        0.879232</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00002</td><td>TERMINATED</td><td>192.168.0.125:24017 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.580892</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.775761</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.67 </td><td style=\"text-align: right;\">0.282232</td><td style=\"text-align: right;\">        0.876717</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00003</td><td>TERMINATED</td><td>192.168.0.125:24018 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.666458</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.779348</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.68 </td><td style=\"text-align: right;\">0.247599</td><td style=\"text-align: right;\">        0.892131</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00004</td><td>TERMINATED</td><td>192.168.0.125:24019 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.116468</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.767283</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1052.02 </td><td style=\"text-align: right;\">0.255386</td><td style=\"text-align: right;\">        0.889995</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00005</td><td>TERMINATED</td><td>192.168.0.125:24020 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.875928</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.748696</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.54 </td><td style=\"text-align: right;\">0.276691</td><td style=\"text-align: right;\">        0.879313</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00006</td><td>TERMINATED</td><td>192.168.0.125:24021 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.765954</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764783</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050.53 </td><td style=\"text-align: right;\">0.2478  </td><td style=\"text-align: right;\">        0.894348</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00007</td><td>TERMINATED</td><td>192.168.0.125:24022 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.269871</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.758804</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1051.99 </td><td style=\"text-align: right;\">0.25969 </td><td style=\"text-align: right;\">        0.888345</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00008</td><td>TERMINATED</td><td>192.168.0.125:24023 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.24546 </td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.781739</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1050    </td><td style=\"text-align: right;\">0.221508</td><td style=\"text-align: right;\">        0.90484 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00009</td><td>TERMINATED</td><td>192.168.0.125:24024 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.246724</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.743587</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         831.816</td><td style=\"text-align: right;\">0.214047</td><td style=\"text-align: right;\">        0.910032</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00010</td><td>TERMINATED</td><td>192.168.0.125:42405 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.343394</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.746739</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         842.169</td><td style=\"text-align: right;\">0.238096</td><td style=\"text-align: right;\">        0.896376</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00011</td><td>TERMINATED</td><td>192.168.0.125:46919 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.519805</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75587 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         844.187</td><td style=\"text-align: right;\">0.206978</td><td style=\"text-align: right;\">        0.913521</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00012</td><td>TERMINATED</td><td>192.168.0.125:46923 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.445556</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.769022</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1105.5  </td><td style=\"text-align: right;\">0.297823</td><td style=\"text-align: right;\">        0.870714</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00013</td><td>TERMINATED</td><td>192.168.0.125:47101 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.332983</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.746087</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1103.6  </td><td style=\"text-align: right;\">0.291819</td><td style=\"text-align: right;\">        0.872715</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00014</td><td>TERMINATED</td><td>192.168.0.125:47102 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.589482</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.777609</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.96 </td><td style=\"text-align: right;\">0.283477</td><td style=\"text-align: right;\">        0.875014</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00015</td><td>TERMINATED</td><td>192.168.0.125:47103 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.211595</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.761957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.98 </td><td style=\"text-align: right;\">0.263868</td><td style=\"text-align: right;\">        0.88464 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00016</td><td>TERMINATED</td><td>192.168.0.125:47104 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.333716</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75    </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1104.52 </td><td style=\"text-align: right;\">0.270398</td><td style=\"text-align: right;\">        0.882639</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00017</td><td>TERMINATED</td><td>192.168.0.125:47105 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.393089</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.770543</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1105.68 </td><td style=\"text-align: right;\">0.259121</td><td style=\"text-align: right;\">        0.887128</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00018</td><td>TERMINATED</td><td>192.168.0.125:47112 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.464856</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.760978</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1101.03 </td><td style=\"text-align: right;\">0.258532</td><td style=\"text-align: right;\">        0.888048</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00019</td><td>TERMINATED</td><td>192.168.0.125:47422 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.728141</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.754239</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1103.1  </td><td style=\"text-align: right;\">0.282495</td><td style=\"text-align: right;\">        0.879367</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00020</td><td>TERMINATED</td><td>192.168.0.125:60806 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.259739</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757609</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1297.25 </td><td style=\"text-align: right;\">0.252851</td><td style=\"text-align: right;\">        0.893294</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00021</td><td>TERMINATED</td><td>192.168.0.125:65214 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.511388</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757717</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         992.928</td><td style=\"text-align: right;\">0.232827</td><td style=\"text-align: right;\">        0.902299</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00022</td><td>TERMINATED</td><td>192.168.0.125:71098 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.573932</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.729674</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1097.44 </td><td style=\"text-align: right;\">0.253164</td><td style=\"text-align: right;\">        0.891888</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00023</td><td>TERMINATED</td><td>192.168.0.125:71211 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.13716 </td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.754239</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1096.79 </td><td style=\"text-align: right;\">0.246975</td><td style=\"text-align: right;\">        0.893618</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00024</td><td>TERMINATED</td><td>192.168.0.125:71212 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.586036</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.758152</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2028.17 </td><td style=\"text-align: right;\">0.70876 </td><td style=\"text-align: right;\">        0.763953</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00025</td><td>TERMINATED</td><td>192.168.0.125:71478 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.236419</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766848</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2024.58 </td><td style=\"text-align: right;\">0.712974</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00026</td><td>TERMINATED</td><td>192.168.0.125:71479 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.152041</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2024.66 </td><td style=\"text-align: right;\">0.710159</td><td style=\"text-align: right;\">        0.763034</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00027</td><td>TERMINATED</td><td>192.168.0.125:71480 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.859108</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.767065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1194.47 </td><td style=\"text-align: right;\">0.713082</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00028</td><td>TERMINATED</td><td>192.168.0.125:71481 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.872506</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.763696</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1188.06 </td><td style=\"text-align: right;\">0.711219</td><td style=\"text-align: right;\">        0.762574</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00029</td><td>TERMINATED</td><td>192.168.0.125:71959 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.746718</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.763804</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1185.85 </td><td style=\"text-align: right;\">0.711106</td><td style=\"text-align: right;\">        0.762493</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00030</td><td>TERMINATED</td><td>192.168.0.125:84837 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.343691</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768043</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1665.3  </td><td style=\"text-align: right;\">0.712976</td><td style=\"text-align: right;\">        0.76152 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00031</td><td>TERMINATED</td><td>192.168.0.125:86321 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.178138</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764891</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1701.69 </td><td style=\"text-align: right;\">0.711721</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00032</td><td>TERMINATED</td><td>192.168.0.125:90472 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.647386</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.7675  </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1785.74 </td><td style=\"text-align: right;\">0.712908</td><td style=\"text-align: right;\">        0.761709</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00033</td><td>TERMINATED</td><td>192.168.0.125:90573 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.452122</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762717</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1792.14 </td><td style=\"text-align: right;\">0.710962</td><td style=\"text-align: right;\">        0.76271 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00034</td><td>TERMINATED</td><td>192.168.0.125:92487 </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.197631</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.757065</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1789.91 </td><td style=\"text-align: right;\">0.709124</td><td style=\"text-align: right;\">        0.764224</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00035</td><td>TERMINATED</td><td>192.168.0.125:92603 </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.496142</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.759565</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1793.13 </td><td style=\"text-align: right;\">0.713636</td><td style=\"text-align: right;\">        0.763575</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00036</td><td>TERMINATED</td><td>192.168.0.125:92933 </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.127511</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768587</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2565.55 </td><td style=\"text-align: right;\">0.713929</td><td style=\"text-align: right;\">        0.761222</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00037</td><td>TERMINATED</td><td>192.168.0.125:101869</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.827456</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2523.46 </td><td style=\"text-align: right;\">0.713243</td><td style=\"text-align: right;\">        0.761817</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00038</td><td>TERMINATED</td><td>192.168.0.125:101870</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.307024</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.766957</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        2537.13 </td><td style=\"text-align: right;\">0.712817</td><td style=\"text-align: right;\">        0.761763</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00039</td><td>TERMINATED</td><td>192.168.0.125:102041</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.630018</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.765326</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1647.38 </td><td style=\"text-align: right;\">0.712451</td><td style=\"text-align: right;\">        0.762196</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00040</td><td>TERMINATED</td><td>192.168.0.125:106248</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.349369</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764891</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1643.49 </td><td style=\"text-align: right;\">0.712109</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00041</td><td>TERMINATED</td><td>192.168.0.125:107749</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.516054</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762174</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1647.46 </td><td style=\"text-align: right;\">0.711074</td><td style=\"text-align: right;\">        0.762926</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00042</td><td>TERMINATED</td><td>192.168.0.125:112004</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.537368</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.756413</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1660.4  </td><td style=\"text-align: right;\">0.708159</td><td style=\"text-align: right;\">        0.764413</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00043</td><td>TERMINATED</td><td>192.168.0.125:112305</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.247884</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.75913 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1656.65 </td><td style=\"text-align: right;\">0.709709</td><td style=\"text-align: right;\">        0.763683</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00044</td><td>TERMINATED</td><td>192.168.0.125:113479</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.875668</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.768043</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1630.44 </td><td style=\"text-align: right;\">0.714536</td><td style=\"text-align: right;\">        0.761466</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00045</td><td>TERMINATED</td><td>192.168.0.125:113722</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.720106</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.764457</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1632.09 </td><td style=\"text-align: right;\">0.71276 </td><td style=\"text-align: right;\">        0.762304</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00046</td><td>TERMINATED</td><td>192.168.0.125:122624</td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.851599</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.762174</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1212.52 </td><td style=\"text-align: right;\">0.711295</td><td style=\"text-align: right;\">        0.76298 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00047</td><td>TERMINATED</td><td>192.168.0.125:123712</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">  0.815862</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">        5</td><td style=\"text-align: right;\">0.759348</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1146.56 </td><td style=\"text-align: right;\">0.710236</td><td style=\"text-align: right;\">        0.763548</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  mean_accuracy</th><th style=\"text-align: right;\">  train_accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_LSTM_d8976_00000</td><td style=\"text-align: right;\">0.279489</td><td style=\"text-align: right;\">       0.760761</td><td style=\"text-align: right;\">        0.877123</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00001</td><td style=\"text-align: right;\">0.275412</td><td style=\"text-align: right;\">       0.755978</td><td style=\"text-align: right;\">        0.879232</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00002</td><td style=\"text-align: right;\">0.282232</td><td style=\"text-align: right;\">       0.775761</td><td style=\"text-align: right;\">        0.876717</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00003</td><td style=\"text-align: right;\">0.247599</td><td style=\"text-align: right;\">       0.779348</td><td style=\"text-align: right;\">        0.892131</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00004</td><td style=\"text-align: right;\">0.255386</td><td style=\"text-align: right;\">       0.767283</td><td style=\"text-align: right;\">        0.889995</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00005</td><td style=\"text-align: right;\">0.276691</td><td style=\"text-align: right;\">       0.748696</td><td style=\"text-align: right;\">        0.879313</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00006</td><td style=\"text-align: right;\">0.2478  </td><td style=\"text-align: right;\">       0.764783</td><td style=\"text-align: right;\">        0.894348</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00007</td><td style=\"text-align: right;\">0.25969 </td><td style=\"text-align: right;\">       0.758804</td><td style=\"text-align: right;\">        0.888345</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00008</td><td style=\"text-align: right;\">0.221508</td><td style=\"text-align: right;\">       0.781739</td><td style=\"text-align: right;\">        0.90484 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00009</td><td style=\"text-align: right;\">0.214047</td><td style=\"text-align: right;\">       0.743587</td><td style=\"text-align: right;\">        0.910032</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00010</td><td style=\"text-align: right;\">0.238096</td><td style=\"text-align: right;\">       0.746739</td><td style=\"text-align: right;\">        0.896376</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00011</td><td style=\"text-align: right;\">0.206978</td><td style=\"text-align: right;\">       0.75587 </td><td style=\"text-align: right;\">        0.913521</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00012</td><td style=\"text-align: right;\">0.297823</td><td style=\"text-align: right;\">       0.769022</td><td style=\"text-align: right;\">        0.870714</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00013</td><td style=\"text-align: right;\">0.291819</td><td style=\"text-align: right;\">       0.746087</td><td style=\"text-align: right;\">        0.872715</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00014</td><td style=\"text-align: right;\">0.283477</td><td style=\"text-align: right;\">       0.777609</td><td style=\"text-align: right;\">        0.875014</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00015</td><td style=\"text-align: right;\">0.263868</td><td style=\"text-align: right;\">       0.761957</td><td style=\"text-align: right;\">        0.88464 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00016</td><td style=\"text-align: right;\">0.270398</td><td style=\"text-align: right;\">       0.75    </td><td style=\"text-align: right;\">        0.882639</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00017</td><td style=\"text-align: right;\">0.259121</td><td style=\"text-align: right;\">       0.770543</td><td style=\"text-align: right;\">        0.887128</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00018</td><td style=\"text-align: right;\">0.258532</td><td style=\"text-align: right;\">       0.760978</td><td style=\"text-align: right;\">        0.888048</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00019</td><td style=\"text-align: right;\">0.282495</td><td style=\"text-align: right;\">       0.754239</td><td style=\"text-align: right;\">        0.879367</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00020</td><td style=\"text-align: right;\">0.252851</td><td style=\"text-align: right;\">       0.757609</td><td style=\"text-align: right;\">        0.893294</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00021</td><td style=\"text-align: right;\">0.232827</td><td style=\"text-align: right;\">       0.757717</td><td style=\"text-align: right;\">        0.902299</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00022</td><td style=\"text-align: right;\">0.253164</td><td style=\"text-align: right;\">       0.729674</td><td style=\"text-align: right;\">        0.891888</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00023</td><td style=\"text-align: right;\">0.246975</td><td style=\"text-align: right;\">       0.754239</td><td style=\"text-align: right;\">        0.893618</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00024</td><td style=\"text-align: right;\">0.70876 </td><td style=\"text-align: right;\">       0.758152</td><td style=\"text-align: right;\">        0.763953</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00025</td><td style=\"text-align: right;\">0.712974</td><td style=\"text-align: right;\">       0.766848</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00026</td><td style=\"text-align: right;\">0.710159</td><td style=\"text-align: right;\">       0.762065</td><td style=\"text-align: right;\">        0.763034</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00027</td><td style=\"text-align: right;\">0.713082</td><td style=\"text-align: right;\">       0.767065</td><td style=\"text-align: right;\">        0.761628</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00028</td><td style=\"text-align: right;\">0.711219</td><td style=\"text-align: right;\">       0.763696</td><td style=\"text-align: right;\">        0.762574</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00029</td><td style=\"text-align: right;\">0.711106</td><td style=\"text-align: right;\">       0.763804</td><td style=\"text-align: right;\">        0.762493</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00030</td><td style=\"text-align: right;\">0.712976</td><td style=\"text-align: right;\">       0.768043</td><td style=\"text-align: right;\">        0.76152 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00031</td><td style=\"text-align: right;\">0.711721</td><td style=\"text-align: right;\">       0.764891</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00032</td><td style=\"text-align: right;\">0.712908</td><td style=\"text-align: right;\">       0.7675  </td><td style=\"text-align: right;\">        0.761709</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00033</td><td style=\"text-align: right;\">0.710962</td><td style=\"text-align: right;\">       0.762717</td><td style=\"text-align: right;\">        0.76271 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00034</td><td style=\"text-align: right;\">0.709124</td><td style=\"text-align: right;\">       0.757065</td><td style=\"text-align: right;\">        0.764224</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00035</td><td style=\"text-align: right;\">0.713636</td><td style=\"text-align: right;\">       0.759565</td><td style=\"text-align: right;\">        0.763575</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00036</td><td style=\"text-align: right;\">0.713929</td><td style=\"text-align: right;\">       0.768587</td><td style=\"text-align: right;\">        0.761222</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00037</td><td style=\"text-align: right;\">0.713243</td><td style=\"text-align: right;\">       0.766957</td><td style=\"text-align: right;\">        0.761817</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00038</td><td style=\"text-align: right;\">0.712817</td><td style=\"text-align: right;\">       0.766957</td><td style=\"text-align: right;\">        0.761763</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00039</td><td style=\"text-align: right;\">0.712451</td><td style=\"text-align: right;\">       0.765326</td><td style=\"text-align: right;\">        0.762196</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00040</td><td style=\"text-align: right;\">0.712109</td><td style=\"text-align: right;\">       0.764891</td><td style=\"text-align: right;\">        0.762223</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00041</td><td style=\"text-align: right;\">0.711074</td><td style=\"text-align: right;\">       0.762174</td><td style=\"text-align: right;\">        0.762926</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00042</td><td style=\"text-align: right;\">0.708159</td><td style=\"text-align: right;\">       0.756413</td><td style=\"text-align: right;\">        0.764413</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00043</td><td style=\"text-align: right;\">0.709709</td><td style=\"text-align: right;\">       0.75913 </td><td style=\"text-align: right;\">        0.763683</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00044</td><td style=\"text-align: right;\">0.714536</td><td style=\"text-align: right;\">       0.768043</td><td style=\"text-align: right;\">        0.761466</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00045</td><td style=\"text-align: right;\">0.71276 </td><td style=\"text-align: right;\">       0.764457</td><td style=\"text-align: right;\">        0.762304</td></tr>\n",
       "<tr><td>train_LSTM_d8976_00046</td><td style=\"text-align: right;\">0.711295</td><td style=\"text-align: right;\">       0.762174</td><td style=\"text-align: right;\">        0.76298 </td></tr>\n",
       "<tr><td>train_LSTM_d8976_00047</td><td style=\"text-align: right;\">0.710236</td><td style=\"text-align: right;\">       0.759348</td><td style=\"text-align: right;\">        0.763548</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 09:44:07,044\tINFO tune.py:1042 -- Total run time: 7080.68 seconds (7080.65 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "os.mkdir(log_dir)\n",
    "analysis = tune.run(\n",
    "    train_LSTM,\n",
    "    config=search_space,\n",
    "    resources_per_trial={\"cpu\": 0.1, \"gpu\": 0.1},\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_accuracy     trial_id\n",
      "0           0.760  d8976_00000\n",
      "1           0.770  d8976_00001\n",
      "2           0.781  d8976_00002\n",
      "3           0.775  d8976_00003\n",
      "4           0.782  d8976_00004\n",
      "5           0.760  d8976_00005\n",
      "6           0.750  d8976_00006\n",
      "7           0.763  d8976_00007\n",
      "8           0.774  d8976_00008\n",
      "9           0.753  d8976_00009\n",
      "10          0.750  d8976_00010\n",
      "11          0.753  d8976_00011\n",
      "12          0.764  d8976_00012\n",
      "13          0.758  d8976_00013\n",
      "14          0.775  d8976_00014\n",
      "15          0.777  d8976_00015\n",
      "16          0.760  d8976_00016\n",
      "17          0.760  d8976_00017\n",
      "18          0.758  d8976_00018\n",
      "19          0.758  d8976_00019\n",
      "20          0.755  d8976_00020\n",
      "21          0.761  d8976_00021\n",
      "22          0.738  d8976_00022\n",
      "23          0.757  d8976_00023\n",
      "24          0.758  d8976_00024\n",
      "25          0.767  d8976_00025\n",
      "26          0.762  d8976_00026\n",
      "27          0.767  d8976_00027\n",
      "28          0.764  d8976_00028\n",
      "29          0.764  d8976_00029\n",
      "30          0.768  d8976_00030\n",
      "31          0.765  d8976_00031\n",
      "32          0.767  d8976_00032\n",
      "33          0.763  d8976_00033\n",
      "34          0.757  d8976_00034\n",
      "35          0.760  d8976_00035\n",
      "36          0.769  d8976_00036\n",
      "37          0.767  d8976_00037\n",
      "38          0.767  d8976_00038\n",
      "39          0.765  d8976_00039\n",
      "40          0.765  d8976_00040\n",
      "41          0.762  d8976_00041\n",
      "42          0.756  d8976_00042\n",
      "43          0.759  d8976_00043\n",
      "44          0.768  d8976_00044\n",
      "45          0.764  d8976_00045\n",
      "46          0.762  d8976_00046\n",
      "47          0.759  d8976_00047\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "trial_list = list(analysis.trial_dataframes.values())\n",
    "for i, trial in enumerate(trial_list):\n",
    "    if trial.empty == False:\n",
    "        d = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"mean_accuracy\": trial.describe().loc[\"mean\", \"mean_accuracy\"],\n",
    "                \"trial_id\": trial.loc[0:0, \"trial_id\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        d = pd.DataFrame.from_dict({\"mean_accuracy\": [np.NaN], \"trial_id\": [np.NaN]})\n",
    "    accuracy_list.append(d)\n",
    "accuracy_df = pd.concat(accuracy_list)\n",
    "accuracy_df = accuracy_df.reset_index().loc[:, [\"mean_accuracy\", \"trial_id\"]]\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0               5        5 0.010     0.146           1           4           32              1                1\n",
      "1               5        5 0.010     0.793           1           4           32              1                2\n",
      "2               5        5 0.010     0.581           1           4           32              1                3\n",
      "3               5        5 0.010     0.666           1           4           64              1                1\n",
      "4               5        5 0.010     0.116           1           4           64              1                2\n",
      "5               5        5 0.010     0.876           1           4           64              1                3\n",
      "6               5        5 0.010     0.766           1           4          128              1                1\n",
      "7               5        5 0.010     0.270           1           4          128              1                2\n",
      "8               5        5 0.010     0.245           1           4          128              1                3\n",
      "9               5        5 0.010     0.247           1           4          256              1                1\n",
      "10              5        5 0.010     0.343           1           4          256              1                2\n",
      "11              5        5 0.010     0.520           1           4          256              1                3\n",
      "12              5        5 0.010     0.446           1           4           32              2                1\n",
      "13              5        5 0.010     0.333           1           4           32              2                2\n",
      "14              5        5 0.010     0.589           1           4           32              2                3\n",
      "15              5        5 0.010     0.212           1           4           64              2                1\n",
      "16              5        5 0.010     0.334           1           4           64              2                2\n",
      "17              5        5 0.010     0.393           1           4           64              2                3\n",
      "18              5        5 0.010     0.465           1           4          128              2                1\n",
      "19              5        5 0.010     0.728           1           4          128              2                2\n",
      "20              5        5 0.010     0.260           1           4          128              2                3\n",
      "21              5        5 0.010     0.511           1           4          256              2                1\n",
      "22              5        5 0.010     0.574           1           4          256              2                2\n",
      "23              5        5 0.010     0.137           1           4          256              2                3\n",
      "24              5        5 0.010     0.586           1          16           32              1                1\n",
      "25              5        5 0.010     0.236           1          16           32              1                2\n",
      "26              5        5 0.010     0.152           1          16           32              1                3\n",
      "27              5        5 0.010     0.859           1          16           64              1                1\n",
      "28              5        5 0.010     0.873           1          16           64              1                2\n",
      "29              5        5 0.010     0.747           1          16           64              1                3\n",
      "30              5        5 0.010     0.344           1          16          128              1                1\n",
      "31              5        5 0.010     0.178           1          16          128              1                2\n",
      "32              5        5 0.010     0.647           1          16          128              1                3\n",
      "33              5        5 0.010     0.452           1          16          256              1                1\n",
      "34              5        5 0.010     0.198           1          16          256              1                2\n",
      "35              5        5 0.010     0.496           1          16          256              1                3\n",
      "36              5        5 0.010     0.128           1          16           32              2                1\n",
      "37              5        5 0.010     0.827           1          16           32              2                2\n",
      "38              5        5 0.010     0.307           1          16           32              2                3\n",
      "39              5        5 0.010     0.630           1          16           64              2                1\n",
      "40              5        5 0.010     0.349           1          16           64              2                2\n",
      "41              5        5 0.010     0.516           1          16           64              2                3\n",
      "42              5        5 0.010     0.537           1          16          128              2                1\n",
      "43              5        5 0.010     0.248           1          16          128              2                2\n",
      "44              5        5 0.010     0.876           1          16          128              2                3\n",
      "45              5        5 0.010     0.720           1          16          256              2                1\n",
      "46              5        5 0.010     0.852           1          16          256              2                2\n",
      "47              5        5 0.010     0.816           1          16          256              2                3\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "0           0.760  d8976_00000              5        5 0.010     0.146           1           4           32              1                1\n",
      "1           0.770  d8976_00001              5        5 0.010     0.793           1           4           32              1                2\n",
      "2           0.781  d8976_00002              5        5 0.010     0.581           1           4           32              1                3\n",
      "3           0.775  d8976_00003              5        5 0.010     0.666           1           4           64              1                1\n",
      "4           0.782  d8976_00004              5        5 0.010     0.116           1           4           64              1                2\n",
      "5           0.760  d8976_00005              5        5 0.010     0.876           1           4           64              1                3\n",
      "6           0.750  d8976_00006              5        5 0.010     0.766           1           4          128              1                1\n",
      "7           0.763  d8976_00007              5        5 0.010     0.270           1           4          128              1                2\n",
      "8           0.774  d8976_00008              5        5 0.010     0.245           1           4          128              1                3\n",
      "9           0.753  d8976_00009              5        5 0.010     0.247           1           4          256              1                1\n",
      "10          0.750  d8976_00010              5        5 0.010     0.343           1           4          256              1                2\n",
      "11          0.753  d8976_00011              5        5 0.010     0.520           1           4          256              1                3\n",
      "12          0.764  d8976_00012              5        5 0.010     0.446           1           4           32              2                1\n",
      "13          0.758  d8976_00013              5        5 0.010     0.333           1           4           32              2                2\n",
      "14          0.775  d8976_00014              5        5 0.010     0.589           1           4           32              2                3\n",
      "15          0.777  d8976_00015              5        5 0.010     0.212           1           4           64              2                1\n",
      "16          0.760  d8976_00016              5        5 0.010     0.334           1           4           64              2                2\n",
      "17          0.760  d8976_00017              5        5 0.010     0.393           1           4           64              2                3\n",
      "18          0.758  d8976_00018              5        5 0.010     0.465           1           4          128              2                1\n",
      "19          0.758  d8976_00019              5        5 0.010     0.728           1           4          128              2                2\n",
      "20          0.755  d8976_00020              5        5 0.010     0.260           1           4          128              2                3\n",
      "21          0.761  d8976_00021              5        5 0.010     0.511           1           4          256              2                1\n",
      "22          0.738  d8976_00022              5        5 0.010     0.574           1           4          256              2                2\n",
      "23          0.757  d8976_00023              5        5 0.010     0.137           1           4          256              2                3\n",
      "24          0.758  d8976_00024              5        5 0.010     0.586           1          16           32              1                1\n",
      "25          0.767  d8976_00025              5        5 0.010     0.236           1          16           32              1                2\n",
      "26          0.762  d8976_00026              5        5 0.010     0.152           1          16           32              1                3\n",
      "27          0.767  d8976_00027              5        5 0.010     0.859           1          16           64              1                1\n",
      "28          0.764  d8976_00028              5        5 0.010     0.873           1          16           64              1                2\n",
      "29          0.764  d8976_00029              5        5 0.010     0.747           1          16           64              1                3\n",
      "30          0.768  d8976_00030              5        5 0.010     0.344           1          16          128              1                1\n",
      "31          0.765  d8976_00031              5        5 0.010     0.178           1          16          128              1                2\n",
      "32          0.767  d8976_00032              5        5 0.010     0.647           1          16          128              1                3\n",
      "33          0.763  d8976_00033              5        5 0.010     0.452           1          16          256              1                1\n",
      "34          0.757  d8976_00034              5        5 0.010     0.198           1          16          256              1                2\n",
      "35          0.760  d8976_00035              5        5 0.010     0.496           1          16          256              1                3\n",
      "36          0.769  d8976_00036              5        5 0.010     0.128           1          16           32              2                1\n",
      "37          0.767  d8976_00037              5        5 0.010     0.827           1          16           32              2                2\n",
      "38          0.767  d8976_00038              5        5 0.010     0.307           1          16           32              2                3\n",
      "39          0.765  d8976_00039              5        5 0.010     0.630           1          16           64              2                1\n",
      "40          0.765  d8976_00040              5        5 0.010     0.349           1          16           64              2                2\n",
      "41          0.762  d8976_00041              5        5 0.010     0.516           1          16           64              2                3\n",
      "42          0.756  d8976_00042              5        5 0.010     0.537           1          16          128              2                1\n",
      "43          0.759  d8976_00043              5        5 0.010     0.248           1          16          128              2                2\n",
      "44          0.768  d8976_00044              5        5 0.010     0.876           1          16          128              2                3\n",
      "45          0.764  d8976_00045              5        5 0.010     0.720           1          16          256              2                1\n",
      "46          0.762  d8976_00046              5        5 0.010     0.852           1          16          256              2                2\n",
      "47          0.759  d8976_00047              5        5 0.010     0.816           1          16          256              2                3\n",
      "    mean_accuracy     trial_id  return_period  seq_len    lr  momentum  optim_type  num_layers  hidden_size  num_fc_layers  activation_type\n",
      "4           0.782  d8976_00004              5        5 0.010     0.116           1           4           64              1                2\n",
      "2           0.781  d8976_00002              5        5 0.010     0.581           1           4           32              1                3\n",
      "15          0.777  d8976_00015              5        5 0.010     0.212           1           4           64              2                1\n",
      "3           0.775  d8976_00003              5        5 0.010     0.666           1           4           64              1                1\n",
      "14          0.775  d8976_00014              5        5 0.010     0.589           1           4           32              2                3\n",
      "8           0.774  d8976_00008              5        5 0.010     0.245           1           4          128              1                3\n",
      "1           0.770  d8976_00001              5        5 0.010     0.793           1           4           32              1                2\n",
      "36          0.769  d8976_00036              5        5 0.010     0.128           1          16           32              2                1\n",
      "44          0.768  d8976_00044              5        5 0.010     0.876           1          16          128              2                3\n",
      "30          0.768  d8976_00030              5        5 0.010     0.344           1          16          128              1                1\n",
      "32          0.767  d8976_00032              5        5 0.010     0.647           1          16          128              1                3\n",
      "27          0.767  d8976_00027              5        5 0.010     0.859           1          16           64              1                1\n",
      "37          0.767  d8976_00037              5        5 0.010     0.827           1          16           32              2                2\n",
      "38          0.767  d8976_00038              5        5 0.010     0.307           1          16           32              2                3\n",
      "25          0.767  d8976_00025              5        5 0.010     0.236           1          16           32              1                2\n",
      "39          0.765  d8976_00039              5        5 0.010     0.630           1          16           64              2                1\n",
      "40          0.765  d8976_00040              5        5 0.010     0.349           1          16           64              2                2\n",
      "31          0.765  d8976_00031              5        5 0.010     0.178           1          16          128              1                2\n",
      "45          0.764  d8976_00045              5        5 0.010     0.720           1          16          256              2                1\n",
      "29          0.764  d8976_00029              5        5 0.010     0.747           1          16           64              1                3\n",
      "12          0.764  d8976_00012              5        5 0.010     0.446           1           4           32              2                1\n",
      "28          0.764  d8976_00028              5        5 0.010     0.873           1          16           64              1                2\n",
      "7           0.763  d8976_00007              5        5 0.010     0.270           1           4          128              1                2\n",
      "33          0.763  d8976_00033              5        5 0.010     0.452           1          16          256              1                1\n",
      "46          0.762  d8976_00046              5        5 0.010     0.852           1          16          256              2                2\n",
      "41          0.762  d8976_00041              5        5 0.010     0.516           1          16           64              2                3\n",
      "26          0.762  d8976_00026              5        5 0.010     0.152           1          16           32              1                3\n",
      "21          0.761  d8976_00021              5        5 0.010     0.511           1           4          256              2                1\n",
      "16          0.760  d8976_00016              5        5 0.010     0.334           1           4           64              2                2\n",
      "5           0.760  d8976_00005              5        5 0.010     0.876           1           4           64              1                3\n",
      "17          0.760  d8976_00017              5        5 0.010     0.393           1           4           64              2                3\n",
      "0           0.760  d8976_00000              5        5 0.010     0.146           1           4           32              1                1\n",
      "35          0.760  d8976_00035              5        5 0.010     0.496           1          16          256              1                3\n",
      "47          0.759  d8976_00047              5        5 0.010     0.816           1          16          256              2                3\n",
      "43          0.759  d8976_00043              5        5 0.010     0.248           1          16          128              2                2\n",
      "18          0.758  d8976_00018              5        5 0.010     0.465           1           4          128              2                1\n",
      "13          0.758  d8976_00013              5        5 0.010     0.333           1           4           32              2                2\n",
      "24          0.758  d8976_00024              5        5 0.010     0.586           1          16           32              1                1\n",
      "19          0.758  d8976_00019              5        5 0.010     0.728           1           4          128              2                2\n",
      "34          0.757  d8976_00034              5        5 0.010     0.198           1          16          256              1                2\n",
      "23          0.757  d8976_00023              5        5 0.010     0.137           1           4          256              2                3\n",
      "42          0.756  d8976_00042              5        5 0.010     0.537           1          16          128              2                1\n",
      "20          0.755  d8976_00020              5        5 0.010     0.260           1           4          128              2                3\n",
      "11          0.753  d8976_00011              5        5 0.010     0.520           1           4          256              1                3\n",
      "9           0.753  d8976_00009              5        5 0.010     0.247           1           4          256              1                1\n",
      "6           0.750  d8976_00006              5        5 0.010     0.766           1           4          128              1                1\n",
      "10          0.750  d8976_00010              5        5 0.010     0.343           1           4          256              1                2\n",
      "22          0.738  d8976_00022              5        5 0.010     0.574           1           4          256              2                2\n",
      "/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/2024-01-31_07.46.06/5_5_0.01_0.11646759543664197_1_4_64_1_2.pt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "config_df = pd.DataFrame(analysis.get_all_configs().values())\n",
    "print(config_df)\n",
    "\n",
    "results = pd.concat([accuracy_df, config_df], axis=1)\n",
    "print(results)\n",
    "\n",
    "sorted_results = results.sort_values(by=\"mean_accuracy\", ascending=False)\n",
    "print(sorted_results.head(100))\n",
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results.to_csv(sorted_results_file)\n",
    "\n",
    "best_config = config_df.iloc[sorted_results.index[0]]\n",
    "id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in best_config.to_list())\n",
    "best_model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/AIWorkSpace/work/fin-ml/runs/StockPCTLabelPredictLSTM/StockPCTLabelPredictLSTM.pt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(best_model_name, f\"{log_dir_base}/{task_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_desc = sorted_results[\"mean_accuracy\"].astype(\"float32\").describe()\n",
    "xlimit_range = [\n",
    "    accuracy_desc[\"min\"] - accuracy_desc[\"std\"],\n",
    "    accuracy_desc[\"max\"] + accuracy_desc[\"std\"],\n",
    "]\n",
    "for hperparameter_name in turning_parameters:\n",
    "    parameter_group = sorted_results.groupby(hperparameter_name)\n",
    "    fix, axs = pyplot.subplots(\n",
    "        1,\n",
    "        len(parameter_group),\n",
    "        layout=\"constrained\",\n",
    "        sharex=False,\n",
    "        sharey=True,\n",
    "        figsize=(12, 2),\n",
    "    )\n",
    "    for i, g in enumerate(parameter_group):\n",
    "        g[1][\"mean_accuracy\"].astype(\"float32\").plot(\n",
    "            kind=\"hist\", bins=50, subplots=True, sharex=False, sharey=True, ax=axs[i]\n",
    "        )\n",
    "        axs[i].set_title(f\"{hperparameter_name}_{g[0]}\")\n",
    "\n",
    "pyplot.xlim(xlimit_range)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                           4\n",
      "mean_accuracy       0.7823684782608694\n",
      "trial_id                   d8976_00004\n",
      "return_period                        5\n",
      "seq_len                              5\n",
      "lr                                0.01\n",
      "momentum           0.11646759543664197\n",
      "optim_type                           1\n",
      "num_layers                           4\n",
      "hidden_size                         64\n",
      "num_fc_layers                        1\n",
      "activation_type                      2\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sorted_results_file = f\"{log_dir}/sorted_results.csv\"\n",
    "sorted_results = pd.read_csv(sorted_results_file, dtype=\"str\")\n",
    "best_config = sorted_results.loc[0]\n",
    "print(best_config)\n",
    "# id_str_of_best = f\"5_5_0.01_{best_config.momentum}_{best_config.optim_type}_{best_config.num_layers}_{best_config.hidden_size}_{best_config.num_fc_layers}_{best_config.activation_type}\"\n",
    "# best_model_name = f\"/mnt/AIWorkSpace/work/fin-ml/runs/{_TARGET_STK}/{time_str}/{id_str_of_best}.pt\"\n",
    "# print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.87\n",
      "Test Accuracy: 0.76500\n",
      "Train F1: 0.00\n",
      "Test F1: 0.00000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "model, config = load_model(f\"{log_dir_base}/{task_name}.pt\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader, features_size = prepare_dataloader(config[\"return_period\"])\n",
    "model.eval()\n",
    "\n",
    "(trainAccuracy, trainF1) = eval_dl_method(model, train_loader, device=device)\n",
    "(testAccuracy, testF1) = eval_dl_method(model, test_loader, device=device)\n",
    "print(f\"Train Accuracy: {trainAccuracy:.2f}\\nTest Accuracy: {testAccuracy:.5f}\")\n",
    "print(f\"Train F1: {trainF1:.2f}\\nTest F1: {testF1:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
