{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#logging\n",
    "from MyPyUtil.logconf import logging\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.ERROR)\n",
    "log.setLevel(logging.INFO)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "torch.seed = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%run 'nb_utils.ipynb'\n",
    "task_name = get_filename_of_ipynb()\n",
    "print(task_name)\n",
    "data_dir = f'{os.getcwd()}/data/'\n",
    "log_dir_base = f'{os.getcwd()}/runs/{task_name}'\n",
    "log_dir = log_dir_base\n",
    "print(f'{data_dir}\\n{log_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters turning\n",
    "from ray import tune, train, ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import yfinance as yfin\n",
    "from MyPyUtil.util import is_contained\n",
    "\n",
    "# Loading the data\n",
    "stk_symbols = [\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"AMZN\",\n",
    "    \"NVDA\",\n",
    "    \"GOOGL\",\n",
    "    \"GOOG\",\n",
    "    \"META\",\n",
    "    \"TSLA\",\n",
    "    \"UNH\",\n",
    "    \"LLY\",\n",
    "    \"JPM\",\n",
    "    \"XOM\",\n",
    "    \"JNJ\",\n",
    "    \"V\",\n",
    "    \"PG\",\n",
    "    \"AVGO\",\n",
    "    \"MA\",\n",
    "    \"HD\",\n",
    "    \"CVX\",\n",
    "    \"MRK\",\n",
    "    \"ABBV\",\n",
    "    \"PEP\",\n",
    "    \"COST\",\n",
    "    \"ADBE\",\n",
    "    \"KO\",\n",
    "    \"CSCO\",\n",
    "    \"WMT\",\n",
    "    \"TMO\",\n",
    "    \"MCD\",\n",
    "    \"PFE\",\n",
    "    \"CRM\",\n",
    "    \"BAC\",\n",
    "    \"ACN\",\n",
    "    \"CMCSA\",\n",
    "    \"LIN\",\n",
    "    \"NFLX\",\n",
    "    \"ABT\",\n",
    "    \"ORCL\",\n",
    "    \"DHR\",\n",
    "    \"AMD\",\n",
    "    \"WFC\",\n",
    "    \"DIS\",\n",
    "    \"TXN\",\n",
    "    \"PM\",\n",
    "    \"VZ\",\n",
    "    \"INTU\",\n",
    "    \"COP\",\n",
    "    \"CAT\",\n",
    "    \"AMGN\",\n",
    "    \"NEE\",\n",
    "    \"INTC\",\n",
    "    \"UNP\",\n",
    "    \"LOW\",\n",
    "    \"IBM\",\n",
    "    \"BMY\",\n",
    "    \"SPGI\",\n",
    "    \"RTX\",\n",
    "    \"HON\",\n",
    "    \"BA\",\n",
    "    \"UPS\",\n",
    "    \"GE\",\n",
    "    \"QCOM\",\n",
    "    \"AMAT\",\n",
    "    \"NKE\",\n",
    "    \"PLD\",\n",
    "    \"NOW\",\n",
    "    \"BKNG\",\n",
    "    \"SBUX\",\n",
    "    \"MS\",\n",
    "    \"ELV\",\n",
    "    \"MDT\",\n",
    "    \"GS\",\n",
    "    \"DE\",\n",
    "    \"ADP\",\n",
    "    \"LMT\",\n",
    "    \"TJX\",\n",
    "    \"T\",\n",
    "    \"BLK\",\n",
    "    \"ISRG\",\n",
    "    \"MDLZ\",\n",
    "    \"GILD\",\n",
    "    \"MMC\",\n",
    "    \"AXP\",\n",
    "    \"SYK\",\n",
    "    \"REGN\",\n",
    "    \"VRTX\",\n",
    "    \"ETN\",\n",
    "    \"LRCX\",\n",
    "    \"ADI\",\n",
    "    \"SCHW\",\n",
    "    \"CVS\",\n",
    "    \"ZTS\",\n",
    "    \"CI\",\n",
    "    \"CB\",\n",
    "    \"AMT\",\n",
    "    \"SLB\",\n",
    "    \"C\",\n",
    "    \"BDX\",\n",
    "    \"MO\",\n",
    "    \"PGR\",\n",
    "    \"TMUS\",\n",
    "    \"FI\",\n",
    "    \"SO\",\n",
    "    \"EOG\",\n",
    "    \"BSX\",\n",
    "    \"CME\",\n",
    "    \"EQIX\",\n",
    "    \"MU\",\n",
    "    \"DUK\",\n",
    "    \"PANW\",\n",
    "    \"PYPL\",\n",
    "    \"AON\",\n",
    "    \"SNPS\",\n",
    "    \"ITW\",\n",
    "    \"KLAC\",\n",
    "    \"LULU\",\n",
    "    \"ICE\",\n",
    "    \"APD\",\n",
    "    \"SHW\",\n",
    "    \"CDNS\",\n",
    "    \"CSX\",\n",
    "    \"NOC\",\n",
    "    \"CL\",\n",
    "    \"MPC\",\n",
    "    \"HUM\",\n",
    "    \"FDX\",\n",
    "    \"WM\",\n",
    "    \"MCK\",\n",
    "    \"TGT\",\n",
    "    \"ORLY\",\n",
    "    \"HCA\",\n",
    "    \"FCX\",\n",
    "    \"EMR\",\n",
    "    \"PXD\",\n",
    "    \"MMM\",\n",
    "    \"MCO\",\n",
    "    \"ROP\",\n",
    "    \"CMG\",\n",
    "    \"PSX\",\n",
    "    \"MAR\",\n",
    "    \"PH\",\n",
    "    \"APH\",\n",
    "    \"GD\",\n",
    "    \"USB\",\n",
    "    \"NXPI\",\n",
    "    \"AJG\",\n",
    "    \"NSC\",\n",
    "    \"PNC\",\n",
    "    \"VLO\",\n",
    "    \"GBP\",\n",
    "    \"F\",\n",
    "    \"MSI\",\n",
    "    \"GM\",\n",
    "    \"TT\",\n",
    "    \"EW\",\n",
    "    \"CARR\",\n",
    "    \"AZO\",\n",
    "    \"ADSK\",\n",
    "    \"TDG\",\n",
    "    \"ANET\",\n",
    "    \"SRE\",\n",
    "    \"ECL\",\n",
    "    \"OXY\",\n",
    "    \"PCAR\",\n",
    "    \"ADM\",\n",
    "    \"MNST\",\n",
    "    \"KMB\",\n",
    "    \"PSA\",\n",
    "    \"CCI\",\n",
    "    \"CHTR\",\n",
    "    \"MCHP\",\n",
    "    \"MSCI\",\n",
    "    \"CTAS\",\n",
    "    \"WMB\",\n",
    "    \"AIG\",\n",
    "    \"STZ\",\n",
    "    \"HES\",\n",
    "    \"NUE\",\n",
    "    \"ROST\",\n",
    "    \"AFL\",\n",
    "    \"AEP\",\n",
    "    \"IDXX\",\n",
    "    \"D\",\n",
    "    \"TEL\",\n",
    "    \"JCI\",\n",
    "    \"MET\",\n",
    "    \"GIS\",\n",
    "    \"IQV\",\n",
    "    \"EXC\",\n",
    "    \"WELL\",\n",
    "    \"DXCM\",\n",
    "    \"HLT\",\n",
    "    \"ON\",\n",
    "    \"COF\",\n",
    "    \"PAYX\",\n",
    "    \"TFC\",\n",
    "    \"USD\",\n",
    "    \"BIIB\",\n",
    "    \"O\",\n",
    "    \"FTNT\",\n",
    "    \"DOW\",\n",
    "    \"TRV\",\n",
    "    \"DLR\",\n",
    "    \"MRNA\",\n",
    "    \"CPRT\",\n",
    "    \"ODFL\",\n",
    "    \"DHI\",\n",
    "    \"YUM\",\n",
    "    \"SPG\",\n",
    "    \"CTSH\",\n",
    "    \"AME\",\n",
    "    \"BKR\",\n",
    "    \"SYY\",\n",
    "    \"A\",\n",
    "    \"CTVA\",\n",
    "    \"CNC\",\n",
    "    \"EL\",\n",
    "    \"AMP\",\n",
    "    # \"CEG\",  # PCT <= -0.05,  size = 0\n",
    "    \"HAL\",\n",
    "    # \"OTIS\",  # PCT <= -0.05,  size = 0\n",
    "    \"ROK\",\n",
    "    \"PRU\",\n",
    "    \"DD\",\n",
    "    \"KMI\",\n",
    "    \"VRSK\",\n",
    "    \"LHX\",\n",
    "    \"DG\",\n",
    "    \"FIS\",\n",
    "    \"CMI\",\n",
    "    \"CSGP\",\n",
    "    \"FAST\",\n",
    "    \"PPG\",\n",
    "    \"GPN\",\n",
    "    \"GWW\",\n",
    "    \"HSY\",\n",
    "    \"BK\",\n",
    "    \"XEL\",\n",
    "    \"DVN\",\n",
    "    \"EA\",\n",
    "    \"NEM\",\n",
    "    \"ED\",\n",
    "    \"URI\",\n",
    "    \"VICI\",\n",
    "    \"PEG\",\n",
    "    \"KR\",\n",
    "    \"RSG\",\n",
    "    \"LEN\",\n",
    "    \"PWR\",\n",
    "    \"WST\",\n",
    "    \"COR\",\n",
    "    \"OKE\",\n",
    "    \"VMC\",\n",
    "    \"KDP\",\n",
    "    \"WBD\",\n",
    "    \"ACGL\",\n",
    "    \"ALL\",\n",
    "    \"IR\",\n",
    "    \"CDW\",\n",
    "    \"FANG\",\n",
    "    \"MLM\",\n",
    "    \"PCG\",\n",
    "    \"DAL\",\n",
    "    \"EXR\",\n",
    "    \"FTV\",\n",
    "    \"AWK\",\n",
    "    \"IT\",\n",
    "    \"KHC\",\n",
    "    # \"GEHC\",  # PCT <= -0.05,  size = 0\n",
    "    \"WEC\",\n",
    "    \"HPQ\",\n",
    "    \"EIX\",\n",
    "    \"CBRE\",\n",
    "    \"APTV\",\n",
    "    \"ANSS\",\n",
    "    \"MTD\",\n",
    "    \"DLTR\",\n",
    "    \"AVB\",\n",
    "    \"ILMN\",\n",
    "    \"ALGN\",\n",
    "    \"LYB\",\n",
    "    \"TROW\",\n",
    "    \"GLW\",\n",
    "    \"EFX\",\n",
    "    \"WY\",\n",
    "    \"ZBH\",\n",
    "    \"XYL\",\n",
    "    \"SBAC\",\n",
    "    \"RMD\",\n",
    "    \"TSCO\",\n",
    "    \"EBAY\",\n",
    "    \"KEYS\",\n",
    "    \"CHD\",\n",
    "    \"STT\",\n",
    "    \"DFS\",\n",
    "    \"HIG\",\n",
    "    \"ALB\",\n",
    "    \"STE\",\n",
    "    \"ES\",\n",
    "    \"TTWO\",\n",
    "    \"MPWR\",\n",
    "    \"CAH\",\n",
    "    \"EQR\",\n",
    "    \"RCL\",\n",
    "    \"WTW\",\n",
    "    \"HPE\",\n",
    "    \"DTE\",\n",
    "    \"GPC\",\n",
    "    \"BR\",\n",
    "    \"ULTA\",\n",
    "    \"FICO\",\n",
    "    \"CTRA\",\n",
    "    \"BAX\",\n",
    "    \"AEE\",\n",
    "    \"MTB\",\n",
    "    \"MKC\",\n",
    "    \"ETR\",\n",
    "    \"WAB\",\n",
    "    \"DOV\",\n",
    "    \"FE\",\n",
    "    \"RJF\",\n",
    "    \"INVH\",\n",
    "    \"FLT\",\n",
    "    \"CLX\",\n",
    "    \"TDY\",\n",
    "    \"TRGP\",\n",
    "    \"DRI\",\n",
    "    \"LH\",\n",
    "    \"HOLX\",\n",
    "    \"VRSN\",\n",
    "    \"MOH\",\n",
    "    \"LUV\",\n",
    "    \"PPL\",\n",
    "    \"ARE\",\n",
    "    \"NVR\",\n",
    "    \"COO\",\n",
    "    \"WBA\",\n",
    "    \"PHM\",\n",
    "    \"NDAQ\",\n",
    "    \"HWM\",\n",
    "    \"RF\",\n",
    "    \"CNP\",\n",
    "    \"IRM\",\n",
    "    \"LVS\",\n",
    "    \"FITB\",\n",
    "    \"EXPD\",\n",
    "    \"VTR\",\n",
    "    \"FSLR\",\n",
    "    \"PFG\",\n",
    "    \"BRO\",\n",
    "    \"J\",\n",
    "    \"IEX\",\n",
    "    \"BG\",\n",
    "    \"ATO\",\n",
    "    \"FDS\",\n",
    "    \"ENPH\",\n",
    "    \"MAA\",\n",
    "    \"CMS\",\n",
    "    \"IFF\",\n",
    "    \"BALL\",\n",
    "    \"SWKS\",\n",
    "    \"CINF\",\n",
    "    \"NTAP\",\n",
    "    \"STLD\",\n",
    "    \"UAL\",\n",
    "    \"WAT\",\n",
    "    \"OMC\",\n",
    "    \"TER\",\n",
    "    \"CCL\",\n",
    "    \"JBHT\",\n",
    "    \"MRO\",\n",
    "    \"TYL\",\n",
    "    \"HBAN\",\n",
    "    \"K\",\n",
    "    \"GRMN\",\n",
    "    \"CBOE\",\n",
    "    \"NTRS\",\n",
    "    \"TSN\",\n",
    "    \"AKAM\",\n",
    "    \"EG\",\n",
    "    \"ESS\",\n",
    "    \"EQT\",\n",
    "    \"TXT\",\n",
    "    \"EXPE\",\n",
    "    \"SJM\",\n",
    "    \"PTC\",\n",
    "    \"DGX\",\n",
    "    \"AVY\",\n",
    "    \"RVTY\",\n",
    "    \"BBY\",\n",
    "    \"CF\",\n",
    "    \"CAG\",\n",
    "    \"EPAM\",\n",
    "    \"AMCR\",\n",
    "    \"LW\",\n",
    "    \"PAYC\",\n",
    "    \"SNA\",\n",
    "    \"AXON\",\n",
    "    \"POOL\",\n",
    "    \"SYF\",\n",
    "    \"SWK\",\n",
    "    \"ZBRA\",\n",
    "    \"DPZ\",\n",
    "    \"PKG\",\n",
    "    \"CFG\",\n",
    "    \"LDOS\",\n",
    "    \"VTRS\",\n",
    "    \"PODD\",\n",
    "    \"LKQ\",\n",
    "    \"MOS\",\n",
    "    \"APA\",\n",
    "    \"EVRG\",\n",
    "    \"TRMB\",\n",
    "    \"MGM\",\n",
    "    \"NDSN\",\n",
    "    \"WDC\",\n",
    "    \"MAS\",\n",
    "    \"LNT\",\n",
    "    \"IPG\",\n",
    "    \"MTCH\",\n",
    "    \"STX\",\n",
    "    \"KMX\",\n",
    "    \"TECH\",\n",
    "    \"WRB\",\n",
    "    \"LYV\",\n",
    "    \"IP\",\n",
    "    \"UDR\",\n",
    "    \"AES\",\n",
    "    \"CE\",\n",
    "    \"INCY\",\n",
    "    \"L\",\n",
    "    \"TAP\",\n",
    "    \"GEN\",\n",
    "    \"CPT\",\n",
    "    \"KIM\",\n",
    "    \"JKHY\",\n",
    "    \"HRL\",\n",
    "    \"HST\",\n",
    "    \"FMC\",\n",
    "    \"CZR\",\n",
    "    \"PEAK\",\n",
    "    \"CDAY\",\n",
    "    \"PNR\",\n",
    "    \"NI\",\n",
    "    \"CHRW\",\n",
    "    \"HSIC\",\n",
    "    \"CRL\",\n",
    "    \"REG\",\n",
    "    \"QRVO\",\n",
    "    \"TFX\",\n",
    "    \"KEY\",\n",
    "    \"GL\",\n",
    "    \"EMN\",\n",
    "    \"WYNN\",\n",
    "    \"ALLE\",\n",
    "    \"AAL\",\n",
    "    \"FFIV\",\n",
    "    \"BWA\",\n",
    "    \"BXP\",\n",
    "    \"MKTX\",\n",
    "    \"ROL\",\n",
    "    \"JNPR\",\n",
    "    \"PNW\",\n",
    "    \"ETSY\",\n",
    "    \"BLDR\",\n",
    "    \"FOXA\",\n",
    "    \"AOS\",\n",
    "    \"HAS\",\n",
    "    \"HII\",\n",
    "    \"NRG\",\n",
    "    \"CPB\",\n",
    "    \"UHS\",\n",
    "    \"BIO\",\n",
    "    \"WRK\",\n",
    "    \"RHI\",\n",
    "    \"CTLT\",\n",
    "    \"XRAY\",\n",
    "    \"BBWI\",\n",
    "    \"NWSA\",\n",
    "    \"TPR\",\n",
    "    \"PARA\",\n",
    "    \"WHR\",\n",
    "    \"BEN\",\n",
    "    \"AIZ\",\n",
    "    \"NCLH\",\n",
    "    \"GNRC\",\n",
    "    \"FRT\",\n",
    "    \"IVZ\",\n",
    "    \"VFC\",\n",
    "    \"CMA\",\n",
    "    \"DVA\",\n",
    "    \"JBL\",\n",
    "    \"HUBB\",\n",
    "    \"ZION\",\n",
    "    \"UBER\",\n",
    "    \"MHK\",\n",
    "    \"RL\",\n",
    "    \"FOX\",\n",
    "    \"BX\",\n",
    "    \"ABNB\",\n",
    "    \"NWS\",\n",
    "]\n",
    "# stk_symbols = [\n",
    "#     \"AAPL\",\n",
    "#     \"MSFT\",\n",
    "#     \"AMZN\",\n",
    "#     \"NVDA\",\n",
    "#     \"GOOGL\",\n",
    "#     \"TSLA\",\n",
    "#     \"META\",\n",
    "#     \"GOOG\",\n",
    "#     \"ADBE\",\n",
    "#     \"NFLX\",\n",
    "#     \"CSCO\",\n",
    "#     \"INTC\",\n",
    "#     \"INTU\",\n",
    "#     \"CMCSA\",\n",
    "#     \"TXN\",\n",
    "#     \"AMAT\",\n",
    "#     \"ADSK\",\n",
    "#     \"AMD\",\n",
    "#     \"QCOM\",\n",
    "#     \"MU\",\n",
    "# ]\n",
    "\n",
    "# stk_symbols = [\n",
    "#     \"AAPL\",\n",
    "#     \"MSFT\",\n",
    "#     \"AMZN\",\n",
    "#     \"NVDA\",\n",
    "#     \"GOOGL\",\n",
    "#     \"TSLA\",\n",
    "#     \"META\",\n",
    "#     \"GOOG\",\n",
    "# ]\n",
    "\n",
    "empty_vol_threshold = 5\n",
    "start = datetime(2014, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "ticks_data = []\n",
    "for symbol in stk_symbols:\n",
    "    stk_file = f\"{data_dir}{symbol}.csv\"\n",
    "    bLoad = False\n",
    "    if os.path.isfile(stk_file):\n",
    "        try:\n",
    "            _stk_data = pd.read_csv(stk_file).set_index(\"Date\")\n",
    "            bLoad = True\n",
    "            print(f\"read {stk_file} completely!\")\n",
    "        except:\n",
    "            None\n",
    "    if bLoad == False:\n",
    "        # _stk_data = web.get_data_yahoo(stk_tickers, start, end)\n",
    "        _stk_data = yfin.download([symbol], start, end).dropna()\n",
    "        _stk_data.to_csv(stk_file)\n",
    "        print(f\"download {symbol} from yfin and write to {stk_file} completely!\")\n",
    "\n",
    "    statistics = _stk_data.describe()\n",
    "    if is_contained(statistics, 0):\n",
    "        if is_contained(\n",
    "            statistics.loc[:, [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]], 0\n",
    "        ) or is_contained(statistics.loc[\"std\"], 0):\n",
    "            print(f\"{symbol}: contains numerical errors. Ignore it.\")\n",
    "            continue\n",
    "        else:\n",
    "            empty_vol_index = _stk_data[_stk_data[\"Volume\"] == 0].index\n",
    "            if len(empty_vol_index) > empty_vol_threshold:\n",
    "                print(\n",
    "                    f\"The total volume with a value of zero ({len(empty_vol_index)}) is greater than the threshold({empty_vol_threshold}). Ignore it.\"\n",
    "                )\n",
    "                continue\n",
    "            print(\n",
    "                f\"A total of {len(empty_vol_index)} volume values ​​are zero. Delete these data.\"\n",
    "            )\n",
    "\n",
    "            cleaned_data = _stk_data.drop(empty_vol_index)\n",
    "            print(\n",
    "                f\"The cleaned data size is {len(cleaned_data)}. The original data size is {len(_stk_data)}.\"\n",
    "            )\n",
    "            if len(cleaned_data) == 0:\n",
    "                continue\n",
    "            _stk_data = cleaned_data\n",
    "\n",
    "    ticks_data.append(_stk_data)\n",
    "    print(f\"{symbol}, size:{len(_stk_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_data = ticks_data[39]\n",
    "print(len(stk_data))\n",
    "describe = stk_data.describe()\n",
    "print(describe)\n",
    "aa = (describe == 0).any(axis=1)\n",
    "print(len(describe[aa]))\n",
    "print(is_contained(describe, 0))\n",
    "print((stk_data[\"Volume\"] == 0).sum())\n",
    "if (stk_data[\"Volume\"] == 0).sum() > 0:\n",
    "    aa = stk_data.drop(stk_data[stk_data[\"Volume\"] == 0].index)\n",
    "    print(aa.describe())\n",
    "# print(pd.isnu describe[describe > 0])\n",
    "# for i, stk_data in enumerate(ticks_data):\n",
    "#     if (stk_data.describe() == 0).sum() > 0:\n",
    "#         print(f\"{i}, {stk_symbols[i]}\\n,{stk_data.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_name = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_name)\n",
    "return_period = 5\n",
    "seq_len = 3\n",
    "validation_size = 0.1\n",
    "epoch_num = 100\n",
    "batch_size = 32\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "shuffle = True\n",
    "print(f\"device_name:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_threshold = 0.05\n",
    "class_percentage_threshold = 0.01  # percentage threshold for class size\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "\n",
    "# # number of classes = 3\n",
    "# # 0: PCT <= -0.05\n",
    "# # 1: 0.05 < PCT < -0.05\n",
    "# # 2: PCT >= 0.05\n",
    "# num_classes = 3\n",
    "\n",
    "\n",
    "# def gen_pct_label(stk_data, _return_period):\n",
    "#     max_price_period = (\n",
    "#         stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "#     )\n",
    "#     max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "#     pct_label = max_pct_period.apply(\n",
    "#         lambda x: 2 if x >= pct_threshold else 0 if x <= -pct_threshold else 1\n",
    "#     ).astype(\"int8\")\n",
    "#     pct_label.name = \"label\"\n",
    "#     return pct_label\n",
    "\n",
    "\n",
    "# number of classes = 2\n",
    "# 0: PCT < 0.05\n",
    "# 1: PCT >= -0.05\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "def gen_pct_label(stk_data, _return_period):\n",
    "    max_price_period = (\n",
    "        stk_data[\"Adj Close\"].rolling(_return_period).max().shift(-_return_period)\n",
    "    )\n",
    "    max_pct_period = (max_price_period - stk_data[\"Adj Close\"]) / stk_data[\"Adj Close\"]\n",
    "    pct_label = max_pct_period.apply(lambda x: 1 if x >= pct_threshold else 0).astype(\n",
    "        \"int8\"\n",
    "    )\n",
    "    pct_label.name = \"label\"\n",
    "    return pct_label\n",
    "\n",
    "\n",
    "def class_percentage(analysis_data):\n",
    "    stat = analysis_data.groupby(\"label\").size()\n",
    "    total = len(analysis_data)\n",
    "    p = []\n",
    "    for i in range(num_classes):\n",
    "        p.append(stat[i] / total if i in stat.index else 0.0)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_adjusted(stk_data):\n",
    "    \"\"\"\n",
    "    Adjusted Open = Open * Adjusted Close / Close\n",
    "    Adjusted High = High * Adjusted Close / Close\n",
    "    Adjusted Low = Low * Adjusted Close / Close\n",
    "    Adjusted volume = Volume / (Adjusted Close / Close)\n",
    "    \"\"\"\n",
    "    ratio_data = stk_data[\"Adj Close\"] / stk_data[\"Close\"]\n",
    "    adjusted_OHLV = pd.DataFrame(index=stk_data.index)\n",
    "    adjusted_OHLV[\"Adj Open\"] = ratio_data * stk_data[\"Open\"]\n",
    "    adjusted_OHLV[\"Adj High\"] = ratio_data * stk_data[\"High\"]\n",
    "    adjusted_OHLV[\"Adj Low\"] = ratio_data * stk_data[\"Low\"]\n",
    "    adjusted_OHLV[\"Adj Close\"] = stk_data[\"Adj Close\"]\n",
    "    adjusted_OHLV[\"Adj Volume\"] = (ratio_data * stk_data[\"Volume\"]).astype(\"int\")\n",
    "    adjusted_OHLV[\"Pre Adj Close\"] = stk_data[\"Adj Close\"].shift(1)\n",
    "    adjusted_OHLV[\"Pre Adj Volume\"] = adjusted_OHLV[\"Adj Volume\"].shift(1)\n",
    "    adjusted_OHLV = adjusted_OHLV.dropna()\n",
    "    adjusted_OHLV[\"Pre Adj Volume\"] = adjusted_OHLV[\"Pre Adj Volume\"].astype(\"int\")\n",
    "    adjusted_OHLV[\"B4_Adj Open pct\"] = (\n",
    "        adjusted_OHLV[\"Adj Open\"] - adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    ) / adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    adjusted_OHLV[\"B4_Adj High pct\"] = (\n",
    "        adjusted_OHLV[\"Adj High\"] - adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    ) / adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    adjusted_OHLV[\"B4_Adj Low pct\"] = (\n",
    "        adjusted_OHLV[\"Adj Low\"] - adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    ) / adjusted_OHLV[\"Pre Adj Close\"]\n",
    "    adjusted_OHLV[\"B4_Adj Volume pct\"] = (\n",
    "        adjusted_OHLV[\"Adj Volume\"] - adjusted_OHLV[\"Pre Adj Volume\"]\n",
    "    ) / adjusted_OHLV[\"Pre Adj Volume\"]\n",
    "\n",
    "    return adjusted_OHLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "# help(ta.adosc)\n",
    "\n",
    "stk_data = ticks_data[0]\n",
    "adjusted_OHLV = to_adjusted(stk_data)\n",
    "data = pd.concat(\n",
    "    [\n",
    "        # stk_data.ta.adosc(prefix=\"B1\"),\n",
    "        # stk_data.ta.kvo(prefix=\"B1\"),\n",
    "        adjusted_OHLV.ta.adosc(\n",
    "            high=\"Adj High\",\n",
    "            low=\"Adj Low\",\n",
    "            close=\"Adj Close\",\n",
    "            volume=\"Adj Volume\",\n",
    "            prefix=\"B2\",\n",
    "        ),\n",
    "        adjusted_OHLV.ta.kvo(\n",
    "            high=\"Adj High\",\n",
    "            low=\"Adj Low\",\n",
    "            close=\"Adj Close\",\n",
    "            volume=\"Adj Volume\",\n",
    "            prefix=\"B2\",\n",
    "        ),\n",
    "        stk_data.ta.rsi(close=\"Adj Close\", length=10, prefix=\"B3\") / 100,\n",
    "        stk_data.ta.rsi(close=\"Adj Close\", length=30, prefix=\"B3\") / 100,\n",
    "        stk_data.ta.rsi(close=\"Adj Close\", length=200, prefix=\"B3\") / 100,\n",
    "        stk_data.ta.stoch(k=10, prefix=\"B3\") / 100,\n",
    "        stk_data.ta.stoch(k=30, prefix=\"B3\") / 100,\n",
    "        stk_data.ta.stoch(k=200, prefix=\"B3\") / 100,\n",
    "        adjusted_OHLV.loc[\n",
    "            :,\n",
    "            [\n",
    "                \"B4_Adj Open pct\",\n",
    "                \"B4_Adj High pct\",\n",
    "                \"B4_Adj Low pct\",\n",
    "                \"B4_Adj Volume pct\",\n",
    "            ],\n",
    "        ],\n",
    "        gen_buy_sell_signal(stk_data),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "data = pd.concat(\n",
    "    [data.astype(\"float32\"), gen_pct_label(stk_data, return_period)],\n",
    "    axis=1,\n",
    ").dropna()\n",
    "cols = data.columns.values\n",
    "print([c for c in cols if c.startswith(\"B1\") or c.startswith(\"B3\") or c == \"Signal\"])\n",
    "print([c for c in cols if c.startswith(\"B2\") or c.startswith(\"B3\") or c == \"Signal\"])\n",
    "print(\n",
    "    [\n",
    "        c\n",
    "        for c in cols\n",
    "        if c.startswith(\"B1\")\n",
    "        or c.startswith(\"B3\")\n",
    "        or c.startswith(\"B4\")\n",
    "        or c == \"Signal\"\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    [\n",
    "        c\n",
    "        for c in cols\n",
    "        if c.startswith(\"B2\")\n",
    "        or c.startswith(\"B3\")\n",
    "        or c.startswith(\"B4\")\n",
    "        or c == \"Signal\"\n",
    "    ]\n",
    ")\n",
    "print(data)  # [['Adj_ADOSC_3_10','Adj_KVO_34_55_13','Adj_KVOs_34_55_13']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_buy_sell_signal(stk_data):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    sma = pd.concat(\n",
    "        [\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=10),\n",
    "            stk_data.ta.sma(close=\"Adj Close\", length=60),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    buy_signal = sma[\"SMA_10\"] > sma[\"SMA_60\"]\n",
    "\n",
    "    buy_sell_signal = stk_data[[]].copy()\n",
    "    buy_sell_signal[\"Signal\"] = (buy_signal).astype(\"int\")\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "\n",
    "def gen_analysis_data(stk_data, _return_period):\n",
    "    import pandas_ta as ta\n",
    "\n",
    "    adjusted_OHLV = to_adjusted(stk_data)\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            # stk_data.ta.adosc(prefix=\"B1\"),\n",
    "            # stk_data.ta.kvo(prefix=\"B1\"),\n",
    "            adjusted_OHLV.ta.adosc(\n",
    "                high=\"Adj High\",\n",
    "                low=\"Adj Low\",\n",
    "                close=\"Adj Close\",\n",
    "                volume=\"Adj Volume\",\n",
    "                prefix=\"B2\",\n",
    "            ),\n",
    "            adjusted_OHLV.ta.kvo(\n",
    "                high=\"Adj High\",\n",
    "                low=\"Adj Low\",\n",
    "                close=\"Adj Close\",\n",
    "                volume=\"Adj Volume\",\n",
    "                prefix=\"B2\",\n",
    "            ),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=10, prefix=\"B3\"),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=30, prefix=\"B3\"),\n",
    "            stk_data.ta.rsi(close=\"Adj Close\", length=200, prefix=\"B3\"),\n",
    "            stk_data.ta.stoch(k=10, prefix=\"B3\"),\n",
    "            stk_data.ta.stoch(k=30, prefix=\"B3\"),\n",
    "            stk_data.ta.stoch(k=200, prefix=\"B3\"),\n",
    "            adjusted_OHLV.loc[\n",
    "                :,\n",
    "                [\n",
    "                    \"B4_Adj Open pct\",\n",
    "                    \"B4_Adj High pct\",\n",
    "                    \"B4_Adj Low pct\",\n",
    "                    \"B4_Adj Volume pct\",\n",
    "                ],\n",
    "            ],\n",
    "            gen_buy_sell_signal(stk_data),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data = pd.concat(\n",
    "        [data.astype(\"float32\"), gen_pct_label(stk_data, _return_period)],\n",
    "        axis=1,\n",
    "    ).dropna()\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_analysis_data(_return_period, verbose=False):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    ticks_dataset = []\n",
    "    ignore_ticks_data_count = 0\n",
    "    for i, tick_data in enumerate(tqdm(ticks_data)):\n",
    "        analysis_data = gen_analysis_data(tick_data, _return_period)\n",
    "        classes_percentage = class_percentage(analysis_data)\n",
    "        if 0 in classes_percentage:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Some classes don't have any data  : {stk_symbols[i]}, {classes_percentage}\"\n",
    "                )\n",
    "            ignore_ticks_data_count += 1\n",
    "        elif any(p < class_percentage_threshold for p in classes_percentage):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Some classes are too small  : {stk_symbols[i]}, {classes_percentage}\"\n",
    "                )\n",
    "            ignore_ticks_data_count += 1\n",
    "        else:\n",
    "            ticks_dataset.append(analysis_data)\n",
    "    if ignore_ticks_data_count > 0:\n",
    "        print(\n",
    "            f\"There are {ignore_ticks_data_count} stocks in total, some classes have no data or are too small\"\n",
    "        )\n",
    "    return ticks_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from MyPyUtil.util import ivmax\n",
    "\n",
    "\n",
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, ticks_data_X, ticks_data_Y, _seq_len, balanced, pattern):\n",
    "        self.ticks_data_X = ticks_data_X\n",
    "        self.ticks_data_Y = ticks_data_Y\n",
    "        self.seq_len = _seq_len\n",
    "        self.balanced = balanced\n",
    "        self.pattern = pattern\n",
    "        self.pattern_size = 0 if pattern == None else len(pattern)\n",
    "        self.features_type = 4  # features_type\n",
    "\n",
    "        len_array = [len(d) - self.seq_len + 1 for d in ticks_data_X]\n",
    "        self.idx_boundary = [len_array[0]]\n",
    "\n",
    "        for i in range(1, len(len_array)):\n",
    "            self.idx_boundary.append(len_array[i] + self.idx_boundary[i - 1])\n",
    "\n",
    "        self.build_class_indices()\n",
    "        if self.balanced and self.pattern != None and len(self.pattern) > 0:\n",
    "            self.build_pattern_info()\n",
    "\n",
    "        self.caculate_features_list()\n",
    "\n",
    "    def caculate_features_list(self):\n",
    "        cols = self.ticks_data_X[0].columns.values\n",
    "        if self.features_type == 1:\n",
    "            self.features = [\n",
    "                c\n",
    "                for c in cols\n",
    "                if c.startswith(\"B1\") or c.startswith(\"B3\") or c == \"Signal\"\n",
    "            ]\n",
    "        elif self.features_type == 2:\n",
    "            self.features = [\n",
    "                c\n",
    "                for c in cols\n",
    "                if c.startswith(\"B2\") or c.startswith(\"B3\") or c == \"Signal\"\n",
    "            ]\n",
    "        elif self.features_type == 3:\n",
    "            self.features = [\n",
    "                c\n",
    "                for c in cols\n",
    "                if c.startswith(\"B1\")\n",
    "                or c.startswith(\"B3\")\n",
    "                or c.startswith(\"B4\")\n",
    "                or c == \"Signal\"\n",
    "            ]\n",
    "        else:\n",
    "            self.features = [\n",
    "                c\n",
    "                for c in cols\n",
    "                if c.startswith(\"B2\")\n",
    "                or c.startswith(\"B3\")\n",
    "                or c.startswith(\"B4\")\n",
    "                or c == \"Signal\"\n",
    "            ]\n",
    "\n",
    "    def build_class_indices(self):\n",
    "        total_y = pd.concat(\n",
    "            [t[self.seq_len - 1 :][\"label\"] for t in self.ticks_data_Y]\n",
    "        ).reset_index()\n",
    "        self.class_indices = []\n",
    "        for i in range(num_classes):\n",
    "            class_idx_list = total_y.index[total_y[\"label\"] == i].tolist()\n",
    "            random.shuffle(class_idx_list)\n",
    "            self.class_indices.append(class_idx_list)\n",
    "\n",
    "        self.class_num_of_max_size, self.max_class_size = ivmax(\n",
    "            [len(class_idx_list) for class_idx_list in self.class_indices]\n",
    "        )\n",
    "\n",
    "    def build_pattern_info(self):\n",
    "        self.inner_class_count_of_pattern = list(np.zeros(num_classes, dtype=int))\n",
    "        self.inner_offset_of_pattern = list(np.zeros(self.pattern_size, dtype=int))\n",
    "        for i, c in enumerate(self.pattern):\n",
    "            self.inner_offset_of_pattern[i] = self.inner_class_count_of_pattern[c]\n",
    "            self.inner_class_count_of_pattern[c] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.balanced:\n",
    "            if self.pattern != None and len(self.pattern) > 0:\n",
    "                return math.ceil(\n",
    "                    self.max_class_size\n",
    "                    / self.inner_class_count_of_pattern[self.class_num_of_max_size]\n",
    "                ) * len(self.pattern)\n",
    "                # return math.ceil(\n",
    "                #     self.max_class_size\n",
    "                #     * (\n",
    "                #         len(self.pattern)\n",
    "                #         / self.inner_class_count_of_pattern[self.class_num_of_max_size]\n",
    "                #     )\n",
    "                # )\n",
    "            else:\n",
    "                return self.max_class_size * num_classes\n",
    "\n",
    "        return self.idx_boundary[-1]\n",
    "\n",
    "    def info(self):\n",
    "        print(f\"Dataset size: {self.idx_boundary[-1]}\")\n",
    "        for i in range(num_classes):\n",
    "            print(\n",
    "                f\"class {i}: {len(self.class_indices[i]) * 100 /self.idx_boundary[-1]:.1f}% {len(self.class_indices[i])}\"\n",
    "            )\n",
    "        if self.balanced:\n",
    "            new_size = self.__len__()\n",
    "            print(f\"\\nNew dataset size after balancing classes: {new_size}\")\n",
    "            if self.pattern != None and len(self.pattern) > 0:\n",
    "                for i in range(num_classes):\n",
    "                    ratio = self.inner_class_count_of_pattern[i] / len(self.pattern)\n",
    "                    print(f\"class {i}: {100 * ratio:.1f}% {new_size * ratio:g}\")\n",
    "            else:\n",
    "                for i in range(num_classes):\n",
    "                    print(f\"class {i}: {100/num_classes:.1f}% {self.max_class_size}\")\n",
    "\n",
    "    def idx_of_balanced_data_to_original_idx(self, idx_of_balanced_data):\n",
    "        if self.pattern != None and self.pattern_size > 0:\n",
    "            pattern_idx = idx_of_balanced_data % self.pattern_size\n",
    "            selected_class = self.pattern[pattern_idx]\n",
    "            idx_of_balanced_class = (\n",
    "                (idx_of_balanced_data // self.pattern_size)\n",
    "                * self.inner_class_count_of_pattern[selected_class]\n",
    "            ) + self.inner_offset_of_pattern[pattern_idx]\n",
    "        else:\n",
    "            selected_class = idx_of_balanced_data % num_classes\n",
    "            idx_of_balanced_class = idx_of_balanced_data // num_classes\n",
    "\n",
    "        offset_balanced_class = idx_of_balanced_class % len(\n",
    "            self.class_indices[selected_class]\n",
    "        )\n",
    "        return self.class_indices[selected_class][offset_balanced_class]\n",
    "\n",
    "    def __getitem__(self, idx_of_balanced_data):\n",
    "        idx = (\n",
    "            self.idx_of_balanced_data_to_original_idx(idx_of_balanced_data)\n",
    "            if self.balanced\n",
    "            else idx_of_balanced_data\n",
    "        )\n",
    "\n",
    "        # print(f\"getitem, idx_of_balanced_data:{idx_of_balanced_data}, idx:{idx}\")\n",
    "        for ticks_data_idx in range(len(self.ticks_data_X)):\n",
    "            if self.idx_boundary[ticks_data_idx] > idx:\n",
    "                break\n",
    "        offset = (\n",
    "            idx if ticks_data_idx == 0 else idx - self.idx_boundary[ticks_data_idx - 1]\n",
    "        )\n",
    "        x = np.array(self.ticks_data_X[ticks_data_idx][offset : offset + self.seq_len])\n",
    "        y = int(self.ticks_data_Y[ticks_data_idx].iloc[offset + self.seq_len - 1, :])\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_LSTMDataset(\n",
    "    _return_period, _seq_len, train_data_pattern=None, features_type=1\n",
    "):\n",
    "    ticks_dataset = prepare_analysis_data(_return_period)\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for idx, dataset in enumerate(ticks_dataset):\n",
    "        # test_size = int(dataset.shape[0] * validation_size)\n",
    "        train_size = int(dataset.shape[0] * (1 - validation_size))\n",
    "        random.seed(42)\n",
    "        train_data = dataset.iloc[0:train_size]\n",
    "        test_data = dataset.iloc[train_size - seq_len + 1 :]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:-1].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[-1:].values]\n",
    "        )\n",
    "        if len(features) != 17:\n",
    "            print(f\"Wrong data: {stk_symbols[idx]}, {X_train_data.columns.values}\")\n",
    "            break\n",
    "        # print(idx)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    train_dataset = LSTMDataSet(\n",
    "        ticks_X_train_data,\n",
    "        ticks_Y_train_data,\n",
    "        _seq_len,\n",
    "        balanced=True,\n",
    "        pattern=train_data_pattern,\n",
    "        features_type=features_type,\n",
    "    )\n",
    "    test_dataset = LSTMDataSet(\n",
    "        ticks_X_test_data,\n",
    "        ticks_Y_test_data,\n",
    "        _seq_len,\n",
    "        balanced=False,\n",
    "        pattern=None,\n",
    "        features_type=features_type,\n",
    "    )\n",
    "\n",
    "    print(\"Training data:\")\n",
    "    train_dataset.info()\n",
    "    print(\"\\nTest data\")\n",
    "    test_dataset.info()\n",
    "\n",
    "    return [train_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_XYData(_return_period):\n",
    "    ticks_dataset = prepare_analysis_data(_return_period)\n",
    "    ticks_X_train_data = []\n",
    "    ticks_Y_train_data = []\n",
    "    ticks_X_test_data = []\n",
    "    ticks_Y_test_data = []\n",
    "    ticks_X_dfm = []\n",
    "    for idx, dataset in enumerate(ticks_dataset):\n",
    "        # test_size = int(dataset.shape[0] * validation_size)\n",
    "        train_size = int(dataset.shape[0] * (1 - validation_size))\n",
    "        random.seed(42)\n",
    "        train_data = dataset.iloc[0:train_size]\n",
    "        test_data = dataset.iloc[train_size - seq_len + 1 :]\n",
    "\n",
    "        X_train_data = train_data.iloc[:, :-1]\n",
    "        Y_train_data = train_data.iloc[:, -1:]\n",
    "\n",
    "        X_test_data = test_data.iloc[:, :-1]\n",
    "        Y_test_data = test_data.iloc[:, -1:]\n",
    "\n",
    "        features = [\n",
    "            ([column], StandardScaler()) for column in X_train_data.columns[:-1].values\n",
    "        ]\n",
    "        features.extend(\n",
    "            [([column], None) for column in X_train_data.columns[-1:].values]\n",
    "        )\n",
    "        if len(features) != 17:\n",
    "            print(f\"Wrong data: {stk_symbols[idx]}, {X_train_data.columns.values}\")\n",
    "            break\n",
    "        # print(idx)\n",
    "        X_dfm = DataFrameMapper(features, input_df=True, df_out=True)\n",
    "        X_train_data = X_dfm.fit_transform(X_train_data)\n",
    "        X_test_data = X_dfm.transform(X_test_data)\n",
    "\n",
    "        ticks_X_dfm.append(X_dfm)\n",
    "        ticks_X_train_data.append(X_train_data)\n",
    "        ticks_Y_train_data.append(Y_train_data)\n",
    "        ticks_X_test_data.append(X_test_data)\n",
    "        ticks_Y_test_data.append(Y_test_data)\n",
    "\n",
    "    return [\n",
    "        ticks_X_train_data,\n",
    "        ticks_Y_train_data,\n",
    "        ticks_X_test_data,\n",
    "        ticks_Y_test_data,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, s1 = prepare_LSTMDataset(return_period, seq_len, [0, 1, 0], features_type=1)\n",
    "print(t1.__getitem__(0))\n",
    "t2, s2 = prepare_LSTMDataset(return_period, seq_len, [0, 1, 0], features_type=2)\n",
    "print(t2.__getitem__(0))\n",
    "t3, s3 = prepare_LSTMDataset(return_period, seq_len, [0, 1, 0], features_type=3)\n",
    "print(t3.__getitem__(0))\n",
    "t4, s4 = prepare_LSTMDataset(return_period, seq_len, [0, 1, 0], features_type=4)\n",
    "print(t3.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.ticks_data_X[0].iloc[[0]].loc[:, t1.features]\n",
    "t1.ticks_data_X[0].iloc[0, :]\n",
    "xx = pd.concat(\n",
    "    [\n",
    "        t1.ticks_data_X[0].iloc[[0]].loc[:, t1.features],\n",
    "        t2.ticks_data_X[0].iloc[[0]].loc[:, t2.features],\n",
    "        t3.ticks_data_X[0].iloc[[0]].loc[:, t3.features],\n",
    "        t4.ticks_data_X[0].iloc[[0]].loc[:, t4.features],\n",
    "    ]\n",
    ")\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    _return_period, _seq_len, train_data_pattern=None, features_type=1\n",
    "):\n",
    "    data = prepare_LSTMDataset(\n",
    "        _return_period, _seq_len, train_data_pattern, features_type=features_type\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data[0],\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        data[1],\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, len(data[0].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class StockPCTLabelPredictLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_fc_layers,\n",
    "            activation_type,\n",
    "        )\n",
    "\n",
    "    def __init__(self, input_size, config):\n",
    "        super().__init__()\n",
    "        self.setup_model(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_fc_layers=config[\"num_fc_layers\"],\n",
    "            activation_type=config[\"activation_type\"],\n",
    "        )\n",
    "\n",
    "    def setup_model(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_fc_layers,\n",
    "        activation_type,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \"\"\"\n",
    "            input_size    : The number of expected features in the input x\n",
    "            hidden_size   : The number of features in the hidden state h\n",
    "            num_layers    : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "            bias          : If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            batch_first   : If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "            dropout       : If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            bidirectional : If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size     : If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        layers = []\n",
    "        in_features = self.hidden_size\n",
    "        for i in range(1, num_fc_layers):\n",
    "            out_features = int(in_features / 2)\n",
    "            if out_features <= num_classes:\n",
    "                break\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            (\n",
    "                layers.append(nn.ReLU() if activation_type == 1 else nn.Sigmoid())\n",
    "                if activation_type == 2\n",
    "                else nn.Tanh()\n",
    "            )\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange = 0.5\n",
    "            nn.init.uniform_(m.weight, -initrange, initrange)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        out, (h_out, _) = self.rnn(x, (h_0, c_0))\n",
    "\n",
    "        fc_input = h_out[-1].view(-1, self.hidden_size)\n",
    "        return self.fc(fc_input)\n",
    "\n",
    "\n",
    "def save_model(model, hyper_parameters, file_path, epoch_num=None):\n",
    "    state = {\n",
    "        \"epoch_num\": epoch_num,\n",
    "        \"time\": str(datetime.now),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hyper_parameters\": hyper_parameters,\n",
    "    }\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    data_dict = torch.load(file_path)\n",
    "    hyper_parameters = data_dict[\"hyper_parameters\"]\n",
    "    model = StockPCTLabelPredictLSTM(\n",
    "        input_size=data_dict[\"input_size\"],\n",
    "        hidden_size=int(hyper_parameters[\"hidden_size\"]),\n",
    "        num_layers=int(hyper_parameters[\"num_layers\"]),\n",
    "        num_fc_layers=int(hyper_parameters[\"num_fc_layers\"]),\n",
    "        activation_type=int(hyper_parameters[\"activation_type\"]),\n",
    "    )\n",
    "    model.load_state_dict(data_dict[\"model_state\"])\n",
    "    return model, hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "METRICS_LABEL_NDX = 0  # ground_truth\n",
    "METRICS_PBTY_NDX = 1  # Probability of predicition\n",
    "METRICS_PRED_NDX = 2  # class(label) of predicition\n",
    "METRICS_LOSS_NDX = 3\n",
    "METRICS_SIZE = 4\n",
    "softmax = nn.Softmax(dim=1)\n",
    "totalTrainingSamples_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def logMetrics(\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5,\n",
    "    config=None,\n",
    "    log_hparam=False,\n",
    "):\n",
    "    log.info(\n",
    "        \"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            task_name,\n",
    "        )\n",
    "    )\n",
    "    F1_rec = namedtuple(\n",
    "        \"f1_rec\",\n",
    "        \"target_class pos_correct neg_correct pos_count neg_count pos_loss neg_loss precision recall F1\",\n",
    "    )\n",
    "    F1_metrics = []\n",
    "    for target_class in reversed(range(num_classes)):\n",
    "        posLabel_mask = metrics_t[METRICS_LABEL_NDX] == target_class\n",
    "        pos_count = posLabel_mask.sum()\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] != target_class\n",
    "        neg_count = negLabel_mask.sum()\n",
    "\n",
    "        posPred_mask = metrics_t[METRICS_PRED_NDX] == target_class\n",
    "        threshold_mask = metrics_t[METRICS_PBTY_NDX] > classificationThreshold\n",
    "        # TP, truePos_count\n",
    "        TP = pos_correct = int((posLabel_mask & posPred_mask & threshold_mask).sum())\n",
    "\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] != target_class\n",
    "        # TN, trueNeg_count\n",
    "        TN = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "\n",
    "        # FP, falsePos_count\n",
    "        FP = neg_count - neg_correct\n",
    "        # FN, falseNeg_count\n",
    "        FN = pos_count - pos_correct\n",
    "\n",
    "        # precision = TP / (TP + FP)\n",
    "        precision = 0.0 if (TP + FP) == 0 else TP / np.float32(TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        recall = 0.0 if (TP + FN) == 0 else TP / np.float32(TP + FN)\n",
    "        # F1 = 2 * precision * recall / (precision + recall)\n",
    "        F1 = (\n",
    "            0.0\n",
    "            if (precision + recall) == 0.0\n",
    "            else (2 * precision * recall) / np.float32(precision + recall)\n",
    "        )\n",
    "        F1_metrics.append(\n",
    "            F1_rec(\n",
    "                target_class,\n",
    "                pos_correct,\n",
    "                neg_correct,\n",
    "                pos_count,\n",
    "                neg_count,\n",
    "                metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean(),\n",
    "                metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean(),\n",
    "                precision,\n",
    "                recall,\n",
    "                F1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if num_classes == 2:\n",
    "            break\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\" e_loss/all\"] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "    log.info(\n",
    "        (\"E{} {:8} { e_loss/all:.4f} loss\").format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            **metrics_dict,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for target_class, rec in enumerate(F1_metrics):\n",
    "        target_class_str = f\"class {rec.target_class}\" if num_classes > 2 else \"\"\n",
    "        metrics_dict[f\"{target_class_str} e_loss/pos\"] = rec.pos_loss\n",
    "        metrics_dict[f\"{target_class_str} e_loss/neg\"] = rec.neg_loss\n",
    "\n",
    "        metrics_dict[f\"{target_class_str} correct/all\"] = (\n",
    "            (rec.pos_correct + rec.neg_correct) / metrics_t.shape[1] * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} correct/neg\"] = (\n",
    "            (rec.neg_correct) / rec.neg_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} correct/pos\"] = (\n",
    "            (rec.pos_correct) / rec.pos_count * 100\n",
    "        )\n",
    "        metrics_dict[f\"{target_class_str} pr/precision\"] = rec.precision\n",
    "        metrics_dict[f\"{target_class_str} pr/recall\"] = rec.recall\n",
    "        metrics_dict[f\"{target_class_str} pr/f1_score\"] = rec.F1\n",
    "\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" correct/all:-5.1f}% correct, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/precision:.4f} precision, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/recall:.4f} recall, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" pr/f1_score:.4f} f1 score\"\n",
    "            ).format(epoch_ndx, mode_str, target_class_str, **metrics_dict)\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" e_loss/neg:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_neg\",\n",
    "                target_class_str,\n",
    "                neg_correct=rec.neg_correct,\n",
    "                neg_count=rec.neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\n",
    "                \"E{} {:8} {} {\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" e_loss/pos:.4f} loss, \"\n",
    "                + \"{\"\n",
    "                + f\"{target_class_str}\"\n",
    "                + \" correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + \"_pos\",\n",
    "                target_class_str,\n",
    "                pos_correct=rec.pos_correct,\n",
    "                pos_count=rec.pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir + f\"/{mode_str}_cls\")\n",
    "    for key, value in metrics_dict.items():\n",
    "        writer.add_scalar(key, value, totalTrainingSamples_count)\n",
    "\n",
    "    writer.add_pr_curve(\n",
    "        \"pr\",\n",
    "        metrics_t[METRICS_LABEL_NDX],\n",
    "        metrics_t[METRICS_PRED_NDX],\n",
    "        totalTrainingSamples_count,\n",
    "    )\n",
    "\n",
    "    bins = [x / 50.0 for x in range(51)]\n",
    "    negHist_mask = negLabel_mask & (metrics_t[METRICS_PBTY_NDX] > 0.01)\n",
    "    posHist_mask = posLabel_mask & (metrics_t[METRICS_PBTY_NDX] < 0.99)\n",
    "    if negHist_mask.any():\n",
    "        writer.add_histogram(\n",
    "            \"is_neg\",\n",
    "            metrics_t[METRICS_PBTY_NDX, negHist_mask],\n",
    "            totalTrainingSamples_count,\n",
    "            bins=bins,\n",
    "        )\n",
    "    if posHist_mask.any():\n",
    "        writer.add_histogram(\n",
    "            \"is_pos\",\n",
    "            metrics_t[METRICS_PBTY_NDX, posHist_mask],\n",
    "            totalTrainingSamples_count,\n",
    "            bins=bins,\n",
    "        )\n",
    "\n",
    "    if log_hparam:\n",
    "        hparam = config.copy()\n",
    "        hparam[\"0:trn,1:val\"] = 0 if mode_str == \"trn\" else 1\n",
    "        writer.add_hparams(\n",
    "            hparam,\n",
    "            {\n",
    "                \"loss\": metrics_t[METRICS_LOSS_NDX].mean(),\n",
    "                \"F1\": F1_metrics[-1].F1,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return float(metrics_dict[\" e_loss/all\"]), F1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, ignore_index=-100, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        # use standard CE loss without reducion as basis\n",
    "        self.CE = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=ignore_index)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        input (B, N)\n",
    "        target (B)\n",
    "        \"\"\"\n",
    "        minus_logpt = self.CE(input, target)\n",
    "        pt = torch.exp(-minus_logpt)  # don't forget the minus here\n",
    "        focal_loss = (1 - pt) ** self.gamma * minus_logpt\n",
    "\n",
    "        # apply class weights\n",
    "        if self.alpha != None:\n",
    "            focal_loss *= self.alpha.gather(0, target)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def computeBatchLoss(model, loss_fn, x, y, metrics_g, batch_idx):\n",
    "    x_g = x.to(device)\n",
    "    y_g = y.to(device)\n",
    "    outputs = model(x_g)\n",
    "    if outputs.isnan().sum() > 0:\n",
    "        return sys.float_info.max\n",
    "\n",
    "    loss_g = loss_fn(outputs, y_g)\n",
    "    probability_g, predition_g = torch.max(softmax(outputs), dim=1)\n",
    "\n",
    "    start_ndx = batch_idx * batch_size\n",
    "    end_ndx = start_ndx + y.size(0)\n",
    "\n",
    "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = y_g\n",
    "    metrics_g[METRICS_PBTY_NDX, start_ndx:end_ndx] = probability_g\n",
    "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predition_g\n",
    "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
    "    loss = loss_g.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyPyUtil.util import enumerateWithEstimate\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def doTraining(model, optimizer, loss_fn, epoch_ndx, train_dl):\n",
    "    global totalTrainingSamples_count\n",
    "    model.train()\n",
    "    trnMetrics_g = torch.zeros(\n",
    "        METRICS_SIZE,\n",
    "        len(train_dl.dataset),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, (x, y) in batch_iter:\n",
    "        # for batch_ndx, (x, y) in enumerate(tqdm(train_dl)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = computeBatchLoss(\n",
    "            model,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            trnMetrics_g,\n",
    "            batch_ndx,\n",
    "        )\n",
    "\n",
    "        if loss == sys.float_info.max:\n",
    "            print(f\"forward error: {batch_ndx}\")\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    totalTrainingSamples_count += len(train_dl.dataset)\n",
    "    return trnMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(model, loss_fn, epoch_ndx, val_dl):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, (x, y) in batch_iter:\n",
    "            # for batch_ndx, (x, y) in enumerate(tqdm(val_dl)):\n",
    "            loss = computeBatchLoss(model, loss_fn, x, y, valMetrics_g, batch_ndx)\n",
    "            if loss == sys.float_info.max:\n",
    "                print(f\"forward error: {batch_ndx}\")\n",
    "\n",
    "    return valMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toClassPattern(pattern_type):\n",
    "    return (\n",
    "        None\n",
    "        if pattern_type == 1\n",
    "        else (\n",
    "            [0, 1]\n",
    "            if pattern_type == 2\n",
    "            else (\n",
    "                [0, 0, 1]\n",
    "                if pattern_type == 3\n",
    "                else [0, 0, 0, 1] if pattern_type == 4 else None\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(config, data):\n",
    "    global totalTrainingSamples_count\n",
    "    best_f1 = 0\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    # print(id_str)\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "    pattern = toClassPattern(config[\"pattern\"])\n",
    "\n",
    "    train_dataset = LSTMDataSet(\n",
    "        data[0],\n",
    "        data[1],\n",
    "        seq_len,\n",
    "        balanced=False if config[\"pattern\"] == 1 else True,\n",
    "        pattern=pattern,\n",
    "        # features_type=config[\"features_type\"],\n",
    "    )\n",
    "    test_dataset = LSTMDataSet(\n",
    "        data[2],\n",
    "        data[3],\n",
    "        seq_len,\n",
    "        balanced=False,\n",
    "        pattern=None,\n",
    "        # features_type=config[\"features_type\"],\n",
    "    )\n",
    "\n",
    "    print(\"Training data:\")\n",
    "    train_dataset.info()\n",
    "    print(\"\\nTest data\")\n",
    "    test_dataset.info()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    features_size = len(train_dataset.features)\n",
    "\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    )\n",
    "    # loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    loss_fn = FocalLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        loss, _ = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"trn\",\n",
    "            trnMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        _, F1_metrics = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"val\",\n",
    "            valMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "        if F1_metrics[0].F1 > best_f1:\n",
    "            best_f1 = F1_metrics[0].F1\n",
    "            save_model(model, config, model_name)\n",
    "            print(f\"current loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "config = {\n",
    "    \"return_period\": return_period,\n",
    "    \"seq_len\": seq_len,\n",
    "    # \"features_type\": 3,\n",
    "    \"pattern\": 1,\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.11646759543664197,\n",
    "    \"optim_type\": 2,  # 1: Adam, 2: SGD  => Adam bad result\n",
    "    \"weight decay\": 0.00001,\n",
    "    \"num_layers\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_fc_layers\": 1,\n",
    "    \"activation_type\": 2,  # Sigmoid\n",
    "}\n",
    "epoch_num = 100\n",
    "# os.mkdir(log_dir)\n",
    "print(log_dir)\n",
    "\n",
    "XYData = prepare_XYData(return_period)\n",
    "start = datetime.now()\n",
    "# torch.seed = 42\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "train_LSTM(config, data=XYData)\n",
    "print(f\"Elapsed time:{datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_train_task(config, data):\n",
    "    global totalTrainingSamples_count\n",
    "    global log_dir\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    momentum = config[\"momentum\"]\n",
    "    optim_type = config[\"optim_type\"]\n",
    "    totalTrainingSamples_count = 0\n",
    "\n",
    "    id_str = \"_\".join(str(v) if v < 1 else f\"{v:g}\" for v in config.values())\n",
    "    # print(id_str)\n",
    "    log_dir = f\"{log_dir_base}/{time_str}/{id_str}\"\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "    model_name = f\"{log_dir}/{id_str}.pt\"\n",
    "    pattern = pattern = toClassPattern(config[\"pattern\"])\n",
    "\n",
    "    train_dataset = LSTMDataSet(\n",
    "        data[0],\n",
    "        data[1],\n",
    "        seq_len,\n",
    "        balanced=False if config[\"pattern\"] == 1 else True,\n",
    "        pattern=pattern,\n",
    "        # features_type=config[\"features_type\"],\n",
    "    )\n",
    "    test_dataset = LSTMDataSet(\n",
    "        data[2],\n",
    "        data[3],\n",
    "        seq_len,\n",
    "        balanced=False,\n",
    "        pattern=None,\n",
    "        # features_type=config[\"features_type\"],\n",
    "    )\n",
    "\n",
    "    print(\"Training data:\")\n",
    "    train_dataset.info()\n",
    "    print(\"\\nTest data\")\n",
    "    test_dataset.info()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        pin_memory_device=device_name,\n",
    "    )\n",
    "\n",
    "    features_size = len(train_dataset.features)\n",
    "\n",
    "    model = StockPCTLabelPredictLSTM(input_size=features_size, config=config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if optim_type == 1\n",
    "        else torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=config[\"weight decay\"],\n",
    "        )\n",
    "    )\n",
    "    if config[\"loss_fn\"] == \"CrossEntropy\":\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    else:\n",
    "        loss_fn = FocalLoss(reduction=\"none\")\n",
    "\n",
    "    for epoch_ndx in range(epoch_num):\n",
    "        trnMetrics_t = doTraining(model, optimizer, loss_fn, epoch_ndx, train_loader)\n",
    "        loss, _ = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"trn\",\n",
    "            trnMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "\n",
    "        valMetrics_t = doValidation(model, loss_fn, epoch_ndx, test_loader)\n",
    "        _, F1_metrics = logMetrics(\n",
    "            epoch_ndx,\n",
    "            \"val\",\n",
    "            valMetrics_t,\n",
    "            classificationThreshold,\n",
    "            config,\n",
    "            (epoch_ndx == epoch_num - 1),\n",
    "        )\n",
    "        if F1_metrics[0].F1 > best_f1:\n",
    "            best_f1 = F1_metrics[0].F1\n",
    "            save_model(model, config, model_name)\n",
    "\n",
    "        train.report(\n",
    "            {\n",
    "                \"loss\": loss,\n",
    "                \"f1_score\": F1_metrics[0].F1,\n",
    "                \"precision\": F1_metrics[0].precision,\n",
    "                \"recall\": F1_metrics[0].recall,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    # \"return_period\": tune.grid_search([5]),  # [2,3,5,10]\n",
    "    # \"seq_len\": tune.grid_search([3]),  # 10]),\n",
    "    # \"features_type\": tune.grid_search([4, 3, 2, 1]),\n",
    "    \"pattern\": tune.grid_search([1]),  # 1, 3,\n",
    "    \"loss_fn\": tune.grid_search([\"CrossEntropy\", \"Focal\"]),\n",
    "    \"lr\": tune.grid_search([0.1]),  # , 0.01, 0.1, 0.08, 0.12]\n",
    "    \"momentum\": tune.grid_search([0.11646759543664197]),  # tune.uniform(0.1, 0.9),\n",
    "    \"optim_type\": tune.grid_search([2]),  # 1: Adam, 2: SGD  => Adam bad result\n",
    "    \"weight decay\": tune.grid_search([0.00001]),  # best value\n",
    "    \"num_layers\": tune.grid_search([4]),  # [1, 2, 4, 8] best value = 3 or 4\n",
    "    \"hidden_size\": tune.grid_search([256]),  # [8, 16, 32, 64, 128]\n",
    "    \"num_fc_layers\": tune.grid_search([1]),  # 1, 2, 3]),\n",
    "    \"activation_type\": tune.grid_search(\n",
    "        [2]\n",
    "    ),  # 1: ReLU(),  2: Sigmoid(),  3: Tanh()  => meaningless num_fc_layers == 1\n",
    "}\n",
    "\n",
    "turning_parameters = []\n",
    "total_configs = 1\n",
    "for k, v in search_space.items():\n",
    "    if (\n",
    "        type(v).__name__ == \"dict\"\n",
    "        and list(v.keys())[0] == \"grid_search\"\n",
    "        and len(list(v.values())[0]) > 1\n",
    "    ):\n",
    "        turning_parameters.append(k)\n",
    "        total_configs *= len(list(v.values())[0])\n",
    "print(turning_parameters)\n",
    "print(f\"Total count of configs = {total_configs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "log_dir = f\"{log_dir_base}/{time_str}\"\n",
    "os.mkdir(log_dir)\n",
    "\n",
    "XYData = prepare_XYData(return_period)\n",
    "# analysis = tune.run(\n",
    "#     train_LSTM,\n",
    "#     config=search_space,\n",
    "#     resources_per_trial={\"cpu\": 0.1, \"gpu\": 0.1},\n",
    "#     metric=\"f1_score\",\n",
    "#     mode=\"max\",\n",
    "# )\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(ray_train_task, data=XYData),\n",
    "        resources={\"cpu\": 0.33, \"gpu\": 0.33},\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"f1_score\",\n",
    "        mode=\"max\",\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
